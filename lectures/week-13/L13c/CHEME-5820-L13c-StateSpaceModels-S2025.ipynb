{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95b1b37d-864f-4fce-8aa8-37675b81f7cb",
   "metadata": {},
   "source": [
    "# L13c: Introduction to Linear Structured State Space Models of Sequences\n",
    "This lecture introduces linear structured state space models of _long_ sequences. These models use a time-invariant linear state space representation of _hidden_ state dynamics, then some output mapping between the hidden state and the observed data. The key topics we will cover include:\n",
    "* __Linear time-invariant state space models__: A time-invariant linear state-space model is a mathematical representation of a linear time-invariant (LTI) system using state variables, where the system's dynamics are described by four constant matrices $\\mathbf{A}$, $\\mathbf{B}$, $\\mathbf{C}$, and $\\mathbf{D}$. These models characterize systems with fixed parameters and linear relationships between inputs, outputs, and internal states over time.\n",
    "* __S4 Leg-S models__: Stanford's S4 Leg-S models are structured state-space sequence models that leverage the HiPPO-LegS matrix initialization. They enable efficient modeling of long-range dependencies by decomposing inputs onto orthogonal polynomial bases. These models excel in tasks requiring handling extremely long sequences, such as those in the [Long Range Arena benchmark](https://arxiv.org/abs/2011.04006), by combining linear state space dynamics with deep learning architectures.\n",
    "\n",
    "The material for this lecture was compiled from the following sources: [click me!](https://github.com/varnerlab/CHEME-5820-Lectures-Spring-2025/tree/main/lectures/week-13/L13c/docs)\n",
    "\n",
    "Let's go!\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db86bdf-9d57-4a26-b733-b83557890476",
   "metadata": {},
   "source": [
    "## Background: Linear Time Invariant State Space Models\n",
    "Linear time invariant (LTI) state space models are a class of _continuous-time_ models that can represent a system's dynamics over time. The following equations characterize them:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\dot{\\mathbf{x}} &= \\mathbf{A} \\mathbf{x} + \\mathbf{B} \\mathbf{u} \\\\\n",
    "\\mathbf{y} &= \\mathbf{C} \\mathbf{x} + \\mathbf{D} \\mathbf{u}\n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\mathbf{x}\\in\\mathbb{R}^{h}$ is an $h$-dimensional state vector, $\\mathbf{u}\\in\\mathbb{R}^{d_{in}}$ is the $d_{in}$ dimensional input vector, $\\mathbf{y}\\in\\mathbb{R}^{d_{out}}$ is the $d_{out}$ dimensional output vector. The LTI system is defined by the system matrices (and the initial state and input):\n",
    "* The $\\mathbf{A}\\in\\mathbb{R}^{h\\times{h}}$ matrix is the state transition matrix, which describes how the state depends upon itself over time.\n",
    "* The $\\mathbf{B}\\in\\mathbb{R}^{h\\times{d_{in}}}$ matrix is the input matrix, which describes how the input vector affects the state.\n",
    "* The $\\mathbf{C}\\in\\mathbb{R}^{d_{out}\\times{h}}$ matrix is the output matrix, which describes how the state affects the output vector.\n",
    "* The $\\mathbf{D}\\in\\mathbb{R}^{d_{out}\\times{d_{in}}}$ matrix is the feedforward matrix, which describes how the input vector affects the output vector.\n",
    "\n",
    "Linear time-invariant state space models have been widely used in control theory, signal processing, and other fields. They can model a wide range of systems, including mechanical, electrical, and biological systems.\n",
    "\n",
    "You may be familiar with these models from your automatic control class, where they are used to model system dynamics. In this lecture, we will focus on the discrete-time version of these models, which are often used in machine learning and signal processing applications.\n",
    "* __Single Input Single Output (SISO)__: The simplest case of a linear time invariant state space model is the single input single output (SISO) case, where there is one input $d_{in} = 1$ and one output $d_{out} = 1$ _per time step_. In this case, the system can be represented by a single transfer function, which describes the relationship between the input and output.\n",
    "* __Multiple Input Multiple Output (MIMO)__: In the multiple input multiple output (MIMO) case, there are multiple inputs and multiple outputs. In this case, the system can be represented by a matrix of transfer functions, which describes the relationship between the inputs and outputs.\n",
    "\n",
    "The different versions of this approach for modeling long sequences differ in the structure of the system matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ba8479",
   "metadata": {},
   "source": [
    "## S4 Methods\n",
    "The S4 (Structured State Space Sequence) models, [developed by the Re lab at Stanford](https://cs.stanford.edu/~chrismre/), represent a significant advancement in sequence modeling by leveraging the mathematical framework of state space models (SSMs) and the HiPPO (Highly Predictive Polynomial Operators) theory. \n",
    "\n",
    "* _Advantage_: Unlike traditional architectures such as RNNs, CNNs, or Transformers, S4 models are designed to efficiently capture long-range dependencies in sequential data using a continuous-time state space formulation and a specialized state matrix known as the HiPPO matrix. This approach enables S4 to process long sequences with linear computational and memory complexity. It is highly scalable and effective for tasks involving extensive context, such as time series forecasting, audio, and language modeling.\n",
    "* _Does it work?_ Yes! The S4 (and a newer variant called [the S5 approach](https://arxiv.org/pdf/2208.04933)) has set new benchmarks for long-range sequence modeling, demonstrating both state-of-the-art performance and efficiency across a variety of domains.\n",
    "\n",
    "### SISO Leg-S $\\mathbf{A}$ and $\\mathbf{B}$ HiPPO matrices\n",
    "The Leg-S HiPPO matrices are a specific type of structured state space model designed to capture long-range dependencies in sequential data efficiently. \n",
    "* _What_? The Leg-S approach is based on [Legendre polynomials](https://en.wikipedia.org/wiki/Legendre_polynomials), which are a set of _orthogonal polynomials_ that can be used to represent functions over the finite interval $[-1,1]$. \n",
    "\n",
    "The Leg-S HiPPO $a_{ik}\\in\\mathbf{A}$ state transition matrix for a `SISO` problem is constructed as:\n",
    "$$\n",
    "\\begin{align*}\n",
    "a_{ik} &= \\begin{cases}\n",
    "    \\left(2i+1\\right)^{1/2}\\left(2k+1\\right)^{1/2} & \\text{if } i>k \\\\\n",
    "    \\left(i+1\\right) & \\text{if } i=k \\\\\n",
    "    0 & \\text{if } i<k \\\\\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "$$\n",
    "where the $b_{n}\\in\\mathbf{B}$ input matrix is constructed as:\n",
    "$$\n",
    "\\begin{align*}\n",
    "b_{i} &= \\left(2i+1\\right)^{1/2} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "This form of $\\mathbf{A}$ and $\\mathbf{B}$ have some nice theoretical properties, including:\n",
    "* _Time invariance_: The Leg-S HiPPO matrices are invariant to the input timescale, which means that they can be used to model systems with different time scales without changing the underlying structure of the model.\n",
    "* _Fast computation and bounded_: The Leg-S HiPPO matrices can be computed efficiently using fast algorithms, making them suitable for real-time applications. They also give rise to bounded gradients and approximation errors.\n",
    "* _Alternatives_? Is Leg-S the only approach to building $\\mathbf{A}$ and $\\mathbf{B}$? No! Other approaches use different polynomials. [For more details, click me!](https://github.com/varnerlab/CHEME-5820-Lectures-Spring-2025/blob/main/lectures/week-13/L13c/docs/Gu-arXix-HiPPO-2020.pdf)\n",
    "\n",
    "Let's build some example $\\mathbf{A}$ and $\\mathbf{B}$ matrices using the Leg-S HiPPO approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9efc7b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2.0 0.0 … 0.0 0.0; 3.872983346207417 3.0 … 0.0 0.0; … ; 7.54983443527075 9.746794344808965 … 10.0 0.0; 7.937253933193771 10.246950765959598 … 19.97498435543818 11.0], [1.7320508075688772; 2.23606797749979; … ; 4.358898943540674; 4.58257569495584;;])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(A,B) = let\n",
    "\n",
    "    # initialize -\n",
    "    h = 10; # internal hidden state memory size\n",
    "    din = 1; # we are SISO, so single input \n",
    "    A = Array{Float64,2}(undef, h, h); # internal hidden state memory\n",
    "    B = Array{Float64,2}(undef, h, din); # internal hidden state memory\n",
    "\n",
    "    # build the A-matrix\n",
    "    for i ∈ 1:h\n",
    "        for k = 1:h\n",
    "            \n",
    "            if (i > k)\n",
    "                A[i,k] = sqrt((2*i+1))*sqrt((2*k+1));\n",
    "\n",
    "            elseif (i == k)\n",
    "                A[i,k] = (i+1);\n",
    "            else\n",
    "                A[i,k] = 0.0;\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # build the B-matrix\n",
    "    for i ∈ 1:h\n",
    "        B[i,1] = sqrt((2*i+1));\n",
    "    end\n",
    "\n",
    "    # return -\n",
    "    (A,B)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33559739-fd1b-43b7-858f-788363c40434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×1 Matrix{Float64}:\n",
       " 1.7320508075688772\n",
       " 2.23606797749979\n",
       " 2.6457513110645907\n",
       " 3.0\n",
       " 3.3166247903554\n",
       " 3.605551275463989\n",
       " 3.872983346207417\n",
       " 4.123105625617661\n",
       " 4.358898943540674\n",
       " 4.58257569495584"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e890ee",
   "metadata": {},
   "source": [
    "So we have $\\mathbf{A}$ and $\\mathbf{B}$ matrices that are structured in a way that allows us to efficiently compute the state space model. Where do we get $\\mathbf{C}$ and $\\mathbf{D}$ from? We estimate these matricies from the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472f4b95",
   "metadata": {},
   "source": [
    "### Training an S4 model\n",
    "We train the S4 model using a standard supervised learning approach, minimizing the difference between the predicted output and the true output (loss function). In particular, we estimate the elements of the $\\mathbf{C}$ and (sometimes) the $\\mathbf{D}$ matrices.\n",
    "* _Loss function_: The loss function is typically a mean squared error (MSE) or cross-entropy loss, depending on the output data type. The loss function measures the difference between the predicted and true output.\n",
    "* _Optimization_: The optimization process is typically done using stochastic gradient descent (SGD) or one of its variants, such as Adam or RMSprop. The optimization process updates the parameters of the model (the $\\mathbf{C}$ and $\\mathbf{D}$ matrices) to minimize the loss function.\n",
    "\n",
    "However, there is an interesting wrinkle. To speed up the calculation, we discretize the continuous-time state space model and use the discrete variables of the hidden state in all calculations.\n",
    "\n",
    "#### Discretization\n",
    "The discrete-time state space model is given by:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{x}_{t+1} &= \\mathbf{\\bar{A}} \\mathbf{x}_{t} + \\mathbf{\\bar{B}} \\mathbf{u}_{t} \\\\\n",
    "\\mathbf{y}_{t} &= \\mathbf{\\bar{C}} \\mathbf{x}_{t} + \\mathbf{\\bar{D}} \\mathbf{u}_{t}\n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\mathbf{x}_{t}$ is the hidden state at time $t$, $\\mathbf{u}_{t}$ is the input at time $t$, and $\\mathbf{y}_{t}$ is the output at time $t$. The discretized matrices $\\mathbf{\\bar{A}}$, $\\mathbf{\\bar{B}}$, and $\\mathbf{\\bar{C}}$ can be obtained from a variety of methods, such as the bilinear method:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{\\bar{A}} &= \\left(\\mathbf{I}-\\left(\\Delta/2\\right)\\cdot\\mathbf{A}\\right)^{-1}\\left(\\mathbf{I}+\\left(\\Delta/2\\right)\\cdot\\mathbf{A}\\right) \\\\\n",
    "\\mathbf{\\bar{B}} &= \\left(\\mathbf{I}-\\left(\\Delta/2\\right)\\cdot\\mathbf{A}\\right)^{-1}\\left(\\Delta\\cdot\\mathbf{B}\\right) \\\\\n",
    "\\mathbf{\\bar{C}} &= \\mathbf{C}\n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\Delta$ is the time step size (sampling frequency), and $\\mathbf{I}$ is the identity matrix. The bilinear method is a standard method for discretizing continuous-time state-space models, and it is used in many applications.\n",
    "* _Simplification_: In most applications, we set $\\mathbf{D} = 0$, which means that the output is only dependent on the hidden state and not on the input, thus $\\mathbf{\\bar{D}} = 0$. If this were not the case, we set $\\mathbf{\\bar{D}} = \\mathbf{D}$, which means that the output is dependent on both the hidden state and the input.\n",
    "\n",
    "What problem are we solving in training, e.g., for a regression task? For a `SISO` problem, we want to find the $\\mathbf{C}$ matrix (which is a row-vector) that minimizes the following loss function:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{L}(\\mathbf{\\bar{C}}) &= \\sum_{t=1}^{T}\\left(y_{t}-\\mathbf{\\bar{C}}\\mathbf{x}_{t}\\right)^{2} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "where $T$ is the number of time steps in the sequence, the loss function measures the (squared) difference between the predicted output and the true output, and we want to minimize this difference by adjusting the $\\mathbf{C}$ matrix.\n",
    "* _Hmmm_: Is that loss just linear regression? Yes! The loss function is a standard linear regression loss function, where we are trying to find the best linear mapping between the hidden state and the output.  However, in this case, the hidden states $\\mathbf{x}_{t}$ are independent variables; they are the output of the S4 Leg-S model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3419e799",
   "metadata": {},
   "source": [
    "## Example\n",
    "We are working on an example of this approach for modeling return distributions [for the upcoming INFORMS conference](https://meetings.informs.org/wordpress/annual/?_gl=1%2Apww31x%2A_gcl_au%2AMTYxNTU3NjcxOS4xNzQ0MDMwNzA1). Let's check out that (incomplete) example where do a sequence to sequence modeling task using the S4-LegS model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2aa052",
   "metadata": {},
   "source": [
    "## Lab\n",
    "In Lab `L13d`, we will implement (and _hopefully_ train) an S4 model using the Leg-S HiPPO matrices for a natural language processing task. We will use our own implementation and training methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc2973a",
   "metadata": {},
   "source": [
    "# Today?\n",
    "That's a wrap! What are some of the interesting things we discussed today?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.4",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
