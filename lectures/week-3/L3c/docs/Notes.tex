\documentclass{article}[11pt]
\usepackage{fullpage,graphicx, setspace, latexsym, cite,amsmath,amssymb,xcolor,subfigure}
%\usepackage{epstopdf}
%\DeclareGraphicsExtensions{.pdf,.eps,.png,.jpg,.mps} 
\usepackage{amssymb} %maths
\usepackage{amsmath} %maths
\usepackage{amsthm, comment}
\usepackage[round,comma,sort,numbers]{natbib}

% \bibliographystyle{plain}
\bibliographystyle{plos2015}

\newtheorem{theorem}{Theorem}
\newtheorem{prop}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{defn}{Definition}
\newtheorem{ex}{Example}
\usepackage{float}

\newcommand*{\underuparrow}[1]{\underset{\uparrow}{#1}}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage[dvipsnames]{xcolor}
\usepackage{algorithmicx}
\usepackage{algorithm} %http://ctan.org/pkg/algorithms
\usepackage{algpseudocode} %http://ctan.org/pkg/algorithmicx
\usepackage{enumitem}
\usepackage{simplemargins}
\usepackage{hyperref}

\usepackage{mdframed}
\definecolor{lgray}{rgb}{0.92,0.92,0.92}
\definecolor{lsalmon}{rgb}{0.9921568627450981,0.9411764705882353, 0.9254901960784314}

\renewcommand{\bibnumfmt}[1]{#1.}
\setlist{noitemsep} % or \setlist{noitemsep} to leave space around whole list
\setallmargins{1in}
\linespread{1.1}

\newcommand{\brows}[1]{%
  \begin{bmatrix}
  \begin{array}{@{\protect\rotvert\;}c@{\;\protect\rotvert}}
  #1
  \end{array}
  \end{bmatrix}
}
\newcommand{\rotvert}{\rotatebox[origin=c]{90}{$\vert$}}
\newcommand{\rowsvdots}{\multicolumn{1}{@{}c@{}}{\vdots}}


\def\R{\mathbb{R}}
\def\Eps{\mathcal{E}}
\def\E{\mathbb{E}}
\def\V{\mathbb{V}}
\def\F{\mathcal{F}}
\def\G{\mathcal{G}}
\def\H{\mathcal{H}}
\def\S{\mathcal{S}}
\def\D{\mathcal{D}}
\def\P{\mathbb{P}}
\def\1{\mathbf{1}}
\def\n{\nappa}
\def\h{\mathbf{w}}
\def\v{\mathbf{v}}
\def\x{\mathbf{x}}
\def\X{\mathcal{X}}
\def\Y{\mathcal{Y}}
\def\eps{\epsilon}
\def\y{\mathbf{y}}
\def\e{\mathbf{e}}
\newcommand{\norm}[1]{\left|\left|#1\right|\right|}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\lecture}[4]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   % \setcounter{lecnum}{#1}
   \setcounter{page}{1}
   \setlength{\headsep}{10mm}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf CHEME 5820: Machine Learning for Engineers
   \hfill Spring 2025} }
       \vspace{4mm}
       \hbox to 6.28in { {\Large \hfill Lecture #1: #2  \hfill} }
       \vspace{2mm}
       \hbox to 6.28in { {\it Lecturer: #3 \hfill #4} }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{Lecture #1: #2}{Lecture #1: #2}

   \noindent{\bf Disclaimer}: {\it These notes have not been subjected to the
   usual scrutiny reserved for formal publications. }
   \vspace*{4mm}
}

\begin{document}
\lecture{3c}{Logistic Regression for Binary Classification Problems}{Jeffrey Varner}{}

\begin{mdframed}[backgroundcolor=lgray]
The key concepts covered in this lecture include:
\begin{itemize}[leftmargin=16pt]
    \item{\textbf{Logistic regression} is a statistical method used for binary classification that models the relationship between a dependent categorical variable (label) and one or more independent variables (features) by estimating probabilities through the logistic function.}
    \item{\textbf{Maximum likelihood estimation (MLE)} is a statistical technique to estimate the parameters of a probability distribution by maximizing the likelihood function, thereby determining the parameter values that make the observed data most probable.}
    \item{\textbf{Gradient descent} is an optimization algorithm used to minimize a function by iteratively adjusting parameters in the opposite direction of the gradient. Iteration continues until a local minimum of the function is found.}
    \item{\textbf{Alternatives to gradient descent} include heuristic optimization algorithms such as the Nelder-Mead Simplex Algorithm, Simulated Annealing, Genetic Algorithms, and Particle Swarm Optimization, which can estimate model parameters without relying on the gradient.}
\end{itemize}
\end{mdframed}

\section{Introduction}
In this lecture, we will introduce logistic regression for binary classification problems. 
Logistic regression is a statistical method used for binary classification that models the relationship between a dependent categorical variable (label) 
and one or more independent variables (features) by estimating probabilities through the logistic function.
We will start by discussing the logistic model and then we'll explore the maximum likelihood estimation of the model parameters. 
Unfortunately, the likelihood function of the logistic regression model has no closed-form analytical solution, so we must estimate the model parameters using numerical optimization algorithms.
Toward this challenge, we will introduce the gradient descent algorithm, which is a numerical optimization algorithm that minimizes a function by iteratively adjusting the parameters in the opposite direction of the gradient.
In addition to gradient descent, we will also discuss alternative heuristic optimization algorithms that can be used to estimate the model parameters without relying on the gradient.
You might consider these alternative methods when the objective function is non-convex, non-differentiable, or has many local minima.
Let's start by discussing the logistic regression model.

\section{Logistic Regression}
Logistic regression is a statistical method used for binary classification problems, where the dependent variable (label) is a binary categorical variable (e.g., $\pm{1}$, etc), 
and the independent variables (features) are continuous or categorical variables. Unlike the Perceptron model, which outputs the class label directly, logistic regression models the probability that a given input belongs to a particular class based on the input features.
The logistic regression model estimates the probability that a given feature vector belongs to a particular class based on the input features.
In particular, logistic regression is a discriminative model, which means it directly models the conditional probability of the label given the features, i.e., $P(y|\mathbf{x})$.
This is in contrast to generative models, e.g., Naive Bayes, which we'll explore later, which model the joint probability of the features and the label, i.e., $P(y,\mathbf{x}) = P(\mathbf{x}|y)\cdot{P(y)}$.
The logistic regression model uses the logistic function to model the probability of the binary label $y\in\{-1,+1\}$ given the (augmented) feature vector $\hat{\mathbf{x}} = \left(x_{1},x_{2},\dots,x_{m},1\right)$:
\begin{equation}\label{eq:logistic}
P_{\theta}(y|\hat{\mathbf{x}}) = \frac{1}{1 + e^{-y\cdot\left(\hat{\mathbf{x}}^{\top}\theta\right)}}
\end{equation}
where $\theta\in\R^{p}$ ($p = m+1$) is an (unknown) parameter vector (that we need to estimate somehow), and $e$ is the base of the natural logarithm.
The logistic function is a sigmoid function that maps the input, i.e., $-y\cdot\left(\hat{\mathbf{x}}^{\top}\theta\right)$ to the range $[0,1]$, which is suitable for modeling probabilities.
The logistic model defines ("parameterizes") a probability distribution $P_{\theta}(y|\hat{\mathbf{x}}) : \mathcal{X} \times \mathcal{Y} \to [0,1]$ which is given by the logistic function:
\begin{align*}
P_\theta(y=1 | \mathbf{x}) & = \sigma(-\left(\hat{\mathbf{x}}^{\top}\theta\right)) \\
P_\theta(y=-1 | \mathbf{x}) & = 1-\sigma(-\left(\hat{\mathbf{x}}^{\top}\theta\right)).
\end{align*}
where $\sigma(z) = 1/(1+e^{-z})$. Clearly this sums to one over $y$ for all $\mathbf{x}$.
The logistic regression model predicts the label $y\in\{-1,+1\}$ for a given feature vector $\hat{\mathbf{x}}$ by comparing the probability $P_{\theta}(y|\mathbf{x})$ to a threshold, e.g., 0.5.
The model predicts the positive class if the probability exceeds the threshold ($y = 1$). Otherwise, it predicts the negative class ($y = -1$).
The logistic regression model is trained by estimating the parameters $\theta$ that maximize the likelihood of the observed labels given the features.
The next section will discuss the maximum likelihood estimation of the logistic regression model parameters.

\subsection{Maximum Likelihood Estimation (MLE)}
Maximum likelihood estimation (MLE) is a technique to estimate the parameters of a probability distribution by maximizing the likelihood function.
In logistic regression, MLE estimates the parameters of the logistic regression model that make the observed label conditioned on the features the most probable.
Given a set of training examples $\mathcal{D} = \left\{\left(\mathbf{x}_{1}, y_{1}\right),\dots,\left(\mathbf{x}_{n}, y_{n}\right)\right\}$, where $\mathbf{x}_{i}\in\mathbb{R}^{m}$ is an $m$-dimensional feature vector 
and $y_{i}\in\mathbb{R}$ is the binary (scalar) label, the likelihood function is defined as:
\begin{equation}
L(\theta) = \prod_{i=1}^{n}P_{\theta}(y_{i}|\hat{\mathbf{x}}_{i})
\end{equation}
where $P_{\theta}(y_{i}|\hat{\mathbf{x}}_{i})$ is the probability of observing the label $y_{i}$ given the feature vector $\hat{\mathbf{x}}_{i}$ and the model parameters $\theta$.
It's hard to maximize the likelihood function directly (because of the product), so we take the logarithm of the likelihood function to simplify the optimization:
\begin{align*}
\log{L}(\theta) & = \sum_{i=1}^{n}\log P_{\theta}(y_{i}|\mathbf{x}_{i})\quad(\text{substituting}\,P_{\theta}(y_{i}|\mathbf{x}_{i}))\\
    & = -\sum_{i=1}^{n}\log\left(1 + e^{-y_{i}\cdot\left(\mathbf{x}_{i}^{\top}\theta\right)}\right)
\end{align*}
The maximum likelihood estimation (MLE) of the logistic regression model parameters $\theta^{\star}$ is obtained by maximizing the log-likelihood function
$\log{L}(\theta)$:
\begin{equation}
\theta^{\star} = \argmax_{\theta}\left[-\sum_{i=1}^{n}\log\left(1 + e^{-y_{i}\cdot\left(\hat{\mathbf{x}}^{\top}_{i}\theta\right)}\right)\right]
\end{equation}
or alternatively by minimizing the negative log-likelihood function $\mathcal{L}(\theta) = -\log{L}(\theta)$:
\begin{equation}
    \theta^{\star}  = \argmin_{\theta} \sum_{i=1}^{n}\log\left(1 + e^{-y_{i}\cdot\left(\hat{\mathbf{x}}^{\top}_{i}\theta\right)}\right)
\end{equation}
Unfortunately, whichever perspective we take, the optimization problem has no closed-form analytical solution.
Thus, we must estimate the model parameters using some numerical algorithm, such as gradient descent algorithm (or one of many other approaches).
These approaches iteratively update the parameters $\theta$ to maximize the log-likelihood function
until a stopping criterion is met, e.g., the change in the parameters is below a threshold, or the maximum number of iterations is reached.

\subsection{Gradient Descent}
Gradient descent is a numerical seach algorithm that minimizes a function by iteratively adjusting the parameters in the opposite direction of the gradient.
Suppose there exists an objective function $\mathcal{L}(\theta)$ that we want to minimize with respect to the parameter $\theta$, i.e., the negative log-likelihood function.
In general, an objective function measures the difference between the predicted values and the observed values in some way, e.g., the mean squared error (MSE), the cross-entropy loss, or the negative log-likelihood.
In logistic regression, the objective function is the negative log-likelihood function, which measures the difference between the predicted probabilities and the observed labels. However, whatever form the objective function takes, we assume that it is differentiable and that we can compute the gradient, i.e., $\nabla{l}(\theta)$ for the negative log-likelihood function, 
which points in the direction of the steepest increase of the function. This gives us a way to update the parameters to minimize the objective function using the update rule:
\begin{equation*}
\theta_{k+1} = \theta_{k} - \alpha(k)\cdot\nabla_{\theta}\mathcal{L}(\theta_{k})\quad\text{where}{~k = 0,1,2,\dots}
\end{equation*}
The (hyper) parameter $\alpha(k)>0$ is the learning rate (which can be a function of the iteration count $k$), and $\nabla_{\theta}\mathcal{L}(\theta)$ is the gradient of the negative log-likelihood function with respect to the parameters.  
We iterate until a stopping criterion is met, i.e., $\norm{\theta_{k+1} - \theta_{k}}\leq\epsilon$, the maximum number of iterations is reached, or some other stopping criterion is met.
Pusedo-code for a naive gradient descent algorithm (for a fixed learning rate) is shown in Algorithm \ref{alg:gd}.
\begin{algorithm}[H]
\caption{Naive Gradient Descent for Negative Log-Likelihood $\mathcal{L}(\theta)$}\label{alg:gd}
\begin{algorithmic}[1]
\State \textbf{Input:} Initial parameters $\theta_0$, learning rate $\alpha$, stopping criterion $\epsilon$, maximum iterations $N$
\State \textbf{Output:} Optimal parameter estimates $\theta$

\State Initialize $\theta \gets \theta_0$\Comment{Initialize parameters to the initial guess}
\State $k \gets 0$\Comment{Initialize iteration counter}
\While{$k \leq N$ \textbf{or} $\|\theta_{k+1}  - \theta_{k} \| \leq \epsilon$}
    \State $\mathbf{d} \gets \nabla \mathcal{L}(\theta_{k})$\Comment{Compute gradient using analytical or numerical method, evaluate at $\theta_{k}$}
    \State $\theta_{k+1} \gets \theta_{k} - \alpha \cdot \mathbf{d}$\Comment{Update parameters using the gradient direction $\mathbf{d}$}
    \State $k \gets k + 1$
\EndWhile

\State \textbf{return} $\theta$
\end{algorithmic}
\end{algorithm}

\subsubsection*{Choose the learning Rate $\alpha(k)$}
The choice of the learning rate $\alpha$ is crucial, as a too-large value can cause the algorithm to diverge, while a too-small value can slow down convergence.
Choosing an appropriate learning rate is often challenging and may require tuning through hyperparameter optimization techniques. 
In practice, we may use adaptive learning rate methods, such as the Adam optimizer, which adjusts the learning rate based on the gradient's magnitude and the second moment of the gradient \cite{ADAM-2014}.
Alternatively, Adagrad and RMSprop (unpublished) are other adaptive learning rate methods that can be used to improve the convergence of the gradient descent algorithm \cite{ADAGrad2011}.

\subsection{Alternatives to Gradient Descent}
The central issue with gradient descent is that it can be slow to converge, especially when the objective function is non-convex or has many local minima.
The choice of the learning rate $\alpha$ is also crucial, as a too-large value can cause the algorithm to diverge, while a too-small value can slow down convergence.
Further, the objective function may not be differentiable, or the gradient may be challenging to compute.
In these cases, alternative heuristic optimization algorithms can be used to estimate the model parameters.
The central theme of these approaches to is to directly evaluate the objective function at different points in the parameter space to find the optimal solution.
New search points are generated based randomly or with some heuristic, and the objective function is evaluated at these points to determine the next search direction.
Let's walk through some of the alternatives to gradient descent.

\subsubsection*{Nelder-Mead Simplex Algorithm}
The Nelder-Mead algorithm \cite{NelderMead-1965}, also known as the simplex algorithm, is a direct search optimization algorithm that does not require the gradient of the objective function.
It works by maintaining a simplex (a geometric shape with $n+1$ vertices in an $n$-dimensional space) that evolves through reflection, expansion, contraction, and shrinkage operations.
Thus, the Nelder-Mead algorithm can be used for optimizing non-differentiable or noisy objective functions, making it suitable for a wide range of optimization problems.
However, the Nelder-Mead algorithm may struggle with high-dimensional problems or functions with many local minima, as it does not leverage gradient information to guide the search.
Pseudo-code for the Nelder-Mead algorithm is shown in Algorithm \ref{alg:nm}.
\begin{algorithm}[H]
\caption{Nelder-Mead Simplex Algorithm}\label{alg:nm}
\begin{algorithmic}[1]
\State \textbf{Input:} Initial simplex vertices $x_0, x_1, \ldots, x_n$, reflection coefficient $\rho$, expansion coefficient $\chi$, contraction coefficient $\gamma$, shrinkage coefficient $\sigma$
\State \textbf{Output:} Optimal solution $x^*$
\State Sort vertices such that $f(x_0) \leq f(x_1) \leq \ldots \leq f(x_n)$
\While{stopping criterion not met}
    \State Compute the centroid $x_c$ of the $n$ best vertices
    \State Reflect: $x_r = x_c + \rho(x_c - x_n)$
    \If{$f(x_0) \leq f(x_r) < f(x_{n-1})$}
        \State Replace $x_n$ with $x_r$
    \ElsIf{$f(x_r) < f(x_0)$}
        \State Expand: $x_e = x_c + \chi(x_r - x_c)$
        \If{$f(x_e) < f(x_r)$}
            \State Replace $x_n$ with $x_e$
        \Else
            \State Replace $x_n$ with $x_r$
        \EndIf
    \Else
        \State Contract: $x_c = x_c + \gamma(x_n - x_c)$
        \If{$f(x_c) < f(x_n)$}
            \State Replace $x_n$ with $x_c$
        \Else
            \State Shrink: $x_i = x_0 + \sigma(x_i - x_0)$ for $i = 1, \ldots, n$
        \EndIf
    \EndIf
\EndWhile
\State \textbf{return} $x^*$
\end{algorithmic}
\end{algorithm}


\subsubsection*{Simulated Annealing}
Simulated annealing, originally developed by Kirkpatrick et al \cite{Kirkpatrick:1983aa}, is a probabilistic optimization algorithm inspired by the physical process of 
heating and then slowly cooling (annealing) materials to minimize defects.
Simulated annealing works by iteratively exploring the solution space. First a random (candidate) solution is generated, and the objective function is evaluated at this point.
The difference in the objective function between the current and candidate solutions is computed, and the candidate solution is accepted if it improves the objective function.
Alternatively, the candidate solution may be accepted with a certain probability even if it worsens the objective function. 
Thus, simulated annealing can escape local minima and explore the solution space more thoroughly than gradient descent, at least when the system is hot.
As the number of iterations increases, the algorithm gradually decreases the temperature, i.e., the probability of accepting worse solutions, allowing it to converge to the global optimum.
This method effectively solves complex optimization problems with large search spaces, where traditional techniques may struggle to find the global optimum.
However, simulated annealing can be computationally expensive and may require tuning of hyperparameters, particularly the annealing schedule, i.e., the decrease is temperature at each iteration which impacts
how willing the algorithm is to accept worse solutions to perform well. Pseudo-code for the simulated annealing algorithm is shown in Algorithm \ref{alg:sa}.

\begin{algorithm}[H]
\caption{Simulated Annealing}\label{alg:sa}
\begin{algorithmic}[1]
\State Initialize current solution $x \gets x_0$
\State Initialize temperature $T \gets T_0$
\State Define cooling schedule $T \gets \alpha \cdot T$, where $0 < \alpha < 1$
\State Define maximum iterations $N$
\State Initialize best solution $x_\text{best} \gets x$

\For{$i = 1$ to $N$}
    \State Generate a new candidate solution $x_\text{new}$ in the neighborhood of $x$
    \State Compute $\Delta E \gets f(x_\text{new}) - f(x)$
    \If{$\Delta E < 0$} 
        \State Accept $x_\text{new}$: $x \gets x_\text{new}$
    \Else
        \State Accept $x_\text{new}$ with probability $P \gets e^{-\Delta E / T}$
        \State Draw a random number $r \in [0, 1]$
        \If{$r < P$}
            \State $x \gets x_\text{new}$
        \EndIf
    \EndIf
    \If{$f(x) < f(x_\text{best})$}
        \State Update $x_\text{best} \gets x$
    \EndIf
    \State Update temperature: $T \gets \alpha \cdot T$
    \If{$T$ is below a threshold $T_\text{min}$}
        \State \textbf{break}
    \EndIf
\EndFor

\State \textbf{return} $x_\text{best}$
\end{algorithmic}
\end{algorithm}


\subsubsection*{Genetic Algorithms}
Genetic algorithms (GAs), popularized by John Holland in the 1970s \cite{Holland:1975aa}, are adaptive heuristic search techniques inspired by natural selection and genetics principles. 
They are designed to solve optimization and search problems by iteratively evolving a population of candidate solutions through selection, crossover, and mutation. 
By mimicking the evolutionary process, GAs aim to improve solution quality over generations, making them particularly effective for complex problems that may be discontinuous, non-differentiable, or highly nonlinear.
However, GAs have several hyperparameters that need to be tuned, such as the population size, crossover rate, mutation rate, and the number of generations.
Further, GAs can be computationally expensive, especially for large search spaces, and may require significant computational resources to converge to the optimal solution.
This will be especially true for complex objective functions that are expensive to evaluate, as the algorithm will need to evaluate the objective function for each candidate solution in the population.
Pseudo-code for the genetic algorithm is shown in Algorithm \ref{alg:ga}.

\begin{algorithm}[H]
\caption{Genetic Algorithm}\label{alg:ga}
\begin{algorithmic}[1]
\State \textbf{Input:} Population size $P$, crossover rate $p_c$, mutation rate $p_m$, maximum generations $G$, fitness function $f$
\State \textbf{Output:} Best solution found after $G$ generations (individual with highest fitness)
\State Initialize population $P_0$ randomly
\State Evaluate fitness of each individual in $P_0$
\State $t \gets 0$

\While{$t < G$}
    \State Select parents from $P_t$ based on fitness
    \State Apply crossover with probability $p_c$ to produce offspring
    \State Apply mutation with probability $p_m$ to offspring
    \State Evaluate fitness of offspring
    \State Form new population $P_{t+1}$ by selecting the best individuals from parents and offspring
    \State $t \gets t + 1$
\EndWhile

\State Identify the best individual in $P_t$
\State \textbf{return} Best individual
\end{algorithmic}
\end{algorithm}


% \begin{itemize}
%     \item \textbf{Selection:}
%     \begin{itemize}
%         \item Selection is the process of choosing individuals from the current population to serve as parents for generating offspring.
%         \item The goal is to favor individuals with higher fitness, ensuring the propagation of desirable traits.
%         \item Common methods include:
%         \begin{itemize}
%             \item \textit{Roulette Wheel Selection}: Probability of selection is proportional to an individual's fitness.
%             \item \textit{Tournament Selection}: A group of individuals is randomly chosen, and the fittest among them is selected.
%             \item \textit{Rank-Based Selection}: Individuals are ranked based on fitness, and selection is made based on rank probabilities.
%         \end{itemize}
%     \end{itemize}

%     \item \textbf{Crossover:}
%     \begin{itemize}
%         \item Crossover is a genetic operator used to combine the genetic information of two parent individuals to create one or more offspring.
%         \item It promotes exploration of the solution space by mixing traits from both parents.
%         \item Common crossover techniques include:
%         \begin{itemize}
%             \item \textit{Single-Point Crossover}: A single crossover point is selected, and genetic material is exchanged between parents beyond this point.
%             \item \textit{Two-Point Crossover}: Two crossover points are selected, and the segment between them is swapped between parents.
%             \item \textit{Uniform Crossover}: Each gene is independently swapped between parents with a fixed probability.
%         \end{itemize}
%     \end{itemize}

%     \item \textbf{Mutation:}
%     \begin{itemize}
%         \item Mutation introduces random changes to the genetic material of individuals.
%         \item It helps maintain genetic diversity and prevents the algorithm from converging prematurely to local optima.
%         \item Common mutation methods include:
%         \begin{itemize}
%             \item \textit{Bit Flip Mutation}: A bit in a binary string representation is randomly flipped (e.g., from 0 to 1 or vice versa).
%             \item \textit{Gaussian Mutation}: A small random value from a Gaussian distribution is added to a real-valued gene.
%             \item \textit{Swap Mutation}: Two genes in a sequence are randomly swapped.
%         \end{itemize}
%     \end{itemize}
% \end{itemize}


\subsubsection*{Particle Swarm Optimization}
Particle Swarm Optimization (PSO), developed by Kennedy and Eberhart in the mid-1990s \cite{PSO1995} is an example of a meta-heuristic optimization algorithm inspired by the social behavior of birds and fish, which utilizes a population of candidate solutions, referred to as particles, that move through the search space to find optimal solutions. 
Each particle adjusts its position based on its own experience and the collective knowledge of the swarm, allowing for efficient exploration and exploitation of the solution space to address complex optimization problems across various fields.

\begin{algorithm}[H]
\caption{Particle Swarm Optimization (PSO)}
\begin{algorithmic}[1]
\State \textbf{Input:} Number of particles $N$, maximum iterations $T$, inertia weight $\omega$, cognitive parameter $c_1$, social parameter $c_2$, objective function $f$
\State \textbf{Output:} Best solution found $x_\text{best}$

\State Initialize particles' positions $x_i$ and velocities $v_i$ randomly for $i = 1, \dots, N$
\State Evaluate fitness of each particle and initialize personal best positions $p_i \gets x_i$
\State Set global best position $g \gets \arg\min_{p_i} f(p_i)$

\For{$t = 1$ to $T$}
    \For{each particle $i = 1$ to $N$}
        \State Update velocity: 
        \[
        v_i \gets \omega v_i + c_1 r_1 (p_i - x_i) + c_2 r_2 (g - x_i)
        \]
        where $r_1, r_2 \sim U(0, 1)$
        \State Update position: 
        \[
        x_i \gets x_i + v_i
        \]
        \State Evaluate fitness $f(x_i)$
        \If{$f(x_i) < f(p_i)$}
            \State Update personal best: $p_i \gets x_i$
        \EndIf
        \If{$f(p_i) < f(g)$}
            \State Update global best: $g \gets p_i$
        \EndIf
    \EndFor
\EndFor

\State \textbf{return} $g$
\end{algorithmic}
\end{algorithm}


\section{Summary and Conclusions}
In this lecture, we introduced logistic regression for binary classification problems.
Logistic regression is a powerful tool for binary classification and is widely applied in numerous fields, including healthcare, finance, and marketing.
Like the Perceptron model, logistic regression models the relationship between the features and the label, but it estimates the probability that a given input belongs to a particular class using the logistic function.
To estimate the parameters that appear in the logistic regression model, we discussed the maximum likelihood estimation (MLE) technique, which maximizes the likelihood of the observed labels given the features.
However, unlike our previous discussion of linear regression, the logistic regression model's likelihood function has no closed-form analytical solution.
Thus, we must estimate the model parameters using numerical optimization algorithms, such as the gradient descent algorithm.
Gradient descent is an optimization algorithm employed to minimize a function by iteratively adjusting parameters in the opposite direction of the gradient. 
However, we also discussed various other (heuritisc) optimization algorithms that be used to estimate model parameters without relying on the gradient. 
For instance, simulated annealing, genetic algorithms, and particle swarm optimization are all methods that can be utilized to estimate the model parameters. 
These methods can be particularly useful when the objective function is non-convex, non-differentiable, or has many local minima.

\bibliography{References-L3c.bib}

\end{document}


