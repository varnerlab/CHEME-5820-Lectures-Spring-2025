{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3ba2e53-ccac-45a1-80e7-eb5dc1f7b849",
   "metadata": {},
   "source": [
    "# L3c: Logistic Regression Models for Binary Classification \n",
    "Fill me in\n",
    "\n",
    "Lecture notes can be found: [here!](https://github.com/varnerlab/CHEME-5820-Lectures-Spring-2025/blob/main/lectures/week-3/L3c/docs/Notes.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b40c86-052e-4198-a15e-1d95b870f729",
   "metadata": {},
   "source": [
    "## Setup, Data, and Prerequisites\n",
    "We set up the computational environment by including the `Include.jl` file, loading any needed resources, such as sample datasets, and setting up any required constants. The `Include.jl` file loads external packages, various functions that we will use in the exercise, and custom types to model the components of our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a4994f3-fee9-4994-9d12-5c70d826e63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"Include.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15674fd2-3644-433d-bfdb-866d30383bfe",
   "metadata": {},
   "source": [
    "### Data\n",
    "This lecture will look at a [banknote authentication dataset](https://archive.ics.uci.edu/dataset/267/banknote+authentication) for classification tasks. We'll load the banknote dataset and split it into `training` and `test` data subsets (randomly).\n",
    "* __Training data__: Training datasets are collections of labeled data used to teach machine learning models, allowing these tools to learn patterns and relationships within the data.\n",
    "* __Test data__: Test datasets, on the other hand, are separate sets of labeled data used to evaluate the performance of trained models on unseen examples, providing an unbiased assessment of the _model's generalization capabilities_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57d466e-a347-4375-b7f1-57ec00f8ae67",
   "metadata": {},
   "source": [
    "#### Banknote Authentication Dataset\n",
    "The second dataset we will explore is the [banknote authentication dataset from the UCI archive](https://archive.ics.uci.edu/dataset/267/banknote+authentication). This dataset has `1372` instances of 4 continuous features and an integer $\\{-1,1\\}$ class variable. \n",
    "* __Description__: Data were extracted from images taken from genuine and forged banknote-like specimens.  An industrial camera, usually used for print inspection, was used for digitization. The final images have 400x 400 pixels. Due to the object lens and distance to the investigated object, gray-scale pictures with a resolution of about 660 dpi were gained. Wavelet Transform tools were used to extract features from images.\n",
    "* __Features__: The data has four continuous features from each image: `variance` of the wavelet transformed image, `skewness` of the wavelet transformed image, `kurtosis` of the wavelet transformed image, and the `entropy` of the wavelet transformed image. The class is $\\{-1,1\\}$ where a class value of `-1` indicates genuine, `1` forged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b764fca8-8c28-4b9b-9ed4-55eb22810644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div style = \"float: left;\"><span>1372×5 DataFrame</span></div><div style = \"float: right;\"><span style = \"font-style: italic;\">1347 rows omitted</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">variance</th><th style = \"text-align: left;\">skewness</th><th style = \"text-align: left;\">curtosis</th><th style = \"text-align: left;\">entropy</th><th style = \"text-align: left;\">class</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: right;\">3.6216</td><td style = \"text-align: right;\">8.6661</td><td style = \"text-align: right;\">-2.8073</td><td style = \"text-align: right;\">-0.44699</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: right;\">4.5459</td><td style = \"text-align: right;\">8.1674</td><td style = \"text-align: right;\">-2.4586</td><td style = \"text-align: right;\">-1.4621</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: right;\">3.866</td><td style = \"text-align: right;\">-2.6383</td><td style = \"text-align: right;\">1.9242</td><td style = \"text-align: right;\">0.10645</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">4</td><td style = \"text-align: right;\">3.4566</td><td style = \"text-align: right;\">9.5228</td><td style = \"text-align: right;\">-4.0112</td><td style = \"text-align: right;\">-3.5944</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">5</td><td style = \"text-align: right;\">0.32924</td><td style = \"text-align: right;\">-4.4552</td><td style = \"text-align: right;\">4.5718</td><td style = \"text-align: right;\">-0.9888</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">6</td><td style = \"text-align: right;\">4.3684</td><td style = \"text-align: right;\">9.6718</td><td style = \"text-align: right;\">-3.9606</td><td style = \"text-align: right;\">-3.1625</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">7</td><td style = \"text-align: right;\">3.5912</td><td style = \"text-align: right;\">3.0129</td><td style = \"text-align: right;\">0.72888</td><td style = \"text-align: right;\">0.56421</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">8</td><td style = \"text-align: right;\">2.0922</td><td style = \"text-align: right;\">-6.81</td><td style = \"text-align: right;\">8.4636</td><td style = \"text-align: right;\">-0.60216</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">9</td><td style = \"text-align: right;\">3.2032</td><td style = \"text-align: right;\">5.7588</td><td style = \"text-align: right;\">-0.75345</td><td style = \"text-align: right;\">-0.61251</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">10</td><td style = \"text-align: right;\">1.5356</td><td style = \"text-align: right;\">9.1772</td><td style = \"text-align: right;\">-2.2718</td><td style = \"text-align: right;\">-0.73535</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">11</td><td style = \"text-align: right;\">1.2247</td><td style = \"text-align: right;\">8.7779</td><td style = \"text-align: right;\">-2.2135</td><td style = \"text-align: right;\">-0.80647</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">12</td><td style = \"text-align: right;\">3.9899</td><td style = \"text-align: right;\">-2.7066</td><td style = \"text-align: right;\">2.3946</td><td style = \"text-align: right;\">0.86291</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">13</td><td style = \"text-align: right;\">1.8993</td><td style = \"text-align: right;\">7.6625</td><td style = \"text-align: right;\">0.15394</td><td style = \"text-align: right;\">-3.1108</td><td style = \"text-align: right;\">-1</td></tr><tr><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1361</td><td style = \"text-align: right;\">-0.24745</td><td style = \"text-align: right;\">1.9368</td><td style = \"text-align: right;\">-2.4697</td><td style = \"text-align: right;\">-0.80518</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1362</td><td style = \"text-align: right;\">-1.5732</td><td style = \"text-align: right;\">1.0636</td><td style = \"text-align: right;\">-0.71232</td><td style = \"text-align: right;\">-0.8388</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1363</td><td style = \"text-align: right;\">-2.1668</td><td style = \"text-align: right;\">1.5933</td><td style = \"text-align: right;\">0.045122</td><td style = \"text-align: right;\">-1.678</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1364</td><td style = \"text-align: right;\">-1.1667</td><td style = \"text-align: right;\">-1.4237</td><td style = \"text-align: right;\">2.9241</td><td style = \"text-align: right;\">0.66119</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1365</td><td style = \"text-align: right;\">-2.8391</td><td style = \"text-align: right;\">-6.63</td><td style = \"text-align: right;\">10.4849</td><td style = \"text-align: right;\">-0.42113</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1366</td><td style = \"text-align: right;\">-4.5046</td><td style = \"text-align: right;\">-5.8126</td><td style = \"text-align: right;\">10.8867</td><td style = \"text-align: right;\">-0.52846</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1367</td><td style = \"text-align: right;\">-2.41</td><td style = \"text-align: right;\">3.7433</td><td style = \"text-align: right;\">-0.40215</td><td style = \"text-align: right;\">-1.2953</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1368</td><td style = \"text-align: right;\">0.40614</td><td style = \"text-align: right;\">1.3492</td><td style = \"text-align: right;\">-1.4501</td><td style = \"text-align: right;\">-0.55949</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1369</td><td style = \"text-align: right;\">-1.3887</td><td style = \"text-align: right;\">-4.8773</td><td style = \"text-align: right;\">6.4774</td><td style = \"text-align: right;\">0.34179</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1370</td><td style = \"text-align: right;\">-3.7503</td><td style = \"text-align: right;\">-13.4586</td><td style = \"text-align: right;\">17.5932</td><td style = \"text-align: right;\">-2.7771</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1371</td><td style = \"text-align: right;\">-3.5637</td><td style = \"text-align: right;\">-8.3827</td><td style = \"text-align: right;\">12.393</td><td style = \"text-align: right;\">-1.2823</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1372</td><td style = \"text-align: right;\">-2.5419</td><td style = \"text-align: right;\">-0.65804</td><td style = \"text-align: right;\">2.6842</td><td style = \"text-align: right;\">1.1952</td><td style = \"text-align: right;\">1</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccc}\n",
       "\t& variance & skewness & curtosis & entropy & class\\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & Float64 & Float64 & Int64\\\\\n",
       "\t\\hline\n",
       "\t1 & 3.6216 & 8.6661 & -2.8073 & -0.44699 & -1 \\\\\n",
       "\t2 & 4.5459 & 8.1674 & -2.4586 & -1.4621 & -1 \\\\\n",
       "\t3 & 3.866 & -2.6383 & 1.9242 & 0.10645 & -1 \\\\\n",
       "\t4 & 3.4566 & 9.5228 & -4.0112 & -3.5944 & -1 \\\\\n",
       "\t5 & 0.32924 & -4.4552 & 4.5718 & -0.9888 & -1 \\\\\n",
       "\t6 & 4.3684 & 9.6718 & -3.9606 & -3.1625 & -1 \\\\\n",
       "\t7 & 3.5912 & 3.0129 & 0.72888 & 0.56421 & -1 \\\\\n",
       "\t8 & 2.0922 & -6.81 & 8.4636 & -0.60216 & -1 \\\\\n",
       "\t9 & 3.2032 & 5.7588 & -0.75345 & -0.61251 & -1 \\\\\n",
       "\t10 & 1.5356 & 9.1772 & -2.2718 & -0.73535 & -1 \\\\\n",
       "\t11 & 1.2247 & 8.7779 & -2.2135 & -0.80647 & -1 \\\\\n",
       "\t12 & 3.9899 & -2.7066 & 2.3946 & 0.86291 & -1 \\\\\n",
       "\t13 & 1.8993 & 7.6625 & 0.15394 & -3.1108 & -1 \\\\\n",
       "\t14 & -1.5768 & 10.843 & 2.5462 & -2.9362 & -1 \\\\\n",
       "\t15 & 3.404 & 8.7261 & -2.9915 & -0.57242 & -1 \\\\\n",
       "\t16 & 4.6765 & -3.3895 & 3.4896 & 1.4771 & -1 \\\\\n",
       "\t17 & 2.6719 & 3.0646 & 0.37158 & 0.58619 & -1 \\\\\n",
       "\t18 & 0.80355 & 2.8473 & 4.3439 & 0.6017 & -1 \\\\\n",
       "\t19 & 1.4479 & -4.8794 & 8.3428 & -2.1086 & -1 \\\\\n",
       "\t20 & 5.2423 & 11.0272 & -4.353 & -4.1013 & -1 \\\\\n",
       "\t21 & 5.7867 & 7.8902 & -2.6196 & -0.48708 & -1 \\\\\n",
       "\t22 & 0.3292 & -4.4552 & 4.5718 & -0.9888 & -1 \\\\\n",
       "\t23 & 3.9362 & 10.1622 & -3.8235 & -4.0172 & -1 \\\\\n",
       "\t24 & 0.93584 & 8.8855 & -1.6831 & -1.6599 & -1 \\\\\n",
       "\t25 & 4.4338 & 9.887 & -4.6795 & -3.7483 & -1 \\\\\n",
       "\t26 & 0.7057 & -5.4981 & 8.3368 & -2.8715 & -1 \\\\\n",
       "\t27 & 1.1432 & -3.7413 & 5.5777 & -0.63578 & -1 \\\\\n",
       "\t28 & -0.38214 & 8.3909 & 2.1624 & -3.7405 & -1 \\\\\n",
       "\t29 & 6.5633 & 9.8187 & -4.4113 & -3.2258 & -1 \\\\\n",
       "\t30 & 4.8906 & -3.3584 & 3.4202 & 1.0905 & -1 \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m1372×5 DataFrame\u001b[0m\n",
       "\u001b[1m  Row \u001b[0m│\u001b[1m variance  \u001b[0m\u001b[1m skewness  \u001b[0m\u001b[1m curtosis  \u001b[0m\u001b[1m entropy   \u001b[0m\u001b[1m class \u001b[0m\n",
       "      │\u001b[90m Float64   \u001b[0m\u001b[90m Float64   \u001b[0m\u001b[90m Float64   \u001b[0m\u001b[90m Float64   \u001b[0m\u001b[90m Int64 \u001b[0m\n",
       "──────┼───────────────────────────────────────────────────\n",
       "    1 │  3.6216      8.6661   -2.8073    -0.44699      -1\n",
       "    2 │  4.5459      8.1674   -2.4586    -1.4621       -1\n",
       "    3 │  3.866      -2.6383    1.9242     0.10645      -1\n",
       "    4 │  3.4566      9.5228   -4.0112    -3.5944       -1\n",
       "    5 │  0.32924    -4.4552    4.5718    -0.9888       -1\n",
       "    6 │  4.3684      9.6718   -3.9606    -3.1625       -1\n",
       "    7 │  3.5912      3.0129    0.72888    0.56421      -1\n",
       "    8 │  2.0922     -6.81      8.4636    -0.60216      -1\n",
       "    9 │  3.2032      5.7588   -0.75345   -0.61251      -1\n",
       "   10 │  1.5356      9.1772   -2.2718    -0.73535      -1\n",
       "   11 │  1.2247      8.7779   -2.2135    -0.80647      -1\n",
       "  ⋮   │     ⋮          ⋮          ⋮          ⋮        ⋮\n",
       " 1363 │ -2.1668      1.5933    0.045122  -1.678         1\n",
       " 1364 │ -1.1667     -1.4237    2.9241     0.66119       1\n",
       " 1365 │ -2.8391     -6.63     10.4849    -0.42113       1\n",
       " 1366 │ -4.5046     -5.8126   10.8867    -0.52846       1\n",
       " 1367 │ -2.41        3.7433   -0.40215   -1.2953        1\n",
       " 1368 │  0.40614     1.3492   -1.4501    -0.55949       1\n",
       " 1369 │ -1.3887     -4.8773    6.4774     0.34179       1\n",
       " 1370 │ -3.7503    -13.4586   17.5932    -2.7771        1\n",
       " 1371 │ -3.5637     -8.3827   12.393     -1.2823        1\n",
       " 1372 │ -2.5419     -0.65804   2.6842     1.1952        1\n",
       "\u001b[36m                                         1351 rows omitted\u001b[0m"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_banknote = CSV.read(joinpath(_PATH_TO_DATA, \"data-banknote-authentication.csv\"), DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cf38d63-8bbd-4d86-a979-6c1a560b0886",
   "metadata": {},
   "outputs": [],
   "source": [
    "D_banknote = Matrix(df_banknote); # get the data as a Matrix (alias for Array{Float64,2})\n",
    "number_of_training_examples_banknote = 1200; # how many training points for the banknote dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19d4d996-851f-441d-8e6a-871e5ce2d448",
   "metadata": {},
   "outputs": [],
   "source": [
    "banknote_training, banknote_test = let\n",
    "\n",
    "    number_of_features = size(D_banknote,2); # number of cols of housing data\n",
    "    number_of_examples = size(D_banknote,1); # number of rows of housing data\n",
    "    full_index_set = range(1,stop=number_of_examples,step=1) |> collect |> Set;\n",
    "    \n",
    "    # build index sets for training and testing\n",
    "    training_index_set = Set{Int64}();\n",
    "    should_stop_loop = false;\n",
    "    while (should_stop_loop == false)\n",
    "        i = rand(1:number_of_examples);\n",
    "        push!(training_index_set,i);\n",
    "\n",
    "        if (length(training_index_set) == number_of_training_examples_banknote)\n",
    "            should_stop_loop = true;\n",
    "        end\n",
    "    end\n",
    "    test_index_set = setdiff(full_index_set,training_index_set);\n",
    "\n",
    "    # build the test and train datasets -\n",
    "    banknote_training = D_banknote[training_index_set |> collect,:];\n",
    "    banknote_test = D_banknote[test_index_set |> collect,:];\n",
    "\n",
    "    # return\n",
    "    banknote_training,banknote_test\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc7b23f8-d3a9-4650-afe5-675f91162175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1200×5 Matrix{Float64}:\n",
       " -2.4621     2.7645   -0.62578  -2.8573     1.0\n",
       "  4.0932     5.4132   -1.8219    0.23576   -1.0\n",
       "  0.11686    3.735    -4.4379   -4.3741     1.0\n",
       "  3.2403    -3.7082    5.2804    0.41291   -1.0\n",
       " -1.4427     3.2922   -1.9702   -3.4392     1.0\n",
       " -1.7064     3.3088   -2.2829   -2.1978     1.0\n",
       "  1.5799    -4.7076    7.9186   -1.5487    -1.0\n",
       "  0.76163    5.8209    1.1959   -0.64613   -1.0\n",
       " -1.7886    -6.3486    5.6154    0.42584    1.0\n",
       " -3.0731    -0.53181   2.3877    0.77627    1.0\n",
       " -1.7559    11.9459    3.0946   -4.8978    -1.0\n",
       "  1.9572    -5.1153    8.6127   -1.4297    -1.0\n",
       " -3.7503   -13.4586   17.5932   -2.7771     1.0\n",
       "  ⋮                                        \n",
       "  4.1038    -4.8069    3.3491   -0.49225   -1.0\n",
       " -1.1391     1.8127    6.9144    0.70127   -1.0\n",
       "  4.3848    -3.0729    3.0423    1.2741    -1.0\n",
       "  0.6005     0.99945  -2.2126    0.097399   1.0\n",
       "  3.6277     0.9829    0.68861   0.63403   -1.0\n",
       "  3.9262     6.0299   -2.0156   -0.065531  -1.0\n",
       " -1.6514    -8.4985    9.1122    1.2379     1.0\n",
       "  4.1665    -0.4449    0.23448   0.27843   -1.0\n",
       "  0.75896    0.29176  -1.6506    0.83834    1.0\n",
       "  5.4021     3.1039   -1.1536    1.5651    -1.0\n",
       "  1.2706     8.035    -0.19651  -2.1888    -1.0\n",
       " -1.0555     0.79459  -1.6968   -0.46768    1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "banknote_training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bba928-549c-46a0-9f58-923de6781612",
   "metadata": {},
   "source": [
    "## Perceptron\n",
    "[The Perceptron (Rosenblatt, 1957)](https://en.wikipedia.org/wiki/Perceptron) takes the (scalar) output of a linear regression model $y_{i}\\in\\mathbb{R}$ and then transforms it using the $\\sigma(\\star) = \\texttt{sign}(\\star)$ function to a discrete set of values representing categories, e.g., $\\sigma:\\mathbb{R}\\rightarrow\\{-1,1\\}$ in the binary classification case. \n",
    "* Suppose there exists a data set\n",
    "$\\mathcal{D} = \\left\\{(\\mathbf{x}_{1},y_{1}),\\dotsc,(\\mathbf{x}_{n},y_{n})\\right\\}$ with $n$ _labeled_ examples, where each example has been labeled by an expert, i.e., a human to be in a category $\\hat{y}_{i}\\in\\{-1,1\\}$, given the $m$-dimensional feature vector $\\mathbf{x}_{i}\\in\\mathbb{R}^{m}$. \n",
    "* [The Perceptron](https://en.wikipedia.org/wiki/Perceptron) _incrementally_ learns a linear decision boundary between _two_ classes of possible objects (binary classification) in $\\mathcal{D}$ by repeatedly processing the data. During each pass, a regression parameter vector $\\mathbf{\\beta}$ is updated until it makes no more than a specified number of mistakes. \n",
    "\n",
    "[The Perceptron](https://en.wikipedia.org/wiki/Perceptron) computes the estimated label $\\hat{y}_{i}$ for feature vector $\\hat{\\mathbf{x}}_{i}$ using the $\\texttt{sign}:\\mathbb{R}\\to\\{-1,1\\}$ function:\n",
    "$$\n",
    "\\begin{equation*}\n",
    "    \\hat{y}_{i} = \\texttt{sign}\\left(\\hat{\\mathbf{x}}_{i}^{\\top}\\cdot\\beta\\right)\n",
    "\\end{equation*}\n",
    "$$\n",
    "where $\\beta=\\left(w_{1},\\dots,w_{n}, b\\right)$ is a column vector of (unknown) classifier parameters, $w_{j}\\in\\mathbb{R}$ corresponding to the importance of feature $j$ and $b\\in\\mathbb{R}$ is a bias parameter, the features $\\hat{\\mathbf{x}}^{\\top}_{i}=\\left(x^{(i)}_{1},\\dots,x^{(i)}_{m}, 1\\right)$ are $p = m+1$-dimensional (row) vectors (features augmented with bias term), and $\\texttt{sign}(z)$ is the function:\n",
    "$$\n",
    "\\begin{equation*}\n",
    "    \\texttt{sign}(z) = \n",
    "    \\begin{cases}\n",
    "        1 & \\text{if}~z\\geq{0}\\\\\n",
    "        -1 & \\text{if}~z<0\n",
    "    \\end{cases}\n",
    "\\end{equation*}\n",
    "$$\n",
    "__Hypothesis__: If data set $\\mathcal{D}$ is linearly separable, the Perceptron will _incrementally_ learn a separating hyperplane in a finite number of passes through the data set $\\mathcal{D}$. However, if the data set $\\mathcal{D}$ is not linearly separable, the Perceptron may not converge. Check out a [perceptron pseudo-code here!](https://github.com/varnerlab/CHEME-5820-Lectures-Spring-2025/blob/main/lectures/week-3/L3a/docs/Notes.pdf)\n",
    "* __Training__: Our Perceptron implementation [based on pseudo-code](https://github.com/varnerlab/CHEME-5820-Lectures-Spring-2025/blob/main/lectures/week-3/L3a/docs/Notes.pdf) stores problem information in [a `MyPerceptronClassificationModel` instance, which holds the (initial) parameters and other data](src/Types.jl) required by the problem.\n",
    "* We then _learn_ the model parameters [using the `learn(...)` method](src/Compute.jl), which takes the training features array `X,` the training labels vector `y`, and the problem instance `model` and returns an updated problem instance holding the updated parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76d94e09-3dc0-4a87-9f2c-5553737ae3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped after number of iterations: 10000. We have number of errors: 10\n"
     ]
    }
   ],
   "source": [
    "model_perceptron = let\n",
    "\n",
    "    # data -\n",
    "    D = banknote_training; # What dataset are we going to use?\n",
    "    number_of_examples = size(D,1); # how many examples do we have (rows)\n",
    "    number_of_features = size(D,2); # how many features do we have (cols)?\n",
    "    X = [D[:,1:end-1] ones(number_of_examples)]; # features: need to add a 1 to each row (for bias), after removing the label\n",
    "    y = D[:,end]; # output: this is the target data (label)\n",
    "\n",
    "    # model\n",
    "    model = build(MyPerceptronClassificationModel, (\n",
    "        parameters = ones(number_of_features), # initial value for the parameters: these will be updated\n",
    "        mistakes = 0 # willing to like with m mistakes\n",
    "    ));\n",
    "\n",
    "    # train -\n",
    "    model = learn(X,y,model, maxiter = 10000, verbose = true); # this is learning the model parameters\n",
    "\n",
    "    # return -\n",
    "    model;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cab8079-f9f9-4760-af9b-dc9b920287b6",
   "metadata": {},
   "source": [
    "Now that we have parameters estimated from the `training` data, we can use those parameters on the `test` dataset to see how well the model can differentiate between an actual banknote and a forgery on data it has never seen. \n",
    "* __Inference__: We run the classification operation on the (unseen) test data [using the `classify(...)` method](src/Compute.jl). This method takes a feature array `X` and the (trained) model instance. It returns the estimated labels. We store the actual (correct) label in the `y_banknote_perceptron::Array{Int64,1}` vector, while the model predicted label is stored in the `ŷ_banknote_perceptron::Array{Int64,1}` array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfc33d6d-be82-4adb-9622-27003684aca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ŷ_banknote_perceptron,y_banknote_perceptron = let\n",
    "\n",
    "    D = banknote_test; # what dataset are going to use?\n",
    "    number_of_examples = size(D,1); # how many examples do we have (rows)\n",
    "    number_of_features = size(D,2); # how many features do we have (cols)?\n",
    "    X = [D[:,1:end-1] ones(number_of_examples)]; # features: need to add a 1 to each row (for bias), after removing the label\n",
    "    y = D[:,end]; # output: this is the *actual* target data (label)\n",
    "\n",
    "    # compute the estimated labels -\n",
    "    ŷ = classify(X,model_perceptron)\n",
    "\n",
    "    # return -\n",
    "    ŷ,y\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21d5be8-d19d-47cb-89f7-92989b58f6ad",
   "metadata": {},
   "source": [
    "## Logistics Regression Model\n",
    "Fill me in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d8e0bf97-69d1-46ad-86ab-2d5131738c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped after number of iterations: 1001. We have error: 0.4353819306885545\n"
     ]
    }
   ],
   "source": [
    "model_logistics = let\n",
    "\n",
    "    # data -\n",
    "    D = banknote_training; # What dataset are we going to use?\n",
    "    number_of_examples = size(D,1); # how many examples do we have (rows)\n",
    "    number_of_features = size(D,2); # how many features do we have (cols)?\n",
    "    X = [D[:,1:end-1] ones(number_of_examples)]; # features: need to add a 1 to each row (for bias), after removing the label\n",
    "    y = D[:,end]; # output: this is the target data (label)\n",
    "\n",
    "    # model\n",
    "    model = build(MyLogisticRegressionClassificationModel, (\n",
    "        parameters = 0.01*ones(number_of_features), # initial value for the parameters: these will be updated\n",
    "        learning_rate = 0.005, # you pick this\n",
    "        ϵ = 1e-6, # you pick this (this is also the step size for the fd approx to the gradient)\n",
    "        loss_function = (x,y,θ) -> log10(1+exp(-y*(dot(x,θ)))) # what??!? Yes, we can pass functions as args!\n",
    "    ));\n",
    "\n",
    "    # train -\n",
    "    model = learn(X,y,model, maxiter = 10000, verbose = true); # this is learning the model parameters\n",
    "\n",
    "    # return -\n",
    "    model;\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "827bdefb-028f-4580-aafd-cacba136dd29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyLogisticRegressionClassificationModel([-6.030459276815737, -3.286346888030927, -4.203302037136272, -0.6753969764130093, 5.326707941131643], 0.01, 1.0e-6, var\"#4#5\"())"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_logistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb9d31e-5743-4d8c-967f-cc50a0c61303",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.3",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
