{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3ba2e53-ccac-45a1-80e7-eb5dc1f7b849",
   "metadata": {},
   "source": [
    "# L3c: Logistic Regression Models for Binary Classification \n",
    "This lecture introduces our second binary classification method: [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression). \n",
    "[Logistic regression](https://en.wikipedia.org/wiki/Logistic_regression) is a statistical method used for binary classification problems, where the dependent variable (label) is a binary categorical variable (e.g., $\\pm{1}$, etc.), and the independent variables (features) are continuous or categorical variables. Unlike the Perceptron model, which outputs the class label directly, logistic regression models the _probability_ that a given input belongs to a particular class based on the input features.\n",
    "\n",
    "The key concepts covered in this lecture include:\n",
    "* __Logistic regression__ is a binary classification that models the relationship between a dependent categorical variable (label) and one or more independent variables (features) by estimating the probability of a label using the [logistic function](https://en.wikipedia.org/wiki/Logistic_function).\n",
    "* __Maximum likelihood estimation (MLE)__ is an approach to estimate the parameters of a probability distribution by maximizing the likelihood function of the output (label), given the input (features), thereby determining the parameter values that make the output most probable given the input.\n",
    "* __Gradient descent__ is an optimization algorithm that minimizes a function by iteratively adjusting parameters in the opposite direction of [the gradient](https://en.wikipedia.org/wiki/Gradient). Iteration continues until some stopping criteria are met, e.g., a logical minimum is found, or we run out of iterations. Sometimes, computing [the gradient](https://en.wikipedia.org/wiki/Gradient) is a hassle.\n",
    "* __Alternatives to gradient descent__ include heuristic optimization algorithms such as the [Nelder-Mead Simplex Algorithm](https://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method), [Simulated Annealing](https://en.wikipedia.org/wiki/Simulated_annealing), [Genetic Algorithms](https://en.wikipedia.org/wiki/Genetic_algorithm), [Particle Swarm Optimization](https://en.wikipedia.org/wiki/Particle_swarm_optimization), etc, which can estimate model parameters without relying on the gradient.\n",
    "\n",
    "Today, we'll analyze the [banknote authentication dataset from the UCI archive](https://archive.ics.uci.edu/dataset/267/banknote+authentication) first using [the Perceptron](https://en.wikipedia.org/wiki/Perceptron) and then [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression). Lecture notes for logistic regression can be found: [here!](https://github.com/varnerlab/CHEME-5820-Lectures-Spring-2025/blob/main/lectures/week-3/L3c/docs/Notes.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b40c86-052e-4198-a15e-1d95b870f729",
   "metadata": {},
   "source": [
    "## Setup, Data, and Prerequisites\n",
    "We set up the computational environment by including the `Include.jl` file, loading any needed resources, such as sample datasets, and setting up any required constants. The `Include.jl` file loads external packages, various functions that we will use in the exercise, and custom types to model the components of our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a4994f3-fee9-4994-9d12-5c70d826e63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"Include.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15674fd2-3644-433d-bfdb-866d30383bfe",
   "metadata": {},
   "source": [
    "### Data\n",
    "This lecture will look at a [banknote authentication dataset](https://archive.ics.uci.edu/dataset/267/banknote+authentication) for classification tasks. We'll load the banknote dataset and split it into `training` and `test` data subsets (randomly).\n",
    "* __Training data__: Training datasets are collections of labeled data used to teach machine learning models, allowing these tools to learn patterns and relationships within the data.\n",
    "* __Test data__: Test datasets, on the other hand, are separate sets of labeled data used to evaluate the performance of trained models on unseen examples, providing an unbiased assessment of the _model's generalization capabilities_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57d466e-a347-4375-b7f1-57ec00f8ae67",
   "metadata": {},
   "source": [
    "#### Banknote Authentication Dataset\n",
    "The second dataset we will explore is the [banknote authentication dataset from the UCI archive](https://archive.ics.uci.edu/dataset/267/banknote+authentication). This dataset has `1372` instances of 4 continuous features and an integer $\\{-1,1\\}$ class variable. \n",
    "* __Description__: Data were extracted from images taken from genuine and forged banknote-like specimens.  An industrial camera, usually used for print inspection, was used for digitization. The final images have 400x 400 pixels. Due to the object lens and distance to the investigated object, gray-scale pictures with a resolution of about 660 dpi were gained. Wavelet Transform tools were used to extract features from images.\n",
    "* __Features__: The data has four continuous features from each image: `variance` of the wavelet transformed image, `skewness` of the wavelet transformed image, `kurtosis` of the wavelet transformed image, and the `entropy` of the wavelet transformed image. The class is $\\{-1,1\\}$ where a class value of `-1` indicates genuine, `1` forged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b764fca8-8c28-4b9b-9ed4-55eb22810644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div style = \"float: left;\"><span>1372Ã—5 DataFrame</span></div><div style = \"float: right;\"><span style = \"font-style: italic;\">1347 rows omitted</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">variance</th><th style = \"text-align: left;\">skewness</th><th style = \"text-align: left;\">curtosis</th><th style = \"text-align: left;\">entropy</th><th style = \"text-align: left;\">class</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: right;\">3.6216</td><td style = \"text-align: right;\">8.6661</td><td style = \"text-align: right;\">-2.8073</td><td style = \"text-align: right;\">-0.44699</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: right;\">4.5459</td><td style = \"text-align: right;\">8.1674</td><td style = \"text-align: right;\">-2.4586</td><td style = \"text-align: right;\">-1.4621</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: right;\">3.866</td><td style = \"text-align: right;\">-2.6383</td><td style = \"text-align: right;\">1.9242</td><td style = \"text-align: right;\">0.10645</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">4</td><td style = \"text-align: right;\">3.4566</td><td style = \"text-align: right;\">9.5228</td><td style = \"text-align: right;\">-4.0112</td><td style = \"text-align: right;\">-3.5944</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">5</td><td style = \"text-align: right;\">0.32924</td><td style = \"text-align: right;\">-4.4552</td><td style = \"text-align: right;\">4.5718</td><td style = \"text-align: right;\">-0.9888</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">6</td><td style = \"text-align: right;\">4.3684</td><td style = \"text-align: right;\">9.6718</td><td style = \"text-align: right;\">-3.9606</td><td style = \"text-align: right;\">-3.1625</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">7</td><td style = \"text-align: right;\">3.5912</td><td style = \"text-align: right;\">3.0129</td><td style = \"text-align: right;\">0.72888</td><td style = \"text-align: right;\">0.56421</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">8</td><td style = \"text-align: right;\">2.0922</td><td style = \"text-align: right;\">-6.81</td><td style = \"text-align: right;\">8.4636</td><td style = \"text-align: right;\">-0.60216</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">9</td><td style = \"text-align: right;\">3.2032</td><td style = \"text-align: right;\">5.7588</td><td style = \"text-align: right;\">-0.75345</td><td style = \"text-align: right;\">-0.61251</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">10</td><td style = \"text-align: right;\">1.5356</td><td style = \"text-align: right;\">9.1772</td><td style = \"text-align: right;\">-2.2718</td><td style = \"text-align: right;\">-0.73535</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">11</td><td style = \"text-align: right;\">1.2247</td><td style = \"text-align: right;\">8.7779</td><td style = \"text-align: right;\">-2.2135</td><td style = \"text-align: right;\">-0.80647</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">12</td><td style = \"text-align: right;\">3.9899</td><td style = \"text-align: right;\">-2.7066</td><td style = \"text-align: right;\">2.3946</td><td style = \"text-align: right;\">0.86291</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">13</td><td style = \"text-align: right;\">1.8993</td><td style = \"text-align: right;\">7.6625</td><td style = \"text-align: right;\">0.15394</td><td style = \"text-align: right;\">-3.1108</td><td style = \"text-align: right;\">-1</td></tr><tr><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1361</td><td style = \"text-align: right;\">-0.24745</td><td style = \"text-align: right;\">1.9368</td><td style = \"text-align: right;\">-2.4697</td><td style = \"text-align: right;\">-0.80518</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1362</td><td style = \"text-align: right;\">-1.5732</td><td style = \"text-align: right;\">1.0636</td><td style = \"text-align: right;\">-0.71232</td><td style = \"text-align: right;\">-0.8388</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1363</td><td style = \"text-align: right;\">-2.1668</td><td style = \"text-align: right;\">1.5933</td><td style = \"text-align: right;\">0.045122</td><td style = \"text-align: right;\">-1.678</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1364</td><td style = \"text-align: right;\">-1.1667</td><td style = \"text-align: right;\">-1.4237</td><td style = \"text-align: right;\">2.9241</td><td style = \"text-align: right;\">0.66119</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1365</td><td style = \"text-align: right;\">-2.8391</td><td style = \"text-align: right;\">-6.63</td><td style = \"text-align: right;\">10.4849</td><td style = \"text-align: right;\">-0.42113</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1366</td><td style = \"text-align: right;\">-4.5046</td><td style = \"text-align: right;\">-5.8126</td><td style = \"text-align: right;\">10.8867</td><td style = \"text-align: right;\">-0.52846</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1367</td><td style = \"text-align: right;\">-2.41</td><td style = \"text-align: right;\">3.7433</td><td style = \"text-align: right;\">-0.40215</td><td style = \"text-align: right;\">-1.2953</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1368</td><td style = \"text-align: right;\">0.40614</td><td style = \"text-align: right;\">1.3492</td><td style = \"text-align: right;\">-1.4501</td><td style = \"text-align: right;\">-0.55949</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1369</td><td style = \"text-align: right;\">-1.3887</td><td style = \"text-align: right;\">-4.8773</td><td style = \"text-align: right;\">6.4774</td><td style = \"text-align: right;\">0.34179</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1370</td><td style = \"text-align: right;\">-3.7503</td><td style = \"text-align: right;\">-13.4586</td><td style = \"text-align: right;\">17.5932</td><td style = \"text-align: right;\">-2.7771</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1371</td><td style = \"text-align: right;\">-3.5637</td><td style = \"text-align: right;\">-8.3827</td><td style = \"text-align: right;\">12.393</td><td style = \"text-align: right;\">-1.2823</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1372</td><td style = \"text-align: right;\">-2.5419</td><td style = \"text-align: right;\">-0.65804</td><td style = \"text-align: right;\">2.6842</td><td style = \"text-align: right;\">1.1952</td><td style = \"text-align: right;\">1</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccc}\n",
       "\t& variance & skewness & curtosis & entropy & class\\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & Float64 & Float64 & Int64\\\\\n",
       "\t\\hline\n",
       "\t1 & 3.6216 & 8.6661 & -2.8073 & -0.44699 & -1 \\\\\n",
       "\t2 & 4.5459 & 8.1674 & -2.4586 & -1.4621 & -1 \\\\\n",
       "\t3 & 3.866 & -2.6383 & 1.9242 & 0.10645 & -1 \\\\\n",
       "\t4 & 3.4566 & 9.5228 & -4.0112 & -3.5944 & -1 \\\\\n",
       "\t5 & 0.32924 & -4.4552 & 4.5718 & -0.9888 & -1 \\\\\n",
       "\t6 & 4.3684 & 9.6718 & -3.9606 & -3.1625 & -1 \\\\\n",
       "\t7 & 3.5912 & 3.0129 & 0.72888 & 0.56421 & -1 \\\\\n",
       "\t8 & 2.0922 & -6.81 & 8.4636 & -0.60216 & -1 \\\\\n",
       "\t9 & 3.2032 & 5.7588 & -0.75345 & -0.61251 & -1 \\\\\n",
       "\t10 & 1.5356 & 9.1772 & -2.2718 & -0.73535 & -1 \\\\\n",
       "\t11 & 1.2247 & 8.7779 & -2.2135 & -0.80647 & -1 \\\\\n",
       "\t12 & 3.9899 & -2.7066 & 2.3946 & 0.86291 & -1 \\\\\n",
       "\t13 & 1.8993 & 7.6625 & 0.15394 & -3.1108 & -1 \\\\\n",
       "\t14 & -1.5768 & 10.843 & 2.5462 & -2.9362 & -1 \\\\\n",
       "\t15 & 3.404 & 8.7261 & -2.9915 & -0.57242 & -1 \\\\\n",
       "\t16 & 4.6765 & -3.3895 & 3.4896 & 1.4771 & -1 \\\\\n",
       "\t17 & 2.6719 & 3.0646 & 0.37158 & 0.58619 & -1 \\\\\n",
       "\t18 & 0.80355 & 2.8473 & 4.3439 & 0.6017 & -1 \\\\\n",
       "\t19 & 1.4479 & -4.8794 & 8.3428 & -2.1086 & -1 \\\\\n",
       "\t20 & 5.2423 & 11.0272 & -4.353 & -4.1013 & -1 \\\\\n",
       "\t21 & 5.7867 & 7.8902 & -2.6196 & -0.48708 & -1 \\\\\n",
       "\t22 & 0.3292 & -4.4552 & 4.5718 & -0.9888 & -1 \\\\\n",
       "\t23 & 3.9362 & 10.1622 & -3.8235 & -4.0172 & -1 \\\\\n",
       "\t24 & 0.93584 & 8.8855 & -1.6831 & -1.6599 & -1 \\\\\n",
       "\t25 & 4.4338 & 9.887 & -4.6795 & -3.7483 & -1 \\\\\n",
       "\t26 & 0.7057 & -5.4981 & 8.3368 & -2.8715 & -1 \\\\\n",
       "\t27 & 1.1432 & -3.7413 & 5.5777 & -0.63578 & -1 \\\\\n",
       "\t28 & -0.38214 & 8.3909 & 2.1624 & -3.7405 & -1 \\\\\n",
       "\t29 & 6.5633 & 9.8187 & -4.4113 & -3.2258 & -1 \\\\\n",
       "\t30 & 4.8906 & -3.3584 & 3.4202 & 1.0905 & -1 \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m1372Ã—5 DataFrame\u001b[0m\n",
       "\u001b[1m  Row \u001b[0mâ”‚\u001b[1m variance  \u001b[0m\u001b[1m skewness  \u001b[0m\u001b[1m curtosis  \u001b[0m\u001b[1m entropy   \u001b[0m\u001b[1m class \u001b[0m\n",
       "      â”‚\u001b[90m Float64   \u001b[0m\u001b[90m Float64   \u001b[0m\u001b[90m Float64   \u001b[0m\u001b[90m Float64   \u001b[0m\u001b[90m Int64 \u001b[0m\n",
       "â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
       "    1 â”‚  3.6216      8.6661   -2.8073    -0.44699      -1\n",
       "    2 â”‚  4.5459      8.1674   -2.4586    -1.4621       -1\n",
       "    3 â”‚  3.866      -2.6383    1.9242     0.10645      -1\n",
       "    4 â”‚  3.4566      9.5228   -4.0112    -3.5944       -1\n",
       "    5 â”‚  0.32924    -4.4552    4.5718    -0.9888       -1\n",
       "    6 â”‚  4.3684      9.6718   -3.9606    -3.1625       -1\n",
       "    7 â”‚  3.5912      3.0129    0.72888    0.56421      -1\n",
       "    8 â”‚  2.0922     -6.81      8.4636    -0.60216      -1\n",
       "    9 â”‚  3.2032      5.7588   -0.75345   -0.61251      -1\n",
       "   10 â”‚  1.5356      9.1772   -2.2718    -0.73535      -1\n",
       "   11 â”‚  1.2247      8.7779   -2.2135    -0.80647      -1\n",
       "  â‹®   â”‚     â‹®          â‹®          â‹®          â‹®        â‹®\n",
       " 1363 â”‚ -2.1668      1.5933    0.045122  -1.678         1\n",
       " 1364 â”‚ -1.1667     -1.4237    2.9241     0.66119       1\n",
       " 1365 â”‚ -2.8391     -6.63     10.4849    -0.42113       1\n",
       " 1366 â”‚ -4.5046     -5.8126   10.8867    -0.52846       1\n",
       " 1367 â”‚ -2.41        3.7433   -0.40215   -1.2953        1\n",
       " 1368 â”‚  0.40614     1.3492   -1.4501    -0.55949       1\n",
       " 1369 â”‚ -1.3887     -4.8773    6.4774     0.34179       1\n",
       " 1370 â”‚ -3.7503    -13.4586   17.5932    -2.7771        1\n",
       " 1371 â”‚ -3.5637     -8.3827   12.393     -1.2823        1\n",
       " 1372 â”‚ -2.5419     -0.65804   2.6842     1.1952        1\n",
       "\u001b[36m                                         1351 rows omitted\u001b[0m"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_banknote = CSV.read(joinpath(_PATH_TO_DATA, \"data-banknote-authentication.csv\"), DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cf38d63-8bbd-4d86-a979-6c1a560b0886",
   "metadata": {},
   "outputs": [],
   "source": [
    "D_banknote = Matrix(df_banknote); # get the data as a Matrix (alias for Array{Float64,2})\n",
    "number_of_training_examples_banknote = 1000; # how many training points for the banknote dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19d4d996-851f-441d-8e6a-871e5ce2d448",
   "metadata": {},
   "outputs": [],
   "source": [
    "banknote_training, banknote_test = let\n",
    "\n",
    "    number_of_features = size(D_banknote,2); # number of cols of housing data\n",
    "    number_of_examples = size(D_banknote,1); # number of rows of housing data\n",
    "    full_index_set = range(1,stop=number_of_examples,step=1) |> collect |> Set;\n",
    "    \n",
    "    # build index sets for training and testing\n",
    "    training_index_set = Set{Int64}();\n",
    "    should_stop_loop = false;\n",
    "    while (should_stop_loop == false)\n",
    "        i = rand(1:number_of_examples);\n",
    "        push!(training_index_set,i);\n",
    "\n",
    "        if (length(training_index_set) == number_of_training_examples_banknote)\n",
    "            should_stop_loop = true;\n",
    "        end\n",
    "    end\n",
    "    test_index_set = setdiff(full_index_set,training_index_set);\n",
    "\n",
    "    # build the test and train datasets -\n",
    "    banknote_training = D_banknote[training_index_set |> collect,:];\n",
    "    banknote_test = D_banknote[test_index_set |> collect,:];\n",
    "\n",
    "    # return\n",
    "    banknote_training,banknote_test\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc7b23f8-d3a9-4650-afe5-675f91162175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000Ã—5 Matrix{Float64}:\n",
       " -2.4621     2.7645   -0.62578  -2.8573     1.0\n",
       " -3.2051    -0.14279   0.97565   0.045675   1.0\n",
       "  4.0932     5.4132   -1.8219    0.23576   -1.0\n",
       "  0.11686    3.735    -4.4379   -4.3741     1.0\n",
       "  3.2403    -3.7082    5.2804    0.41291   -1.0\n",
       " -0.9607     2.6963   -3.1226   -1.3121     1.0\n",
       " -1.7064     3.3088   -2.2829   -2.1978     1.0\n",
       "  1.5799    -4.7076    7.9186   -1.5487    -1.0\n",
       " -2.9821     4.1986   -0.5898   -3.9642     1.0\n",
       " -1.7559    11.9459    3.0946   -4.8978    -1.0\n",
       " -3.0731    -0.53181   2.3877    0.77627    1.0\n",
       "  1.9572    -5.1153    8.6127   -1.4297    -1.0\n",
       " -3.7503   -13.4586   17.5932   -2.7771     1.0\n",
       "  â‹®                                        \n",
       " -3.1273    -7.1121   11.3897   -0.083634   1.0\n",
       " -3.6961   -13.6779   17.5795   -2.6181     1.0\n",
       "  1.8799     2.4707    2.4931    0.37671   -1.0\n",
       " -1.1391     1.8127    6.9144    0.70127   -1.0\n",
       "  4.3848    -3.0729    3.0423    1.2741    -1.0\n",
       "  3.6277     0.9829    0.68861   0.63403   -1.0\n",
       " -1.6514    -8.4985    9.1122    1.2379     1.0\n",
       "  3.9262     6.0299   -2.0156   -0.065531  -1.0\n",
       "  4.1665    -0.4449    0.23448   0.27843   -1.0\n",
       "  0.75896    0.29176  -1.6506    0.83834    1.0\n",
       "  5.4021     3.1039   -1.1536    1.5651    -1.0\n",
       " -1.0555     0.79459  -1.6968   -0.46768    1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "banknote_training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bba928-549c-46a0-9f58-923de6781612",
   "metadata": {},
   "source": [
    "## Method 1: Perceptron\n",
    "[The Perceptron (Rosenblatt, 1957)](https://en.wikipedia.org/wiki/Perceptron) takes the (scalar) output of a linear regression model $y_{i}\\in\\mathbb{R}$ and then transforms it using the $\\sigma(\\star) = \\texttt{sign}(\\star)$ function to a discrete set of values representing categories, e.g., $\\sigma:\\mathbb{R}\\rightarrow\\{-1,1\\}$ in the binary classification case. \n",
    "* Suppose there exists a data set\n",
    "$\\mathcal{D} = \\left\\{(\\mathbf{x}_{1},y_{1}),\\dotsc,(\\mathbf{x}_{n},y_{n})\\right\\}$ with $n$ _labeled_ examples, where each example has been labeled by an expert, i.e., a human to be in a category $y_{i}\\in\\{-1,1\\}$, given the $m$-dimensional feature vector $\\mathbf{x}_{i}\\in\\mathbb{R}^{m}$. \n",
    "* [The Perceptron](https://en.wikipedia.org/wiki/Perceptron) _incrementally_ learns a linear decision boundary between _two_ classes of possible objects (binary classification) by repeatedly processing the dataset $\\mathcal{D}$. During each pass, a regression parameter vector $\\mathbf{\\beta}$ is updated until it makes no more than a specified number of mistakes. \n",
    "\n",
    "[The Perceptron](https://en.wikipedia.org/wiki/Perceptron) computes the estimated label $\\hat{y}_{i}$ for feature vector $\\hat{\\mathbf{x}}_{i}$ using the $\\texttt{sign}:\\mathbb{R}\\to\\{-1,1\\}$ function:\n",
    "$$\n",
    "\\begin{equation*}\n",
    "    \\hat{y}_{i} = \\texttt{sign}\\left(\\hat{\\mathbf{x}}_{i}^{\\top}\\cdot\\beta\\right)\n",
    "\\end{equation*}\n",
    "$$\n",
    "where $\\beta=\\left(w_{1},\\dots,w_{n}, b\\right)$ is a column vector of (unknown) classifier parameters, $w_{j}\\in\\mathbb{R}$ corresponding to the importance of feature $j$ and $b\\in\\mathbb{R}$ is a bias parameter, the features $\\hat{\\mathbf{x}}^{\\top}_{i}=\\left(x^{(i)}_{1},\\dots,x^{(i)}_{m}, 1\\right)$ are $p = m+1$-dimensional (row) vectors (features augmented with bias term), and $\\texttt{sign}(z)$ is the function:\n",
    "$$\n",
    "\\begin{equation*}\n",
    "    \\texttt{sign}(z) = \n",
    "    \\begin{cases}\n",
    "        1 & \\text{if}~z\\geq{0}\\\\\n",
    "        -1 & \\text{if}~z<0\n",
    "    \\end{cases}\n",
    "\\end{equation*}\n",
    "$$\n",
    "__Hypothesis__: [If the dataset $\\mathcal{D}$ is linearly separable](https://github.com/varnerlab/CHEME-5820-Labs-Spring-2025/blob/main/labs/week-3/L3b/figs/Fig-LinearSeperableData-Hyperplane.pdf), the Perceptron will _incrementally_ learn a separating hyperplane in a finite number of passes through the data set $\\mathcal{D}$. However, if the [dataset $\\mathcal{D}$ is __not__ linearly separable](https://github.com/varnerlab/CHEME-5820-Labs-Spring-2025/blob/main/labs/week-3/L3b/figs/Fig-NotLinearSeperableData-Hyperplane.svg), the Perceptron may not converge. Check out a [perceptron pseudo-code here!](https://github.com/varnerlab/CHEME-5820-Lectures-Spring-2025/blob/main/lectures/week-3/L3a/docs/Notes.pdf)\n",
    "* __Training__: Our Perceptron implementation [based on pseudo-code](https://github.com/varnerlab/CHEME-5820-Lectures-Spring-2025/blob/main/lectures/week-3/L3a/docs/Notes.pdf) stores problem information in [a `MyPerceptronClassificationModel` instance, which holds the (initial) parameters and other data](src/Types.jl) required by the problem.\n",
    "* We then _learn_ the model parameters [using the `learn(...)` method](src/Compute.jl), which takes the training features array `X,` the training labels vector `y`, and the problem instance `model` and returns an updated problem instance holding the updated parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76d94e09-3dc0-4a87-9f2c-5553737ae3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped after number of iterations: 10000. We have number of errors: 11\n"
     ]
    }
   ],
   "source": [
    "model_perceptron = let\n",
    "\n",
    "    # data -\n",
    "    D = banknote_training; # What dataset are we going to use?\n",
    "    number_of_examples = size(D,1); # how many examples do we have (rows)\n",
    "    number_of_features = size(D,2); # how many features do we have (cols)?\n",
    "    X = [D[:,1:end-1] ones(number_of_examples)]; # features: need to add a 1 to each row (for bias), after removing the label\n",
    "    y = D[:,end]; # output: this is the target data (label)\n",
    "\n",
    "    # model\n",
    "    model = build(MyPerceptronClassificationModel, (\n",
    "        parameters = ones(number_of_features), # initial value for the parameters: these will be updated\n",
    "        mistakes = 0 # willing to like with m mistakes\n",
    "    ));\n",
    "\n",
    "    # train -\n",
    "    model = learn(X,y,model, maxiter = 10000, verbose = true); # this is learning the model parameters\n",
    "\n",
    "    # return -\n",
    "    model;\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f01e326d-e0e2-4dbd-9aeb-e707f6baf0d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Vector{Float64}:\n",
       " -180.25442099982783\n",
       "  -99.66765799978478\n",
       " -104.83100600015813\n",
       "   -9.344243000011186\n",
       "  168.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_perceptron.Î²"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cab8079-f9f9-4760-af9b-dc9b920287b6",
   "metadata": {},
   "source": [
    "__Inference__: Now that we have parameters estimated from the `training` data, we can use those parameters on the `test` dataset to see how well the model can differentiate between an actual banknote and a forgery on data it has never seen. We run the classification operation on the (unseen) test data [using the `classify(...)` method](src/Compute.jl). This method takes a feature array `X` and the (trained) model instance. It returns the estimated labels. \n",
    "* We store the actual (correct) label in the `y_banknote_perceptron::Array{Int64,1}` vector, while the model predicted label is stored in the `yÌ‚_banknote_perceptron::Array{Int64,1}` array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfc33d6d-be82-4adb-9622-27003684aca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "yÌ‚_banknote_perceptron,y_banknote_perceptron = let\n",
    "\n",
    "    D = banknote_test; # what dataset are going to use?\n",
    "    number_of_examples = size(D,1); # how many examples do we have (rows)\n",
    "    number_of_features = size(D,2); # how many features do we have (cols)?\n",
    "    X = [D[:,1:end-1] ones(number_of_examples)]; # features: need to add a 1 to each row (for bias), after removing the label\n",
    "    y = D[:,end]; # output: this is the *actual* target data (label)\n",
    "\n",
    "    # compute the estimated labels -\n",
    "    yÌ‚ = classify(X,model_perceptron)\n",
    "\n",
    "    # return -\n",
    "    yÌ‚,y\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03af9c4c-fd0d-43ed-84c1-bd032cde980c",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "The confusion matrix is a $2\\times{2}$ matrix that contains four entries: true positive (TP), false positive (FP), true negative (TN), and false negative (FN). [Click me for a confusion matrix schematic!](https://github.com/varnerlab/CHEME-5820-Labs-Spring-2025/blob/main/labs/week-3/L3b/figs/Fig-BinaryConfusionMatrix.pdf) The four cases are:\n",
    "* The __true positive (TP)__ case $(\\text{actual}, \\text{model}) = (+,+)$ in the confusion matrix is the number of positive examples that were correctly classified as positive.\n",
    "* The __false negative (FN)__ case $(\\text{actual}, \\text{model}) = (+,-)$ is the number of actual positive examples the model incorrectly classified as negative.\n",
    "* The __false positive (FP)__ case $(\\text{actual}, \\text{model}) = (-,+)$ is the number of actual negative examples that were incorrectly classified as positive by the model.\n",
    "* The __true negative (TN)__ case $(\\text{actual}, \\text{model}) = (-,-)$ is the number of actual negative examples that were correctly classified as negative by the model.\n",
    "\n",
    "Let's compute these four values [using the `confusion(...)` method](src/Compute.jl) and store them in the `CM_perceptron::Array{Int64,2}` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dcd9b9d7-bc50-4e8b-82ec-9b7e889733eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2Ã—2 Matrix{Int64}:\n",
       " 164    5\n",
       "   1  202"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CM_perceptron = confusion(y_banknote_perceptron, yÌ‚_banknote_perceptron) # call with the percepton values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce233c6-189b-4cfc-9c61-8cab910ab1c8",
   "metadata": {},
   "source": [
    "Let's compute the overall error rate for the perceptron using [the confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix). The [`confusion(...)` method](src/Compute.jl) takes the actual labels and the computed labels and returns the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ae430ba-accc-4fbd-8512-d74970303398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction correct: 0.9838709677419355 Fraction incorrect 0.016129032258064502\n"
     ]
    }
   ],
   "source": [
    "number_of_test_banknotes = length(y_banknote_perceptron);\n",
    "correct_prediction_perceptron = CM_perceptron[1,1] + CM_perceptron[2,2];\n",
    "(correct_prediction_perceptron/number_of_test_banknotes) |> f-> println(\"Fraction correct: $(f) Fraction incorrect $(1-f)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21d5be8-d19d-47cb-89f7-92989b58f6ad",
   "metadata": {},
   "source": [
    "## Method 2: Logistic Regression Model\n",
    "[Logistic regression](https://en.wikipedia.org/wiki/Logistic_regression) is a method for binary classification problems, where the dependent variable (label) is a binary categorical variable (e.g., $\\pm{1}$, etc.), and the independent variables (features) are continuous or categorical variables. Unlike the Perceptron model, which outputs the class label directly, logistic regression models the _probability_ that a given input belongs to a particular class based on the input features, \n",
    "\n",
    "### Model\n",
    "Suppose we have a dataset $\\mathcal{D} = \\left\\{(\\mathbf{x}_{i},y_{i}) \\mid i = 1,2,\\dots,n\\right\\}$ where the features $\\mathbf{x}\\in\\mathbb{R}^{m}$ are $m$-dimensional vectors composed of continuous or categorical variables, and $y\\in\\mathbb{R}$ is a scalar label, e.g., $y\\in\\left\\{-1,1\\right\\}$.\n",
    "The logistic regression problem attempts to estimate the parameters $\\theta\\in\\mathbb{R}^{p}$ (where $p=m+1$) of the conditional probability $P_{\\theta}(y|\\hat{\\mathbf{x}})$, i.e., the probability that a particular label is observed given the augmented feature vector $\\hat{\\mathbf{x}} = \\left(x_{1},x_{2},\\dots,x_{m},1\\right)$. We model this probability using [the logistic function](https://en.wikipedia.org/wiki/Logistic_function):\n",
    "$$\n",
    "\\begin{equation}\n",
    "P_{\\theta}(y|\\hat{\\mathbf{x}}) = \\frac{1}{1 + e^{-y\\cdot\\left(\\hat{\\mathbf{x}}^{\\top}\\theta\\right)}}\n",
    "\\end{equation}\n",
    "$$\n",
    "The challenge is to estimate the parameters $\\theta\\in\\mathbb{R}^{p}$. One (standard) way we do this is to minimize the negative log-likelihood: function $\\mathcal{L}(\\theta) = -\\log{L}(\\theta)$ (see the [notes](https://github.com/varnerlab/CHEME-5820-Lectures-Spring-2025/blob/main/lectures/week-3/L3c/docs/Notes.pdf) for a more detailed discussion):\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\theta^{\\star}  = \\arg\\min_{\\theta}\\left[\\sum_{i=1}^{n}\\log\\left(1 + e^{-y_{i}\\cdot\\left(\\hat{\\mathbf{x}}^{\\top}_{i}\\theta\\right)}\\right)\\right]\n",
    "\\end{equation}\n",
    "$$\n",
    "Unfortunately, this problem has no closed-form analytical solution. Thus, we have to use some numerical technique, e.g., [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent), to estimate an approximate value for the parameters, i.e., $\\theta^{\\star}\\sim\\hat{\\theta}^{\\star}$.\n",
    "\n",
    "### Gradient descent\n",
    "Gradient descent is a numerical search algorithm that minimizes a function by iteratively adjusting the parameters in the opposite direction of the gradient. Suppose there exists an objective function, e.g., the negative log-likelihood $\\mathcal{L}(\\theta)$ that we want to minimize with respect to parameters $\\theta\\in\\mathbb{R}^{p}$. We assume $\\mathcal{L}(\\theta)$ is _at least once differentiable_ with respect to the parameters, i.e., we can compute the gradient $\\nabla_{\\theta}{\\mathcal{L}}(\\theta)$. The gradient points in the direction of the steepest increase of the function. Thus, we can iteratively update the parameters to minimize the objective function using the update rule:\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\theta_{k+1} = \\theta_{k} - \\alpha(k)\\cdot\\nabla_{\\theta}\\mathcal{L}(\\theta_{k})\\quad\\text{where}{~k = 0,1,2,\\dots}\n",
    "\\end{equation*}\n",
    "$$\n",
    "where $k$ denotes the iteration index, and $\\nabla_{\\theta}\\mathcal{L}(\\theta)$ is the gradient of the negative log-likelihood function with respect to the parameters $\\theta$.\n",
    "* __What is $\\alpha(k)$?__ The (hyper) parameter $\\alpha(k)>0$ is the _learning rate_ which can be a function of the iteration count $k$. This is a user-adjustable parameter, and we'll assume it's constant for today.\n",
    "* __Stopping?__ Gradient descent will continue to iterate until a stopping criterion is met, i.e., $\\lVert\\theta_{k+1} - \\theta_{k}\\rVert\\leq\\epsilon$ or the maximum number of iterations is reached, or some other stopping criterion is met, i.e., the gradient is small at the current iteration $\\lVert\\nabla_{\\theta}\\mathcal{L}(\\theta_{k})\\rVert\\leq\\epsilon$.\n",
    "\n",
    "Pusedocode for a naive gradient descent algorithm (for a fixed learning rate) is shown in the [lecture notes](https://github.com/varnerlab/CHEME-5820-Lectures-Spring-2025/blob/main/lectures/week-3/L3c/docs/Notes.pdf). If you don't like computing derivatives (who does, am I right?) there are alternatives:\n",
    "* __Alternatives to gradient descent__ include heuristic optimization algorithms such as the [Nelder-Mead Simplex Algorithm](https://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method), [Simulated Annealing](https://en.wikipedia.org/wiki/Simulated_annealing), [Genetic Algorithms](https://en.wikipedia.org/wiki/Genetic_algorithm), [Particle Swarm Optimization](https://en.wikipedia.org/wiki/Particle_swarm_optimization), etc, which can estimate model parameters without relying on the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4202415-82d9-4660-90c1-78f9957c6d9f",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "We implemented [the `MyLogisticRegressionClassificationModel` type](src/Types.jl), which contains data required to solve the logistic regression problem, i.e., parameters, the learning rate, a stopping tolerance parameter $\\epsilon$, and a loss (objective) function that we want to minimize. \n",
    "* __Technical note__: In this implementation, we approximated the gradient calculation using [a forward finite difference](https://en.wikipedia.org/wiki/Finite_difference). In general, this is not a great idea. This is one of my super pet peeves of gradient descent; computing the gradient is usually a hassle, and we do a bunch of function evaluations to get a good approximation of the gradient. However, finite difference is easy to implement.\n",
    "* In the code block below, we [build a `model::MyLogisticRegressionClassificationModel` instance using a `build(...)` method](src/Factory.jl). The model instance initially has a random guess for the classifier parameters. We use gradient descent to refine that guess [using the `learn(...)` method](src/Compute.jl), which returns an updated model instance (with the best parameters that we found so far). We return the updated model instance and save it in the `model_logistic::MyLogisticRegressionClassificationModel` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8e0bf97-69d1-46ad-86ab-2d5131738c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped after number of iterations: 10001. We have error: 0.025613373045017016\n"
     ]
    }
   ],
   "source": [
    "model_logistic = let\n",
    "\n",
    "    # data -\n",
    "    D = banknote_training; # What dataset are we going to use?\n",
    "    number_of_examples = size(D,1); # how many examples do we have (rows)\n",
    "    number_of_features = size(D,2); # how many features do we have (cols)?\n",
    "    X = [D[:,1:end-1] ones(number_of_examples)]; # features: need to add a 1 to each row (for bias), after removing the label\n",
    "    y = D[:,end]; # output: this is the target data (label)\n",
    "\n",
    "    # model\n",
    "    model = build(MyLogisticRegressionClassificationModel, (\n",
    "        parameters = 0.01*ones(number_of_features), # initial value for the parameters: these will be updated\n",
    "        learning_rate = 0.005, # you pick this\n",
    "        Ïµ = 1e-4, # you pick this (this is also the step size for the fd approx to the gradient)\n",
    "        loss_function = (x,y,Î¸) -> log10(1+exp(-y*(dot(x,Î¸)))) # what??!? Wow, that is nice. Yes, we can pass functions as args!\n",
    "    ));\n",
    "\n",
    "    # train -\n",
    "    model = learn(X,y,model, maxiter = 10000, verbose = true); # this is learning the model parameters\n",
    "\n",
    "    # return -\n",
    "    model;\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f6b0d79e-e9d4-4f4e-a6f4-2ed540a6802e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Vector{Float64}:\n",
       " -8.382256499125878\n",
       " -4.262197574175405\n",
       " -5.448355414826784\n",
       " -0.5491634846777512\n",
       "  7.544177029375256"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_logistic.Î²"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bfff6f-5820-4ac7-8173-515d7b16fa4b",
   "metadata": {},
   "source": [
    "Let's use the updated `model_logistic::MyLogisticRegressionClassificationModel` instance (that has learned some parameters from the `training` data) and test how well we classify data that we have never seen, i.e., how well we classify the `test` dataset.\n",
    "\n",
    "__Inference__: We run the classification operation on the (unseen) test data [using the `classify(...)` method](src/Compute.jl). This method takes a feature array `X` and the (trained) model instance. It returns the probability of a label in the `P::Array{Float64,2}` array (which is different than the Perceptron). Each row of `P` corresponds to a test instance, in which each column corresponds to a label, in the case `1` and `-1`.\n",
    "* We store the actual (correct) label in the `y_banknote_logistic::Array{Int64,1}` vector. We compute the predicted label for each test instance by finding the highest probability column. We store the predicted labels in the `yÌ‚_banknote_logistic::Array{Int64,1}` vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3bb9d31e-5743-4d8c-967f-cc50a0c61303",
   "metadata": {},
   "outputs": [],
   "source": [
    "yÌ‚_banknote_logistic,y_banknote_logistic, P = let\n",
    "\n",
    "    D = banknote_test; # What dataset are you going to use?\n",
    "    number_of_examples = size(D,1); # how many examples do we have (rows)\n",
    "    number_of_features = size(D,2); # how many features do we have (cols)?\n",
    "    X = [D[:,1:end-1] ones(number_of_examples)]; # features: need to add a 1 to each row (for bias), after removing the label\n",
    "    y = D[:,end]; # output: this is the *actual* target data (label)\n",
    "\n",
    "    # compute the estimated labels -\n",
    "    P = classify(X,model_logistic) # logistic regression returns a x x 2 array holding the probability\n",
    "\n",
    "    # convert the probability to a choice ... for each row (test instance), compute the col with the highest probability\n",
    "    yÌ‚ = zeros(number_of_examples);\n",
    "    for i âˆˆ 1:number_of_examples\n",
    "        a = argmax(P[i,:]); # col index with largest value\n",
    "        yÌ‚[i] = 1; # default\n",
    "        if (a == 2)\n",
    "            yÌ‚[i] = -1;\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # return -\n",
    "    yÌ‚, y, P\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1975da7e-d14d-4861-84d7-e3464807c3db",
   "metadata": {},
   "source": [
    "__Performance__: Once we have has converged, we can evaluate the binary classifier's performance using various metrics. The central idea is to compare the predicted labels $\\hat{y}_{i}$ to the actual labels $y_{i}$ in the `test` dataset. \n",
    "Various metrics can be used to evaluate the performance of a binary classifier, but they all start with computing [the confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix).\n",
    "* Let's compute confusion matrix [using the `confusion(...)` method](src/Compute.jl) and store it in the `CM_logistic::Array{Int64,2}` variable. The [`confusion(...)` method](src/Compute.jl) takes the actual labels and the computed labels and returns the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e55d32d6-ed9a-4f7f-b5ce-a6bd3d00e80d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2Ã—2 Matrix{Int64}:\n",
       " 167    2\n",
       "   1  202"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CM_logistic = confusion(y_banknote_logistic, yÌ‚_banknote_logistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89532332-88e8-4110-92a9-66f6c640a465",
   "metadata": {},
   "source": [
    "Let's compute the overall error rate for the logistic regression using [the confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fa0d49e0-9d8d-4727-a155-7bbfe48e559c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction correct: 0.9919354838709677 Fraction incorrect 0.008064516129032251\n"
     ]
    }
   ],
   "source": [
    "number_of_test_banknotes = length(y_banknote_perceptron);\n",
    "correct_prediction_logistic = CM_logistic[1,1] + CM_logistic[2,2];\n",
    "(correct_prediction_logistic/number_of_test_banknotes) |> f-> println(\"Fraction correct: $(f) Fraction incorrect $(1-f)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c45890-b72e-4f47-860d-6d1f8f6b5a78",
   "metadata": {},
   "source": [
    "## Today?\n",
    "That's a wrap! What are some of the interesting things we discussed today?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.3",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
