\documentclass{article}[11pt]
\usepackage{fullpage,graphicx, setspace, latexsym, cite,amsmath,amssymb,xcolor,subfigure}
%\usepackage{epstopdf}
%\DeclareGraphicsExtensions{.pdf,.eps,.png,.jpg,.mps} 
\usepackage{amssymb} %maths
\usepackage{amsmath} %maths
\usepackage{amsthm, comment}
\usepackage[round,comma,sort,numbers]{natbib}

% \bibliographystyle{plain}
\bibliographystyle{plos2015}

\newtheorem{theorem}{Theorem}
\newtheorem{prop}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{defn}{Definition}
\newtheorem{ex}{Example}
\usepackage{float}

\newcommand*{\underuparrow}[1]{\underset{\uparrow}{#1}}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage[dvipsnames]{xcolor}
\usepackage{algorithmicx}
\usepackage{algorithm} %http://ctan.org/pkg/algorithms
\usepackage{algpseudocode} %http://ctan.org/pkg/algorithmicx
\usepackage{enumitem}
\usepackage{simplemargins}
\usepackage{hyperref}

\usepackage{mdframed}
\definecolor{lgray}{rgb}{0.92,0.92,0.92}
\definecolor{lsalmon}{rgb}{0.9921568627450981,0.9411764705882353, 0.9254901960784314}

\renewcommand{\bibnumfmt}[1]{#1.}
\setlist{noitemsep} % or \setlist{noitemsep} to leave space around whole list
\setallmargins{1in}
\linespread{1.1}

\newcommand{\brows}[1]{%
  \begin{bmatrix}
  \begin{array}{@{\protect\rotvert\;}c@{\;\protect\rotvert}}
  #1
  \end{array}
  \end{bmatrix}
}
\newcommand{\rotvert}{\rotatebox[origin=c]{90}{$\vert$}}
\newcommand{\rowsvdots}{\multicolumn{1}{@{}c@{}}{\vdots}}


\def\R{\mathbb{R}}
\def\Eps{\mathcal{E}}
\def\E{\mathbb{E}}
\def\V{\mathbb{V}}
\def\F{\mathcal{F}}
\def\G{\mathcal{G}}
\def\H{\mathcal{H}}
\def\S{\mathcal{S}}
\def\D{\mathcal{D}}
\def\P{\mathbb{P}}
\def\1{\mathbf{1}}
\def\n{\nappa}
\def\h{\mathbf{w}}
\def\v{\mathbf{v}}
\def\x{\mathbf{x}}
\def\X{\mathcal{X}}
\def\Y{\mathcal{Y}}
\def\eps{\epsilon}
\def\y{\mathbf{y}}
\def\e{\mathbf{e}}
\newcommand{\norm}[1]{\left|\left|#1\right|\right|}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\lecture}[4]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   % \setcounter{lecnum}{#1}
   \setcounter{page}{1}
   \setlength{\headsep}{10mm}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf CHEME 5820: Machine Learning for Engineers
   \hfill Spring 2025} }
       \vspace{4mm}
       \hbox to 6.28in { {\Large \hfill Lecture #1: #2  \hfill} }
       \vspace{2mm}
       \hbox to 6.28in { {\it Lecturer: #3 \hfill #4} }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{Lecture #1: #2}{Lecture #1: #2}

   \noindent{\bf Disclaimer}: {\it These notes have not been subjected to the
   usual scrutiny reserved for formal publications. }
   \vspace*{4mm}
}

\begin{document}
\lecture{5c}{Support Vector Machines (SVMs)}{Jeffrey Varner}{}

\begin{mdframed}[backgroundcolor=lgray]
The key concepts covered in this lecture include:
\begin{itemize}[leftmargin=16pt]
\item{Support Vector Machines (SVMs)}
\end{itemize}
\end{mdframed}

\section{Introduction}
In the previous lectures, we've discussed several binary classification algorithms, 
including the perceptron, logistic regression, K-nearest neighbors, and the concept of Kernel functions and the kerner trick.
In this lecture, we will conclude our dicussion (for now) of classification algorithms by introducing support vector machines (SVMs), 
a powerful and versatile machine learning algorithm that can be used for both classification and regression tasks. 
SVMs are particularly well-suited for binary classification problems, where the goal is to separate data points into two classes using a hyperplane, e.g., like the perceptron.
However, SVMs are based on the concept of finding an \textit{optimal seperating hyperplane} (not just some seperating hyperplane) 
that maximizes the margin between the two classes.

\section{Support Vector Machines (SVMs)}
Support vector machines (SVMs) are a class of supervised learning algorithms that can be used for both classification and regression tasks.
In many ways, SVMs are similar to the perceptron and logistic regression, as they are also based on the concept (in the simplest case) of finding a hyperplane that separates data points into different classes.
Suppose, we have dataset $\mathcal{D} = \{(\hat{\mathbf{x}}_{i}, y_{i}) \mid i = 1,2,\dots,n\}$, where $\hat{\mathbf{x}}_i \in \R^p$ is an \textit{augmented} feature vector ($m$ features with additional 1 last entry to model the bias) and $y_i \in \{-1, 1\}$ is the corresponding class label.
Then, the goal of SVMs is to find the hyperplane $\mathcal{H} = \{\hat{\mathbf{x}} \mid \left<\hat{\mathbf{x}},\theta\right> = 0\}$ that separates the data points into two classes (those points above the hyperplane, and those points below the hyperplane), 
where $\theta \in \R^{p}$ ($p=m+1$) is the normal vector to the hyperplane, or alternatively, the parameters of the model that we need to estimate.
So far, this is similar to the perceptron and logistic regression, but the key difference is that SVMs aim to find the \textit{optimal} hyperplane in some sense. 
Let's explore the notion of an \textit{optimal hyperplane} in more detail.

\subsection{Maximum Margin Classifier}
Suppose we have found a hyperplane $\mathcal{H}$ that separates the data points into two classes.
Then, the margin of the hyperplane is defined as the distance $\gamma$ between the hyperplane and the closest data point from either class.
Thus, the margin is a measure of how well the hyperplane separates the two classes, and the goal of a maximizing margin SVM classifier is to find the hyperplane that maximizes the margin.
Let's develop a model for the margin of the seperating hyperplane.

Consider some feature vector $\hat{\mathbf{x}} \in \R^{m}$. Let $\mathbf{d}$ denote the vector from the hyperplane $\mathcal{H}$ to the feature vector $\hat{\mathbf{x}}$.
Finally, let the point $\mathbf{p}$ be the projection of $\hat{\mathbf{x}}$ onto the hyperplane $\mathcal{H}$.
Because the vector $\mathbf{d}$ is orthogonal to the hyperplane $\mathcal{H}$, we can write $\mathbf{d} = \hat{\mathbf{x}} - \mathbf{p}$.
Futher, the vector $\mathbf{d}$ can be written as some scalar multiple of the normal vector $\theta$, i.e., $\mathbf{d} = \lambda \theta$.
Then we can find the value of $\lambda$ by taking the dot product of $\mathbf{d}$ with the normal vector $\theta$:
\begin{align*}
    \hat{\mathbf{p}} & = \hat{\mathbf{x}} - \mathbf{d}\quad\mid\,\text{take dot product with $\theta$} \\
    \left<\hat{\mathbf{p}},\theta\right> & = \left<\hat{\mathbf{x}},\theta\right> - \left<\mathbf{d},\theta\right> = 0\quad\mid\,\text{substitute $\mathbf{d} = \lambda \theta$} \\
    \left<\hat{\mathbf{p}},\theta\right> & = \left<\hat{\mathbf{x}},\theta\right> - \lambda \left<\theta,\theta\right> = 0\quad\mid\,\text{solve for $\lambda$} \\
    \lambda & = \frac{\left<\hat{\mathbf{x}},\theta\right>}{\left<\theta,\theta\right>}
\end{align*}
We can now find the length of the vector $\mathbf{d}$ by computing $\norm{\mathbf{d}}_{2}$:
\begin{align*}
    \norm{\mathbf{d}}_{2} & = \sqrt{\mathbf{d}^{\top}\mathbf{d}}\quad\mid\,\text{substitute $\mathbf{d} = \lambda \theta$} \\
    & = \sqrt{\lambda^{2}\theta^{\top}\theta}\quad\mid\,\text{substitute $\lambda = \frac{\left<\hat{\mathbf{x}},\theta\right>}{\left<\theta,\theta\right>}$} \\
    & = \sqrt{\frac{\left<\hat{\mathbf{x}},\theta\right>^{2}}{\left<\theta,\theta\right>^{2}}\,\theta^{\top}\theta}\quad\mid\, \text{substitute $\left<\theta,\theta\right> = \theta^{\top}\theta$ and simplify} \\
    & = \frac{\left<\hat{\mathbf{x}},\theta\right>}{\norm{\theta}_{2}}\quad\mid\,\text{where $\norm{\theta}_{2} = \sqrt{\left<\theta,\theta\right>}$}
\end{align*}
The length of the distance vector $\mathbf{d}$ gives us the distance from a feature vector $\hat{\mathbf{x}}$ to the hyperplane $\mathcal{H}$.
Thus, we can define the margin $\gamma_{\theta}$ of the hyperplane $\mathcal{H}$ as the distance between the hyperplane and the closest data point (Defn. \ref{defn:margin}).
\begin{defn}\label{defn:margin}
    The margin $\gamma_{\theta}$ of a hyperplane $\mathcal{H}$ is given by the distance between the hyperplane and the closest data point:
    \begin{align*}
        \gamma_{\theta} & = \min_{i}\left\{\frac{|\left<\hat{\mathbf{x}}_{i},\theta\right>|}{\norm{\theta}_{2}}\right\}
    \end{align*}
where we use the absolute value to account for the fact that the distance can be positive or negative, i.e., the data point can be on either side of the hyperplane.
The margin and the hyperplane are scale invaraient, i.e., $\gamma_{\theta} = \gamma_{c\theta}$ for any $c \neq 0$.
\end{defn}

Now that have a model for the margin of the hyperplane, we can define the problem of finding the optimal hyperplane as an optimization problem.
Ideally, we would like to estimate the parameters $\theta$ of the hyperplane that maximize the margin $\gamma_{\theta}$, i.e., the distance between the hyperplane and the closest data point is maxzimized:
\begin{equation*}\label{eq:max-margin}
    \max_{\theta}\gamma_{\theta}\quad\text{subject to}\quad y_{i}\left<\hat{\mathbf{x}}_{i},\theta\right> \geq 0\quad\forall i
\end{equation*}
We can substitute the definition of the margin $\gamma_{\theta}$ into the optimization problem, which gives as a new objective function:
\begin{equation*}
    \max_{\theta}\left[\min_{i}\left\{\frac{|\left<\hat{\mathbf{x}}_{i},\theta\right>|}{\norm{\theta}_{2}}\right\}\right]\quad\text{subject to}\quad y_{i}\left<\hat{\mathbf{x}}_{i},\theta\right> \geq 0\quad\forall i
\end{equation*}
However, we can factor out the norm of the normal vector $\theta$ from the objective function, which simplifies the optimization problem:
\begin{equation*}
    \max_{\theta}\frac{1}{\norm{\theta}_{2}}\left[\min_{i}\left\{|\left<\hat{\mathbf{x}}_{i},\theta\right>|\right\}\right]\quad\text{subject to}\quad y_{i}\left<\hat{\mathbf{x}}_{i},\theta\right> \geq 0\quad\forall i
\end{equation*}
Finally, we can use the scale invariance of the margin and the hyperplane,  using the fact that maximing the inverse of a function is equivalent to minimizing the function, 
to simplify the optimization problem (Defn. \ref{defn:max-margin}):
\begin{defn}\label{defn:max-margin}
    The maximum margin classifier problem is defined as the optimization problem:
    \begin{align*}
    \text{find}\qquad & \min_{\theta}\norm{\theta}_{2}^{2}\\
    \text{subject to}\qquad & y_{i}\left<\hat{\mathbf{x}}_{i},\theta\right> \geq 0\quad\forall i\\
    \text{subject to}\qquad & \min_{i}\left\{|\left<\hat{\mathbf{x}}_{i},\theta\right>|\right\} = 1
    \end{align*}
where the objective function is the squared norm of the normal vector $\theta$, and the constraints ensure that the data points are correctly classified by the hyperplane, 
and the margin of the hyperplane is equal to 1.
\end{defn}

\section{Summary and Conclusions}
In this lecture, we introduced support vector machines (SVMs), a powerful and versatile machine learning algorithm that can be used for both classification and regression tasks.
SVMs are particularly well-suited for binary classification problems, where the goal is to separate data points into two classes using an optimal hyperplane.

\bibliography{References-L3c.bib}

\end{document}


