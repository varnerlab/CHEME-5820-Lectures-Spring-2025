%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Jeffrey Varner at 2025-01-10 17:30:54 -0500 


%% Saved with string encoding Unicode (UTF-8) 



@article{Perceptron1960,
	author = {Rosenblatt, Frank},
	date-added = {2025-01-10 17:29:07 -0500},
	date-modified = {2025-01-10 17:30:54 -0500},
	doi = {10.1109/JRPROC.1960.287598},
	journal = {Proceedings of the IRE},
	keywords = {Computational modeling;Brain modeling;Laboratories;Computer simulation;Predictive models;Neurons;Pattern recognition;Digital simulation;Mathematical analysis;Retina},
	number = {3},
	pages = {301-309},
	title = {Perceptron Simulation Experiments},
	volume = {48},
	year = {1960},
	bdsk-url-1 = {https://doi.org/10.1109/JRPROC.1960.287598}}

@article{NelderMead-1965,
	abstract = {A method is described for the minimization of a function of n variables, which depends on the comparison of function values at the (n + 1) vertices of a general simplex, followed by the replacement of the vertex with the highest value by another point. The simplex adapts itself to the local landscape, and contracts on to the final minimum. The method is shown to be effective and computationally compact. A procedure is given for the estimation of the Hessian matrix in the neighbourhood of the minimum, needed in statistical estimation problems.},
	author = {Nelder, J. A. and Mead, R.},
	date-added = {2025-01-10 17:10:44 -0500},
	date-modified = {2025-01-10 17:11:03 -0500},
	doi = {10.1093/comjnl/7.4.308},
	eprint = {https://academic.oup.com/comjnl/article-pdf/7/4/308/1013182/7-4-308.pdf},
	issn = {0010-4620},
	journal = {The Computer Journal},
	month = {01},
	number = {4},
	pages = {308-313},
	title = {{A Simplex Method for Function Minimization}},
	url = {https://doi.org/10.1093/comjnl/7.4.308},
	volume = {7},
	year = {1965},
	bdsk-url-1 = {https://doi.org/10.1093/comjnl/7.4.308}}

@misc{ADAM-2014,
	archiveprefix = {arXiv},
	author = {Diederik P. Kingma and Jimmy Ba},
	date-added = {2025-01-10 16:42:32 -0500},
	date-modified = {2025-01-10 16:43:03 -0500},
	eprint = {1412.6980},
	primaryclass = {cs.LG},
	title = {Adam: A Method for Stochastic Optimization},
	url = {https://arxiv.org/abs/1412.6980},
	year = {2017},
	bdsk-url-1 = {https://arxiv.org/abs/1412.6980}}

@article{ADAGrad2011,
	abstract = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.},
	author = {Duchi, John and Hazan, Elad and Singer, Yoram},
	date-added = {2025-01-10 16:38:50 -0500},
	date-modified = {2025-01-10 16:39:21 -0500},
	issn = {1532-4435},
	issue_date = {2/1/2011},
	journal = {J. Mach. Learn. Res.},
	month = jul,
	number = {null},
	numpages = {39},
	pages = {2121--2159},
	publisher = {JMLR.org},
	title = {{Adaptive Subgradient Methods for Online Learning and Stochastic Optimization}},
	volume = {12},
	year = {2011}}

@inproceedings{PSO1995,
	author = {Kennedy, J. and Eberhart, R.},
	booktitle = {Proceedings of ICNN'95 - International Conference on Neural Networks},
	date-added = {2025-01-10 15:11:30 -0500},
	date-modified = {2025-01-10 15:11:46 -0500},
	doi = {10.1109/ICNN.1995.488968},
	keywords = {Particle swarm optimization;Birds;Educational institutions;Marine animals;Testing;Humans;Genetic algorithms;Optimization methods;Artificial neural networks;Performance evaluation},
	pages = {1942-1948 vol.4},
	title = {Particle swarm optimization},
	volume = {4},
	year = {1995},
	bdsk-url-1 = {https://doi.org/10.1109/ICNN.1995.488968}}

@book{Holland:1975aa,
	address = {Ann Arbor},
	annote = {LDR    00944cam  2200253 i 4500
001    318240
005    20150820130553.0
008    750717s1975    miua     b    001 0 eng  
035    $9(DLC)   74078988
906    $a7$bcbc$corignew$d2$eopcn$f19$gy-gencatlg
010    $a   74078988 
020    $a0472084607
040    $aDLC$cDLC$dDLC
050 00 $aQH546$b.H64 1975
082 00 $a574.5
100 1  $aHolland, John H.$q(John Henry),$d1929-2015.
245 10 $aAdaptation in natural and artificial systems :$ban introductory analysis with applications to biology, control, and artificial intelligence /$cby John H. Holland.
260    $aAnn Arbor :$bUniversity of Michigan Press,$c[1975]
300    $aviii, 183 p. :$bill. ;$c25 cm.
504    $aBibliography: p. 175-177.
500    $aIncludes index.
650  0 $aAdaptation (Biology)$xMathematical models.
985    $fea27 2-3-86
991    $bc-GenColl$hQH546$i.H64 1975$p00004559903$tCopy 1$wBOOKS
},
	author = {Holland, John H},
	call-number = {QH546},
	date-added = {2025-01-10 15:07:05 -0500},
	date-modified = {2025-01-10 15:07:05 -0500},
	dewey-call-number = {574.5},
	genre = {Adaptation (Biology)},
	isbn = {0472084607},
	library-id = {74078988},
	publisher = {University of Michigan Press},
	title = {Adaptation in natural and artificial systems: an introductory analysis with applications to biology, control, and artificial intelligence},
	year = {1975}}

@article{Kirkpatrick:1983aa,
	abstract = {There is a deep and useful connection between statistical mechanics (the behavior of systems with many degrees of freedom in thermal equilibrium at a finite temperature) and multivariate or combinatorial optimization (finding the minimum of a given function depending on many parameters). A detailed analogy with annealing in solids provides a framework for optimization of the properties of very large and complex systems. This connection to statistical mechanics exposes new information and provides an unfamiliar perspective on traditional optimization problems and methods.},
	author = {Kirkpatrick, S and Gelatt, Jr, C D and Vecchi, M P},
	date-added = {2025-01-10 15:01:27 -0500},
	date-modified = {2025-01-10 15:01:27 -0500},
	doi = {10.1126/science.220.4598.671},
	journal = {Science},
	journal-full = {Science (New York, N.Y.)},
	month = {May},
	number = {4598},
	pages = {671-80},
	pmid = {17813860},
	pst = {ppublish},
	title = {Optimization by simulated annealing},
	volume = {220},
	year = {1983},
	bdsk-url-1 = {https://doi.org/10.1126/science.220.4598.671}}

@article{SG2019,
	abstract = {BACKGROUND: Following visible successes on a wide range of predictive tasks, machine learning techniques are attracting substantial interest from medical researchers and clinicians. We address the need for capacity development in this area by providing a conceptual introduction to machine learning alongside a practical guide to developing and evaluating predictive algorithms using freely-available open source software and public domain data. METHODS: We demonstrate the use of machine learning techniques by developing three predictive models for cancer diagnosis using descriptions of nuclei sampled from breast masses. These algorithms include regularized General Linear Model regression (GLMs), Support Vector Machines (SVMs) with a radial basis function kernel, and single-layer Artificial Neural Networks. The publicly-available dataset describing the breast mass samples (N=683) was randomly split into evaluation (n=456) and validation (n=227) samples. We trained algorithms on data from the evaluation sample before they were used to predict the diagnostic outcome in the validation dataset. We compared the predictions made on the validation datasets with the real-world diagnostic decisions to calculate the accuracy, sensitivity, and specificity of the three models. We explored the use of averaging and voting ensembles to improve predictive performance. We provide a step-by-step guide to developing algorithms using the open-source R statistical programming environment. RESULTS: The trained algorithms were able to classify cell nuclei with high accuracy (.94 -.96), sensitivity (.97 -.99), and specificity (.85 -.94). Maximum accuracy (.96) and area under the curve (.97) was achieved using the SVM algorithm. Prediction performance increased marginally (accuracy =.97, sensitivity =.99, specificity =.95) when algorithms were arranged into a voting ensemble. CONCLUSIONS: We use a straightforward example to demonstrate the theory and practice of machine learning for clinicians and medical researchers. The principals which we demonstrate here can be readily applied to other complex tasks including natural language processing and image recognition.},
	address = {Department of Engineering, University of Cambridge, Trumpington Street, Cambridge, CB2 1PZ, UK.; Department of Surgery, Harvard Medical School, 25 Shattuck Street, Boston, 01225, Massachusetts, USA. cgibbons2@bwh.harvard.edu.; Department of Surgery, Brigham and Women's Hospital, 75 Francis Street, Boston, 01225, Massachusetts, USA. cgibbons2@bwh.harvard.edu.; University of Cambridge Psychometrics Centre, Trumpington Street, Cambridge, CB2 1AG, UK. cgibbons2@bwh.harvard.edu.},
	auid = {ORCID: 0000-0002-4732-7305},
	author = {Sidey-Gibbons, Jenni A M and Sidey-Gibbons, Chris J},
	cois = {ETHICS APPROVAL AND CONSENT TO PARTICIPATE: In this manuscript we use de-identified data from a public repository [17]. The data are included on the BMC Med Res Method website. As such, ethical approval was not required. CONSENT FOR PUBLICATION: All contributing parties consent for the publication of this work. COMPETING INTERESTS: The authors report no competing interests relating to this work. PUBLISHER'S NOTE: Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.},
	crdt = {2019/03/21 06:00},
	date = {2019 Mar 19},
	date-added = {2025-01-10 14:50:34 -0500},
	date-modified = {2025-01-10 14:50:34 -0500},
	dcom = {20200210},
	dep = {20190319},
	doi = {10.1186/s12874-019-0681-4},
	edat = {2019/03/21 06:00},
	gr = {CDA 10-019/ImVA/Intramural VA/United States; CDF-2017-10-019/DH{\_}/Department of Health/United Kingdom},
	issn = {1471-2288 (Electronic); 1471-2288 (Linking)},
	jid = {100968545},
	journal = {BMC Med Res Methodol},
	jt = {BMC medical research methodology},
	keywords = {Classification; Computer-assisted; Decision making; Diagnosis; Medical informatics; Programming languages; Supervised machine learning},
	language = {eng},
	lid = {10.1186/s12874-019-0681-4 {$[$}doi{$]$}; 64},
	lr = {20240411},
	mh = {*Algorithms; Breast Neoplasms/*diagnosis; Diagnosis, Computer-Assisted/*methods; Female; Humans; *Machine Learning; *Neural Networks, Computer; Sensitivity and Specificity; Software; *Support Vector Machine},
	mhda = {2020/02/11 06:00},
	month = {Mar},
	number = {1},
	oto = {NOTNLM},
	own = {NLM},
	pages = {64},
	phst = {2018/06/11 00:00 {$[$}received{$]$}; 2019/02/14 00:00 {$[$}accepted{$]$}; 2019/03/21 06:00 {$[$}entrez{$]$}; 2019/03/21 06:00 {$[$}pubmed{$]$}; 2020/02/11 06:00 {$[$}medline{$]$}; 2019/03/19 00:00 {$[$}pmc-release{$]$}},
	pii = {10.1186/s12874-019-0681-4; 681},
	pl = {England},
	pmc = {PMC6425557},
	pmcr = {2019/03/19},
	pmid = {30890124},
	pst = {epublish},
	pt = {Journal Article; Research Support, Non-U.S. Gov't},
	sb = {IM},
	status = {MEDLINE},
	title = {Machine learning in medicine: a practical introduction.},
	volume = {19},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1186/s12874-019-0681-4}}

@article{Francis-QR-1962,
	abstract = {The QR transformation is an analogue to the LR transformation (Rutishauser, 1958) based on unitary transformations. Both these transformations are global iterative methods for finding the eigenvalues of a matrix, the matrix converging in general to triangular form. In Par t1 of this paper the QR transformation was briefly described and we were then principally concerned with proving convergence, the main result being expressed in theorem 3. We also showed that if the matrix is first reduced to almost triangular form important advantages are gained (further advantages will become apparent) and we gave in outline a way in which convergence could be improved. In this part of the paper we consider the practical application of the QR transformation. Two versions of the algorithm have been programmed for the Pegasus computer; these are described and an attempt is made to evaluate the method. Some results and detailed algorithms are given in appendices. Part 1 was published on pp. 265--71 of this volume (Oct. 61).},
	author = {Francis, J. G. F.},
	date-added = {2024-12-17 05:56:32 -0500},
	date-modified = {2024-12-17 05:58:31 -0500},
	doi = {10.1093/comjnl/4.4.332},
	eprint = {https://academic.oup.com/comjnl/article-pdf/4/4/332/8201663/040332.pdf},
	issn = {0010-4620},
	journal = {The Computer Journal},
	month = {01},
	number = {4},
	pages = {332-345},
	title = {{The QR Transformation---Part 2}},
	url = {https://doi.org/10.1093/comjnl/4.4.332},
	volume = {4},
	year = {1962},
	bdsk-url-1 = {https://doi.org/10.1093/comjnl/4.4.332}}

@article{Francis-QR-1961,
	abstract = {The LR transformation, due to Rutishauser, has proved to be a powerful method for finding the eigenvalues of symmetric band matrices. Little attention, however, has been paid to its application to the more difficult problem of finding eigenvalues of general unsymmetric matrices. If the matrices are large two important difficulties are likely to occur. Firstly, triangular decomposition, which is the basis of the method, is by no means always numerically stable, and secondly, the amount of computation required by the method is likely to be very great. This paper describes an algorithm similar to the LR transformation except that the transformations involved in it are all unitary and can thus be expected to by numerically stable. It is then shown that there are various advantages in first converting the matrix to almost-triangular form; in particular, the amount of work involved in the algorithm can then be greatly reduced.Part 1 of the paper is largely concerned with proof of convergence, and the theoretical aspect. Part 2, to be published in January, discussed practical computation and gives results of experiments.},
	author = {Francis, J. G. F.},
	date-added = {2024-12-17 05:54:58 -0500},
	date-modified = {2024-12-17 05:58:21 -0500},
	doi = {10.1093/comjnl/4.3.265},
	eprint = {https://academic.oup.com/comjnl/article-pdf/4/3/265/1080833/040265.pdf},
	issn = {0010-4620},
	journal = {The Computer Journal},
	month = {01},
	number = {3},
	pages = {265-271},
	title = {{The QR Transformation A Unitary Analogue to the LR Transformation---Part 1}},
	url = {https://doi.org/10.1093/comjnl/4.3.265},
	volume = {4},
	year = {1961},
	bdsk-url-1 = {https://doi.org/10.1093/comjnl/4.3.265}}

@book{golub13,
	added-at = {2014-06-23T11:34:50.000+0200},
	author = {Golub, Gene H. and van Loan, Charles F.},
	biburl = {https://www.bibsonomy.org/bibtex/2b9e78e06f69f858cbc968e62c71bb0ef/ytyoun},
	date-added = {2024-12-17 05:20:50 -0500},
	date-modified = {2024-12-17 05:20:50 -0500},
	edition = {Fourth},
	interhash = {a6e3f89a44ff7ccc942c17c894a0dab5},
	intrahash = {b9e78e06f69f858cbc968e62c71bb0ef},
	isbn = {1421407949 9781421407944},
	keywords = {GvL cauchy circulant courant-fischer determinant dft eigenvalues interlacing linear.algebra matrix pseudoinverse textbook},
	publisher = {JHU Press},
	refid = {824733531},
	timestamp = {2017-08-18T08:02:54.000+0200},
	title = {Matrix Computations},
	url = {http://www.cs.cornell.edu/cv/GVL4/golubandvanloan.htm},
	year = 2013,
	bdsk-url-1 = {http://www.cs.cornell.edu/cv/GVL4/golubandvanloan.htm}}

@article{Lloyd-1982,
	author = {Lloyd, S.},
	date-added = {2024-12-16 15:27:58 -0500},
	date-modified = {2024-12-16 15:28:20 -0500},
	doi = {10.1109/TIT.1982.1056489},
	journal = {IEEE Transactions on Information Theory},
	number = {2},
	pages = {129-137},
	title = {Least squares quantization in PCM},
	volume = {28},
	year = {1982},
	bdsk-url-1 = {https://doi.org/10.1109/TIT.1982.1056489}}
