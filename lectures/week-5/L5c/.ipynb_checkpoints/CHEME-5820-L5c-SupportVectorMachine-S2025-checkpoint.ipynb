{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b798cf0-f1b9-4ba0-b4c0-e6d677317636",
   "metadata": {},
   "source": [
    "# L5c: Support Vector Machine (SVM) Classification\n",
    "In this lecture, we introduce our last (for now) classification approach, namely [support vector machines (SVMs)](https://en.wikipedia.org/wiki/Support_vector_machine). Support vector machines are a _supervised_ learning approach to learn the _best_ possible separating hyperplane. The key ideas of this lecture are:\n",
    "\n",
    "* A __support vector machine__ is a _supervised_ machine learning algorithm that finds an optimal (linear) hyperplane in an $N$-dimensional space to classify (binary) data points distinctly, maximizing the _margin_ between different classes. The _margin_ in a support vector machine is defined as the distance from the separating hyperplane to the closest data points of either class.\n",
    "* A __hard margin support vector machine__ is a binary linear classifier that finds the optimal hyperplane to separate two classes of data points with the maximum possible _margin_, allowing no misclassifications and requiring the data to be linearly separable.\n",
    "* A __soft margin support vector machine__ is a variant of the SVM algorithm that allows for some misclassification of training data points, enabling it to handle non-linearly separable datasets and reduce overfitting by finding a balance between maximizing the decision boundary margin and minimizing classification errors.\n",
    "\n",
    "Lecture notes for today can be found: [here!](https://github.com/varnerlab/CHEME-5820-Lectures-Spring-2025/blob/main/lectures/week-5/L5c/docs/Notes.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6543fd-981f-4864-ba39-560658e45cb6",
   "metadata": {},
   "source": [
    "## Setup, Data, and Prerequisites\n",
    "We set up the computational environment by including the `Include.jl` file, loading any needed resources, such as sample datasets, and setting up any required constants. The `Include.jl` file loads external packages, various functions that we will use in the exercise, and custom types to model the components of our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fb1becf6-20b1-4396-a990-53d05d229fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"Include.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92f2541-7cd5-4344-893a-d4d3bf59a8ae",
   "metadata": {},
   "source": [
    "### Data\n",
    "This lecture will look at a [banknote authentication dataset](https://archive.ics.uci.edu/dataset/267/banknote+authentication) for classification tasks. We'll load the banknote dataset and split it into `training` and `test` data subsets (randomly).\n",
    "* __Training data__: Training datasets are collections of labeled data used to teach machine learning models, allowing these tools to learn patterns and relationships within the data.\n",
    "* __Test data__: Test datasets, on the other hand, are separate sets of labeled data used to evaluate the performance of trained models on unseen examples, providing an unbiased assessment of the _model's generalization capabilities_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c4559c-2d27-440f-81ac-346395163975",
   "metadata": {},
   "source": [
    "#### Banknote Authentication Dataset\n",
    "The second dataset we will explore is the [banknote authentication dataset from the UCI archive](https://archive.ics.uci.edu/dataset/267/banknote+authentication). This dataset has `1372` instances of 4 continuous features and an integer $\\{-1,1\\}$ class variable. \n",
    "* __Description__: Data were extracted from images taken from genuine and forged banknote-like specimens.  An industrial camera, usually used for print inspection, was used for digitization. The final images have 400x 400 pixels. Due to the object lens and distance to the investigated object, gray-scale pictures with a resolution of about 660 dpi were gained. Wavelet Transform tools were used to extract features from images.\n",
    "* __Features__: The data has four continuous features from each image: `variance` of the wavelet transformed image, `skewness` of the wavelet transformed image, `kurtosis` of the wavelet transformed image, and the `entropy` of the wavelet transformed image. The class is $\\{-1,1\\}$ where a class value of `-1` indicates genuine, `1` forged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88a1eaff-035b-4da2-83db-7484b7912438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div style = \"float: left;\"><span>1372Ã—5 DataFrame</span></div><div style = \"float: right;\"><span style = \"font-style: italic;\">1347 rows omitted</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">variance</th><th style = \"text-align: left;\">skewness</th><th style = \"text-align: left;\">curtosis</th><th style = \"text-align: left;\">entropy</th><th style = \"text-align: left;\">class</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: right;\">3.6216</td><td style = \"text-align: right;\">8.6661</td><td style = \"text-align: right;\">-2.8073</td><td style = \"text-align: right;\">-0.44699</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: right;\">4.5459</td><td style = \"text-align: right;\">8.1674</td><td style = \"text-align: right;\">-2.4586</td><td style = \"text-align: right;\">-1.4621</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: right;\">3.866</td><td style = \"text-align: right;\">-2.6383</td><td style = \"text-align: right;\">1.9242</td><td style = \"text-align: right;\">0.10645</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">4</td><td style = \"text-align: right;\">3.4566</td><td style = \"text-align: right;\">9.5228</td><td style = \"text-align: right;\">-4.0112</td><td style = \"text-align: right;\">-3.5944</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">5</td><td style = \"text-align: right;\">0.32924</td><td style = \"text-align: right;\">-4.4552</td><td style = \"text-align: right;\">4.5718</td><td style = \"text-align: right;\">-0.9888</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">6</td><td style = \"text-align: right;\">4.3684</td><td style = \"text-align: right;\">9.6718</td><td style = \"text-align: right;\">-3.9606</td><td style = \"text-align: right;\">-3.1625</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">7</td><td style = \"text-align: right;\">3.5912</td><td style = \"text-align: right;\">3.0129</td><td style = \"text-align: right;\">0.72888</td><td style = \"text-align: right;\">0.56421</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">8</td><td style = \"text-align: right;\">2.0922</td><td style = \"text-align: right;\">-6.81</td><td style = \"text-align: right;\">8.4636</td><td style = \"text-align: right;\">-0.60216</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">9</td><td style = \"text-align: right;\">3.2032</td><td style = \"text-align: right;\">5.7588</td><td style = \"text-align: right;\">-0.75345</td><td style = \"text-align: right;\">-0.61251</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">10</td><td style = \"text-align: right;\">1.5356</td><td style = \"text-align: right;\">9.1772</td><td style = \"text-align: right;\">-2.2718</td><td style = \"text-align: right;\">-0.73535</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">11</td><td style = \"text-align: right;\">1.2247</td><td style = \"text-align: right;\">8.7779</td><td style = \"text-align: right;\">-2.2135</td><td style = \"text-align: right;\">-0.80647</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">12</td><td style = \"text-align: right;\">3.9899</td><td style = \"text-align: right;\">-2.7066</td><td style = \"text-align: right;\">2.3946</td><td style = \"text-align: right;\">0.86291</td><td style = \"text-align: right;\">-1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">13</td><td style = \"text-align: right;\">1.8993</td><td style = \"text-align: right;\">7.6625</td><td style = \"text-align: right;\">0.15394</td><td style = \"text-align: right;\">-3.1108</td><td style = \"text-align: right;\">-1</td></tr><tr><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1361</td><td style = \"text-align: right;\">-0.24745</td><td style = \"text-align: right;\">1.9368</td><td style = \"text-align: right;\">-2.4697</td><td style = \"text-align: right;\">-0.80518</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1362</td><td style = \"text-align: right;\">-1.5732</td><td style = \"text-align: right;\">1.0636</td><td style = \"text-align: right;\">-0.71232</td><td style = \"text-align: right;\">-0.8388</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1363</td><td style = \"text-align: right;\">-2.1668</td><td style = \"text-align: right;\">1.5933</td><td style = \"text-align: right;\">0.045122</td><td style = \"text-align: right;\">-1.678</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1364</td><td style = \"text-align: right;\">-1.1667</td><td style = \"text-align: right;\">-1.4237</td><td style = \"text-align: right;\">2.9241</td><td style = \"text-align: right;\">0.66119</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1365</td><td style = \"text-align: right;\">-2.8391</td><td style = \"text-align: right;\">-6.63</td><td style = \"text-align: right;\">10.4849</td><td style = \"text-align: right;\">-0.42113</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1366</td><td style = \"text-align: right;\">-4.5046</td><td style = \"text-align: right;\">-5.8126</td><td style = \"text-align: right;\">10.8867</td><td style = \"text-align: right;\">-0.52846</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1367</td><td style = \"text-align: right;\">-2.41</td><td style = \"text-align: right;\">3.7433</td><td style = \"text-align: right;\">-0.40215</td><td style = \"text-align: right;\">-1.2953</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1368</td><td style = \"text-align: right;\">0.40614</td><td style = \"text-align: right;\">1.3492</td><td style = \"text-align: right;\">-1.4501</td><td style = \"text-align: right;\">-0.55949</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1369</td><td style = \"text-align: right;\">-1.3887</td><td style = \"text-align: right;\">-4.8773</td><td style = \"text-align: right;\">6.4774</td><td style = \"text-align: right;\">0.34179</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1370</td><td style = \"text-align: right;\">-3.7503</td><td style = \"text-align: right;\">-13.4586</td><td style = \"text-align: right;\">17.5932</td><td style = \"text-align: right;\">-2.7771</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1371</td><td style = \"text-align: right;\">-3.5637</td><td style = \"text-align: right;\">-8.3827</td><td style = \"text-align: right;\">12.393</td><td style = \"text-align: right;\">-1.2823</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1372</td><td style = \"text-align: right;\">-2.5419</td><td style = \"text-align: right;\">-0.65804</td><td style = \"text-align: right;\">2.6842</td><td style = \"text-align: right;\">1.1952</td><td style = \"text-align: right;\">1</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccc}\n",
       "\t& variance & skewness & curtosis & entropy & class\\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & Float64 & Float64 & Int64\\\\\n",
       "\t\\hline\n",
       "\t1 & 3.6216 & 8.6661 & -2.8073 & -0.44699 & -1 \\\\\n",
       "\t2 & 4.5459 & 8.1674 & -2.4586 & -1.4621 & -1 \\\\\n",
       "\t3 & 3.866 & -2.6383 & 1.9242 & 0.10645 & -1 \\\\\n",
       "\t4 & 3.4566 & 9.5228 & -4.0112 & -3.5944 & -1 \\\\\n",
       "\t5 & 0.32924 & -4.4552 & 4.5718 & -0.9888 & -1 \\\\\n",
       "\t6 & 4.3684 & 9.6718 & -3.9606 & -3.1625 & -1 \\\\\n",
       "\t7 & 3.5912 & 3.0129 & 0.72888 & 0.56421 & -1 \\\\\n",
       "\t8 & 2.0922 & -6.81 & 8.4636 & -0.60216 & -1 \\\\\n",
       "\t9 & 3.2032 & 5.7588 & -0.75345 & -0.61251 & -1 \\\\\n",
       "\t10 & 1.5356 & 9.1772 & -2.2718 & -0.73535 & -1 \\\\\n",
       "\t11 & 1.2247 & 8.7779 & -2.2135 & -0.80647 & -1 \\\\\n",
       "\t12 & 3.9899 & -2.7066 & 2.3946 & 0.86291 & -1 \\\\\n",
       "\t13 & 1.8993 & 7.6625 & 0.15394 & -3.1108 & -1 \\\\\n",
       "\t14 & -1.5768 & 10.843 & 2.5462 & -2.9362 & -1 \\\\\n",
       "\t15 & 3.404 & 8.7261 & -2.9915 & -0.57242 & -1 \\\\\n",
       "\t16 & 4.6765 & -3.3895 & 3.4896 & 1.4771 & -1 \\\\\n",
       "\t17 & 2.6719 & 3.0646 & 0.37158 & 0.58619 & -1 \\\\\n",
       "\t18 & 0.80355 & 2.8473 & 4.3439 & 0.6017 & -1 \\\\\n",
       "\t19 & 1.4479 & -4.8794 & 8.3428 & -2.1086 & -1 \\\\\n",
       "\t20 & 5.2423 & 11.0272 & -4.353 & -4.1013 & -1 \\\\\n",
       "\t21 & 5.7867 & 7.8902 & -2.6196 & -0.48708 & -1 \\\\\n",
       "\t22 & 0.3292 & -4.4552 & 4.5718 & -0.9888 & -1 \\\\\n",
       "\t23 & 3.9362 & 10.1622 & -3.8235 & -4.0172 & -1 \\\\\n",
       "\t24 & 0.93584 & 8.8855 & -1.6831 & -1.6599 & -1 \\\\\n",
       "\t25 & 4.4338 & 9.887 & -4.6795 & -3.7483 & -1 \\\\\n",
       "\t26 & 0.7057 & -5.4981 & 8.3368 & -2.8715 & -1 \\\\\n",
       "\t27 & 1.1432 & -3.7413 & 5.5777 & -0.63578 & -1 \\\\\n",
       "\t28 & -0.38214 & 8.3909 & 2.1624 & -3.7405 & -1 \\\\\n",
       "\t29 & 6.5633 & 9.8187 & -4.4113 & -3.2258 & -1 \\\\\n",
       "\t30 & 4.8906 & -3.3584 & 3.4202 & 1.0905 & -1 \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m1372Ã—5 DataFrame\u001b[0m\n",
       "\u001b[1m  Row \u001b[0mâ”‚\u001b[1m variance  \u001b[0m\u001b[1m skewness  \u001b[0m\u001b[1m curtosis  \u001b[0m\u001b[1m entropy   \u001b[0m\u001b[1m class \u001b[0m\n",
       "      â”‚\u001b[90m Float64   \u001b[0m\u001b[90m Float64   \u001b[0m\u001b[90m Float64   \u001b[0m\u001b[90m Float64   \u001b[0m\u001b[90m Int64 \u001b[0m\n",
       "â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
       "    1 â”‚  3.6216      8.6661   -2.8073    -0.44699      -1\n",
       "    2 â”‚  4.5459      8.1674   -2.4586    -1.4621       -1\n",
       "    3 â”‚  3.866      -2.6383    1.9242     0.10645      -1\n",
       "    4 â”‚  3.4566      9.5228   -4.0112    -3.5944       -1\n",
       "    5 â”‚  0.32924    -4.4552    4.5718    -0.9888       -1\n",
       "    6 â”‚  4.3684      9.6718   -3.9606    -3.1625       -1\n",
       "    7 â”‚  3.5912      3.0129    0.72888    0.56421      -1\n",
       "    8 â”‚  2.0922     -6.81      8.4636    -0.60216      -1\n",
       "    9 â”‚  3.2032      5.7588   -0.75345   -0.61251      -1\n",
       "   10 â”‚  1.5356      9.1772   -2.2718    -0.73535      -1\n",
       "   11 â”‚  1.2247      8.7779   -2.2135    -0.80647      -1\n",
       "  â‹®   â”‚     â‹®          â‹®          â‹®          â‹®        â‹®\n",
       " 1363 â”‚ -2.1668      1.5933    0.045122  -1.678         1\n",
       " 1364 â”‚ -1.1667     -1.4237    2.9241     0.66119       1\n",
       " 1365 â”‚ -2.8391     -6.63     10.4849    -0.42113       1\n",
       " 1366 â”‚ -4.5046     -5.8126   10.8867    -0.52846       1\n",
       " 1367 â”‚ -2.41        3.7433   -0.40215   -1.2953        1\n",
       " 1368 â”‚  0.40614     1.3492   -1.4501    -0.55949       1\n",
       " 1369 â”‚ -1.3887     -4.8773    6.4774     0.34179       1\n",
       " 1370 â”‚ -3.7503    -13.4586   17.5932    -2.7771        1\n",
       " 1371 â”‚ -3.5637     -8.3827   12.393     -1.2823        1\n",
       " 1372 â”‚ -2.5419     -0.65804   2.6842     1.1952        1\n",
       "\u001b[36m                                         1351 rows omitted\u001b[0m"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_banknote = CSV.read(joinpath(_PATH_TO_DATA, \"data-banknote-authentication.csv\"), DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55e838da-0d3d-46a4-8302-830e1782cbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "D_banknote = Matrix(df_banknote); # get the data as a Matrix (alias for Array{Float64,2})\n",
    "number_of_training_examples_banknote = 1000; # how many training points for the banknote dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb9ab5a0-cfe2-45cf-95b0-7c665f08179b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "banknote_training, banknote_test = let\n",
    "\n",
    "    number_of_features = size(D_banknote,2); # number of cols of housing data\n",
    "    number_of_examples = size(D_banknote,1); # number of rows of housing data\n",
    "    full_index_set = range(1,stop=number_of_examples,step=1) |> collect |> Set;\n",
    "    \n",
    "    # build index sets for training and testing\n",
    "    training_index_set = Set{Int64}();\n",
    "    should_stop_loop = false;\n",
    "    while (should_stop_loop == false)\n",
    "        i = rand(1:number_of_examples);\n",
    "        push!(training_index_set,i);\n",
    "\n",
    "        if (length(training_index_set) == number_of_training_examples_banknote)\n",
    "            should_stop_loop = true;\n",
    "        end\n",
    "    end\n",
    "    test_index_set = setdiff(full_index_set,training_index_set);\n",
    "\n",
    "    # build the test and train datasets -\n",
    "    banknote_training = D_banknote[training_index_set |> collect,:];\n",
    "    banknote_test = D_banknote[test_index_set |> collect,:];\n",
    "\n",
    "    # return\n",
    "    banknote_training,banknote_test\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe512880-fa06-4e0b-ae1f-27fcc676dc3c",
   "metadata": {},
   "source": [
    "## Theory: Support Vector Machine (SVM)\n",
    "Suppose, we have dataset $\\mathcal{D} = \\{(\\hat{\\mathbf{x}}_{i}, y_{i}) \\mid i = 1,2,\\dots,n\\}$, where $\\hat{\\mathbf{x}}_i \\in \\mathbb{R}^p$ is an _augmented_ feature vector ($m$ features with additional `1` to model the bias on the end of the vector) and $y_i \\in \\{-1, 1\\}$ is the corresponding class label.\n",
    "* __Objective__: the goal of an SVM is to find the hyperplane $\\mathcal{H}(\\hat{\\mathbf{x}}) = \\{\\hat{\\mathbf{x}} \\mid \\left<\\hat{\\mathbf{x}},\\theta\\right> = 0\\}$ that separates the data points into two classes (those points above the hyperplane, and those points below the hyperplane), \n",
    "where $\\theta \\in \\mathbb{R}^{p}$ ($p=m+1$) is the normal vector to the hyperplane, or alternatively, the parameters of the model that we need to estimate.\n",
    "* __Why another method__? Support vector machines (SVMs) and other approaches, e.g., [the perceptron](https://en.wikipedia.org/wiki/Perceptron) differ primarily in their optimization objectives and training methods: while a [perceptron](https://en.wikipedia.org/wiki/Perceptron) can find _a hyperplane_ that separates classes, SVMs seek to find the _best hyperplane_ in the sense that the _margin_ between classes is maximized.\n",
    "\n",
    "There are (at least) two strategies that we could use to estimate the unknown parameters $\\theta \\in \\mathbb{R}^{p}$, depending upon if we know beforehand whether the dataset $\\mathcal{D}$ is linearly separable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dca27fe-23aa-436b-88d9-9e6a05f76157",
   "metadata": {},
   "source": [
    "### Case 1: Linearly separable \n",
    "Let's take a look at a [schematic of the ideas behind a hard margin support vector machine](https://github.com/varnerlab/CHEME-5820-Lectures-Spring-2025/blob/main/lectures/week-5/L5c/docs/figs/Fig-SVM-Schematic.pdf). If the data is linearly separable, a hyperplane $\\mathcal{H}(\\hat{\\mathbf{x}})$ exists that perfectly seperates the data. We can estimate the _best_ hyperplane by maximizing the _margin_, which is equivalent to minimizing the parameter vector $\\theta$. The _maximum hard margin problem_ is given by:\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\min_{\\theta}\\quad & \\frac{1}{2}\\lVert{\\theta}\\rVert_{2}^{2}\\\\\n",
    "    \\text{subject to}\\quad & y_{i}\\left<\\hat{\\mathbf{x}}_{i},\\theta\\right> \\geq 1\\quad\\forall i\n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\theta\\in\\mathbb{R}^{p}$ denote the unknown parameters that we are trying to estimate,\n",
    "$\\hat{\\mathbf{x}}_{i}\\in\\mathbb{R}^{p}$ are the augmented (training) feature vectors, $y_{i}\\in\\{-1,1\\}$ are the class labels, and $p=m+1$ is the number of parameters, where $m$ is the number of features. The index $i$ runs over the training examples, i.e., one constraint per training example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13126254-e4a7-46d0-a311-5c0b04a91bfb",
   "metadata": {},
   "source": [
    "### Case 2: Not linearly separable \n",
    "Let's take a look at a [schematic of the ideas behind a soft margin support vector machine](https://github.com/varnerlab/CHEME-5820-Lectures-Spring-2025/blob/main/lectures/week-5/L5c/docs/figs/Fig-SVM-Schematic-Softmargin.pdf).\n",
    "If the data is _not linearly separable_, then we know that a perfect $\\mathcal{H}(\\hat{\\mathbf{x}})$ will not exist, i.e., \n",
    "no hyperplane will separate the data without making at least one mistake. In this case, we can estimate the _best_ hyperplane possible by solving the maximum soft margin problem given by:\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\min_{\\theta}\\quad & \\frac{1}{2}\\lVert{\\theta}\\rVert_{2}^{2} + C\\sum_{i=1}^{n}\\xi_{i}\\\\\n",
    "    \\text{subject to}\\quad & y_{i}\\left<\\hat{\\mathbf{x}}_{i},\\theta\\right> \\geq 1 - \\xi_{i}\\quad\\forall i\\\\\n",
    "    & \\xi_{i} \\geq 0\\quad\\forall i\n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\xi_{i}$ is a _slack variable_, that quantifies the cost of a classification mistake, and $C>{0}$ is a user-adjustable parameter that controls the trade-off between maximizing the margin and minimizing the slack variables.\n",
    "* __Values of $C$__: If $C\\gg{1}$ the classifier will behave like the maximum (hard) margin classifier, i.e., mistakes will be expensive, and the search will avoid making choices with mistakes. However, if $C\\ll{1}$, the classifier will allow more slack (mistakes), i.e., mistakes are cheap, so what's it matter!\n",
    "\n",
    "__Do we solve the soft margin problem?__\n",
    "\n",
    "Typically, we don't solve the problem above directly; instead, we reformulate it as an _unconstrained_ problem [using the hinge-loss function](https://en.wikipedia.org/wiki/Hinge_loss):\n",
    "$$\n",
    "\\begin{equation*}\n",
    "    \\min_{\\theta}\\frac{1}{2}\\lVert{\\theta}\\rVert_{2}^{2} + C\\sum_{i=1}^{n}\\max\\{0, 1 - y_{i}\\left<\\hat{\\mathbf{x}}_{i},\\theta\\right>\\}\n",
    "\\end{equation*}\n",
    "$$\n",
    "where the sum is computed over $n$ training examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b339857d-9c26-49b7-90ba-3124d5d974b0",
   "metadata": {},
   "source": [
    "## Banknote Classification Problem using an SVM\n",
    "In this example, we [use the SVM implementation exported by the `LIBSVM.jl` package](https://github.com/JuliaML/LIBSVM.jl) to classify the [banknote authentication dataset from the UCI archive](https://archive.ics.uci.edu/dataset/267/banknote+authentication). In particular, we use the `training` dataset to estimate the unknown model parameters $\\theta$ [using the `svmtrain(...)` method](https://github.com/JuliaML/LIBSVM.jl/blob/master/src/LIBSVM.jl). \n",
    "* The [`svmtrain(...)` method](https://github.com/JuliaML/LIBSVM.jl/blob/master/src/LIBSVM.jl) takes a matrix of feature vectors where augmented training examples matrix $\\hat{\\mathbf{X}}^{\\top}$, i.e., the examples are on the columns and the features are the rows, and a label vector $\\mathbf{y}\\in\\left\\{-1,1\\right\\}$.\n",
    "* The [`svmtrain(...)` method](https://github.com/JuliaML/LIBSVM.jl/blob/master/src/LIBSVM.jl) returns a [model instance](https://github.com/JuliaML/LIBSVM.jl/blob/master/src/LIBSVM.jl) that holds the trained data and a bunch of other data associated with the problem.\n",
    "* __Hmmm__: One of the (super) interesting optional arguments [the `svmtrain(...)` method](https://github.com/JuliaML/LIBSVM.jl/blob/master/src/LIBSVM.jl) is the `kernel` argument. Check out the documentation to see what kernels are supported! Wow! we get [kernelized SVM capability](https://en.wikipedia.org/wiki/Support_vector_machine#Nonlinear_kernels) right out of the box. Buy versus build, 99% buy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4802b219-3ef9-45ba-91cb-e1e8f8b56578",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = let\n",
    "\n",
    "    # Setup the data that we are using\n",
    "    D = banknote_training; # what dataset are we looking at?\n",
    "    number_of_examples = size(D,1); # how many rows?\n",
    "    X = [D[:,1:end-1] ones(number_of_examples)] |> transpose |> Matrix; # features (arranged as m x n)\n",
    "    y = D[:,end]; # label\n",
    "\n",
    "    # Train the data -\n",
    "    model = svmtrain(X, y, kernel=LIBSVM.Kernel.Linear); # we are using the LIBSVM\n",
    "\n",
    "    # return\n",
    "    model\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89c480b-1f20-4e2c-b1c3-ca1dea5b6dfe",
   "metadata": {},
   "source": [
    "__Inference__: Now that we have parameters estimated from the `training` data, we can use those parameters on the `test` dataset to see how well the model can differentiate between an actual banknote and a forgery on data it has never seen. We run the classification operation on the (unseen) test data [using the `svmpredict(...)` method](https://github.com/JuliaML/LIBSVM.jl/blob/master/src/LIBSVM.jl). \n",
    "* The [`svmpredict(...)` method](https://github.com/JuliaML/LIBSVM.jl/blob/master/src/LIBSVM.jl) returns the predicted label which we store in the `yÌ‚::Array{Int64,1}` array. We store the actual (correct) label in the `y::Array{Int64,1}` vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1047fae5-e228-47cf-adb4-be29b78f538a",
   "metadata": {},
   "outputs": [],
   "source": [
    "yÌ‚,y,d = let\n",
    "\n",
    "     # Setup the data that we are using\n",
    "    D = banknote_test; # what dataset are we looking at?\n",
    "    number_of_examples = size(D,1); # how many rows?\n",
    "    X = [D[:,1:end-1] ones(number_of_examples)] |> transpose |> Matrix; # features (arranged as m x n)\n",
    "    y = D[:,end]; # label\n",
    "    \n",
    "    # Test model on the other half of the data.\n",
    "    Å·, decision_values = svmpredict(model, X);\n",
    "\n",
    "    # return -\n",
    "    yÌ‚,y,decision_values\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e049b86c-6943-429c-9d31-9597263f4373",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "The confusion matrix is a $2\\times{2}$ matrix that contains four entries: true positive (TP), false positive (FP), true negative (TN), and false negative (FN). [Click me for a confusion matrix schematic!](https://github.com/varnerlab/CHEME-5820-Labs-Spring-2025/blob/main/labs/week-3/L3b/figs/Fig-BinaryConfusionMatrix.pdf). Let's compute these four values [using the `confusion(...)` method](src/Compute.jl) and store them in the `CM_perceptron::Array{Int64,2}` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "33aa975d-384a-468b-8a7d-5799ff326974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2Ã—2 Matrix{Int64}:\n",
       " 153    0\n",
       "   0  219"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CM = confusion(y, yÌ‚) # call with the SVM percepton values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "db77e034-e450-4761-b4b6-174fa52f69b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction correct: 1.0 Fraction incorrect 0.0\n"
     ]
    }
   ],
   "source": [
    "number_of_test_points = length(y);\n",
    "correct_prediction_perceptron = CM[1,1] + CM[2,2];\n",
    "(correct_prediction_perceptron/number_of_test_points) |> f-> println(\"Fraction correct: $(f) Fraction incorrect $(1-f)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a531e9d8-2e96-4840-a8cb-d78f217aa2d3",
   "metadata": {},
   "source": [
    "# Today?\n",
    "That's a wrap! What are some of the interesting things we discussed today? Let's review by looking at [the schematic of a hard margin support vector machine!](https://github.com/varnerlab/CHEME-5820-Lectures-Spring-2025/blob/main/lectures/week-5/L5c/docs/figs/Fig-SVM-Schematic.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.3",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
