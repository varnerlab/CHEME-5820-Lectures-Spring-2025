{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c3374c5-23ad-48f0-85d4-2bdc4c379f2d",
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "source": [
    "# L14a: Natural Language Models Before Transformers\n",
    "In this lecture, we'll look at natural language models before the advent of transformers. We'll discuss the limitations of these models and how they paved the way for the development of transformer architectures.\n",
    "\n",
    "The key concepts of this lecture include:\n",
    "* Fill me in\n",
    "\n",
    "The sources for this lecture were:\n",
    "* [Vaswani, Ashish, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin. “Attention is All you Need.” Neural Information Processing Systems (2017).](https://arxiv.org/abs/1706.03762)\n",
    "* [Ramsauer, H., Schafl, B., Lehner, J., Seidl, P., Widrich, M., Gruber, L., Holzleitner, M., Pavlovi'c, M., Sandve, G.K., Greiff, V., Kreil, D.P., Kopp, M., Klambauer, G., Brandstetter, J., & Hochreiter, S. (2020). Hopfield Networks is All You Need. ArXiv, abs/2008.02217.](https://arxiv.org/abs/2008.02217)\n",
    "* [Phuong, M., & Hutter, M. (2022). Formal Algorithms for Transformers. ArXiv, abs/2207.09238.](https://arxiv.org/abs/2207.09238)\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e81d9b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
