{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c3374c5-23ad-48f0-85d4-2bdc4c379f2d",
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "source": [
    "# L14a: Natural Language Models Before Transformers\n",
    "In this lecture, we'll look at natural language models before the advent of transformers. We'll discuss the limitations of these models and how they paved the way for the development of transformer architectures.\n",
    "\n",
    "The key concepts of this lecture include:\n",
    "* Fill me in\n",
    "\n",
    "The sources for this lecture were:\n",
    "* [Rong, X. (2014). word2vec Parameter Learning Explained. ArXiv, abs/1411.2738.](https://arxiv.org/abs/1411.2738)\n",
    "* [Vaswani, Ashish, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin. “Attention is All you Need.” Neural Information Processing Systems (2017).](https://arxiv.org/abs/1706.03762)\n",
    "* [Ramsauer, H., Schafl, B., Lehner, J., Seidl, P., Widrich, M., Gruber, L., Holzleitner, M., Pavlovi'c, M., Sandve, G.K., Greiff, V., Kreil, D.P., Kopp, M., Klambauer, G., Brandstetter, J., & Hochreiter, S. (2020). Hopfield Networks is All You Need. ArXiv, abs/2008.02217.](https://arxiv.org/abs/2008.02217)\n",
    "* [Phuong, M., & Hutter, M. (2022). Formal Algorithms for Transformers. ArXiv, abs/2207.09238.](https://arxiv.org/abs/2207.09238)\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e81d9b",
   "metadata": {},
   "source": [
    "## Understanding Contextualized Word Representations\n",
    "Contextualized word representations are advanced techniques in natural language processing that capture the meaning of words based on their specific context within a sentence or passage. Unlike traditional word embeddings, which assign a single vector to each word regardless of usage, contextualized models generate dynamic representations that reflect the surrounding words and syntax. This approach enables more nuanced understanding of language, allowing models to distinguish between different senses of a word and improving performance on a wide range of linguistic tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee2e76e",
   "metadata": {},
   "source": [
    "### The Continuous Bag of Words (CBOW)\n",
    "The Continuous Bag of Words (CBOW) model is a neural network architecture used for learning word embeddings. It predicts a target word based on its surrounding context words. The model takes a set of context words as input and outputs the probability distribution of the target word. CBOW is efficient and effective for capturing semantic relationships between words, making it a popular choice in natural language processing tasks.\n",
    "\n",
    "Finish me."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52884ff7",
   "metadata": {},
   "source": [
    "### The Skip-Gram Model\n",
    "The skip-gram model is a neural network-based approach in natural language processing designed to learn word embeddings by predicting the surrounding context words given a target word within a fixed window in a text corpus.\n",
    "* _What is it?_ A skip-gram model consists of a single hidden layer that transforms a one-hot encoded input word into a dense vector representation, optimizing the embedding so that words appearing in similar contexts have similar vector representations. This method effectively captures semantic relationships and contextual similarity between words, making it foundational for many downstream NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8db5313",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
