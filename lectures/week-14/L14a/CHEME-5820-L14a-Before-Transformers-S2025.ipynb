{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c3374c5-23ad-48f0-85d4-2bdc4c379f2d",
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "source": [
    "# L14a: Natural Language Embedding Models\n",
    "In this lecture, we'll look at natural language models before the advent of transformers. In particular, we'll introduce [embedding models](https://en.wikipedia.org/wiki/Word_embedding), which are techniques used to represent words in a continuous vector space. These models are crucial for understanding the evolution of natural language processing (NLP) and the development of transformer architectures.\n",
    "\n",
    "The key concepts of this lecture include:\n",
    "* Fill me in\n",
    "\n",
    "The sources for this lecture were:\n",
    "* [Rong, X. (2014). word2vec Parameter Learning Explained. ArXiv, abs/1411.2738.](https://arxiv.org/abs/1411.2738)\n",
    "* [Vaswani, Ashish, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin. “Attention is All you Need.” Neural Information Processing Systems (2017).](https://arxiv.org/abs/1706.03762)\n",
    "* [Ramsauer, H., Schafl, B., Lehner, J., Seidl, P., Widrich, M., Gruber, L., Holzleitner, M., Pavlovi'c, M., Sandve, G.K., Greiff, V., Kreil, D.P., Kopp, M., Klambauer, G., Brandstetter, J., & Hochreiter, S. (2020). Hopfield Networks is All You Need. ArXiv, abs/2008.02217.](https://arxiv.org/abs/2008.02217)\n",
    "* [Phuong, M., & Hutter, M. (2022). Formal Algorithms for Transformers. ArXiv, abs/2207.09238.](https://arxiv.org/abs/2207.09238)\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e81d9b",
   "metadata": {},
   "source": [
    "## Embedding Models\n",
    "The overall goal of embedding models is to represent language sequences, e.g., characters, words, documents, etc in a continuous vector space, where similar words are _close together_ in the embedding space. Let's take a look at some of the most popular embedding models, the continuous bag of words (CBOW) and skip-gram models. \n",
    "* _Key idea_: These CBOW and Skip-Gram models are based on the idea that words that appear in similar contexts tend to have similar meanings. The CBOW model predicts a target word based on its context, while the skip-gram model does the opposite: it predicts the context given a target word.\n",
    "\n",
    "Before we dive into the details of these models, let's first introduce some key concepts, terminology and notation that will be used throughout this lecture.\n",
    "\n",
    "### Vocabulary, Tokens and Tokenization\n",
    "Let $\\mathcal{V}$ be the vocabulary of tokens (characters, sub-words, full words, documents, etc) in our [corpus](https://en.wikipedia.org/wiki/Corpus), and let $N_{\\mathcal{V}} = \\dim\\mathcal{V}$ be the size of the vocabulary. Let $\\mathbf{x}\\equiv \\{x_1, x_2, \\ldots, x_n\\in\\mathcal{V}\\}$ be a sequence of tokens in the corpus i.e., a sentence or document, where $n$ is the length of the sequence, and $x_i$ is the $i$-th token in the sequence. \n",
    "\n",
    "Let's consider a simple example: `My grandma makes the best apple pie.`\n",
    "\n",
    "Tokens are the basic units of text that we will be working with. In this space, tokens can be characters, sub-words, full words, or even entire documents. The process of converting a sequence of text into tokens is called _tokenization_.\n",
    "* _Character-level tokenization_. Given the example above, one possible choice is to let the vocabulary $\\mathcal{V}$ be the (English) alphabet (plus punctuation). Thus, we’d get a sequence $\\mathbf{x}\\in\\mathcal{V}$ of length 36: `[‘M’, ‘y’, ‘ ’, ..., ’.’]`. Character-level tokenization tends to yield _very long sequences_.\n",
    "* _Word-level tokenization_. Another possible choice is to let the vocabulary $\\mathcal{V}$ be the set of all words in the corpus. Thus, we’d get a sequence $\\mathbf{x}\\in\\mathcal{V}$ of length 8: `[‘My’, ‘grandma’, ‘makes’, ‘the’, ‘best’, ‘apple’, ‘pie’, ‘.’]`. Word-level tokenization tends to yield _shorter sequences_, however, word-level tokenization tends to\n",
    "require a very large vocabulary and cannot deal with new words at test time.\n",
    "* _Sub-word tokenization_. A third possible choice is to let the vocabulary $\\mathcal{V}$ be the set of commonly occurring word segments like `cious`, `ing`, `pre`. Common words like `is` are often a separate token, and single characters are also included in the vocabulary $\\mathcal{V}$ to ensure all words are expressible.\n",
    "\n",
    "Given a choice of tokenization / vocabulary, each vocabulary element is assigned a unique index $\\left\\{1, 2,\\dots,N_{\\mathcal{V}}-3\\right\\}$. A number of special (control) tokens are then added to the vocabulary, let's use `3` but there could be more:\n",
    "* $\\texttt{mask} \\rightarrow N_{\\mathcal{V}} - 2$: the `mask` token that is used to mask out a toekn in the input sequence. This is used in training to predict the masked word.\n",
    "* $\\texttt{bos} \\rightarrow N_{\\mathcal{V}} - 1$: the begining of sequence (bos) token is used to indicate the start of a sequence. \n",
    "* $\\texttt{eos} \\rightarrow N_{\\mathcal{V}}$: the end of sequence (eos) token is used to indicate the end of a sequence.\n",
    "\n",
    "A piece of text is represented as a sequence of indices (called token IDs) corresponding to its (sub)words, preceded by $\\texttt{bos}$-token and followed by the $\\texttt{eos}$-token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee2e76e",
   "metadata": {},
   "source": [
    "### Context Continuous Bag of Words (CBOW)\n",
    "The Continuous Bag of Words (CBOW) model is a neural network architecture used for learning word embeddings that was popularized by the [word2vec algorithm](https://arxiv.org/abs/1301.3781). \n",
    "\n",
    "* _What is it?_ The CBOW model predicts the probability of a _target word_ based on its surrounding _context words_. The CBOW is encoded as a feedforward neural network with a single hidden layer. The input (context) vector $\\mathbf{x}\\in\\mathbb{R}^{N_{\\mathcal{V}}}$ is a [one-hot encoded vector](https://en.wikipedia.org/wiki/One-hot) representing the _context words_, while the output is a _softmax layer_ that computes the probability of the target word given the context.\n",
    "\n",
    "In the simplest case, the hidden layer $\\mathbf{h}\\in\\mathbb{R}^{h}$ is a computed using a linear layer with no activation function:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{h} &= \\mathbf{W}_{1} \\cdot \\mathbf{x} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\mathbf{W}_{1}\\in\\mathbb{R}^{h\\times{N_{\\mathcal{V}}}}$ is the (unkown) weight matrix of the hidden layer, and $\\mathbf{x}$ is [the one-hot encoded vector](https://en.wikipedia.org/wiki/One-hot) of context word(s). The hidden layer is then mapped through another linear layer:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{u} &= \\mathbf{W}_{2} \\cdot \\mathbf{h} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "which produces the $\\mathbf{u}\\in\\mathbb{R}^{N_{\\mathcal{V}}}$ vector, where $\\mathbf{W}_{2}\\in\\mathbb{R}^{N_{\\mathcal{V}}\\times{h}}$ is the (unknown) weight matrix for the output layer. The output layer is then passed through a softmax activation function to obtain the probability distribution over the vocabulary:\n",
    "$$\n",
    "\\begin{align*}\n",
    "p(w_{i} | \\mathbf{x}) = y_i &= \\frac{e^{\\mathbf{u}_i}}{\\sum_{j=1}^{N_{\\mathcal{V}}} e^{\\mathbf{u}_j}} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "where $p(w_{i} | \\mathbf{x})$ is the probability of observing the ith word in the vocabulary as the output (target) given the context vector $\\mathbf{x}$, $N_{\\mathcal{V}}$ is the size of the vocabulary, and $e^{\\mathbf{u}_i}$ is the exponential function applied to the ith element of the vector $\\mathbf{u}$.\n",
    "\n",
    "#### Training\n",
    "The training objective of the CBOW model is to maximize the likelihood of target word(s) given the context words. This is done by minimizing the negative log-likelihood loss function:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\min\\mathcal{L} &= -\\sum_{i=1}^{N_{\\mathcal{V}}} y_{i}\\cdot\\log p(w_{i} | \\mathbf{x}) \\\\\n",
    "&= -\\sum_{i=1}^{N_{\\mathcal{V}}} y_{i}\\cdot\\log \\left( \\frac{e^{\\mathbf{u}_i}}{\\sum_{j=1}^{N_{\\mathcal{V}}} e^{\\mathbf{u}_j}} \\right) \\\\\n",
    "&= -\\sum_{i=1}^{N_{\\mathcal{V}}} y_{i}\\cdot\\left( \\mathbf{u}_i - \\log \\left( \\sum_{j=1}^{N_{\\mathcal{V}}} e^{\\mathbf{u}_j} \\right) \\right) \\\\\n",
    "&= \\sum_{i=1}^{N_{\\mathcal{V}}} y_{i}\\cdot\\left(\\log \\left( \\sum_{j=1}^{N_{\\mathcal{V}}} e^{\\mathbf{u}_j} \\right) -  \\mathbf{u}_i\\right)\\quad\\text{substitute}~u_{i} = \\langle \\mathbf{w}_{2}^{(i)},\\mathbf{W}_{1}\\cdot\\mathbf{x}\\rangle \\\\\n",
    "&= \\sum_{i=1}^{N_{\\mathcal{V}}} y_{i}\\cdot\\left(\\log \\left( \\sum_{j=1}^{N_{\\mathcal{V}}} e^{\\langle \\mathbf{w}_{2}^{(j)},\\mathbf{W}_{1}\\cdot\\mathbf{x}\\rangle} \\right) -  \\langle \\mathbf{w}_{2}^{(i)},\\mathbf{W}_{1}\\cdot\\mathbf{x}\\rangle\\right)\\blacksquare\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\mathcal{L}$ is the loss function, $y_{i}$ is the one-hot encoded vector of the target word(s), and $\\mathbf{W}_{1}$ and $\\mathbf{W}_{2}$ are the weight matrices of the hidden and output layers, respectively, and $\\langle \\cdot,\\cdot\\rangle$ is the inner product. Finally, the term $\\mathbf{w}_{2}^{(i)}$ is the ith column of the weight matrix $\\mathbf{W}_{2}$, which corresponds to the target word $w_{i}$.\n",
    "\n",
    "A variety of optimization algorithms can be used to minimize the loss function. Let's implement the CBOW model, and mess around with the hyperparameters to see how they affect the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8eca43",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Example: CBOW Model of Sarcasm Headlines\n",
    "Fill me in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd6cf2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"Include.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccb607c",
   "metadata": {},
   "source": [
    "### Sarcasm Data\n",
    "We'll load a public dataset of headlines curated as either sarcastic or not sarcastic. The dataset we'll use is available on [Kaggle](https://www.kaggle.com/datasets/rmisra/news-headlines-dataset-for-sarcasm-detection) and is also discussed in the publications:\n",
    "1. [Misra, Rishabh and Prahal Arora. \"Sarcasm Detection using News Headlines Dataset.\" AI Open (2023).](https://www.sciencedirect.com/science/article/pii/S2666651023000013?via%3Dihub)\n",
    "2. [Misra, Rishabh and Jigyasa Grover. \"Sculpting Data for ML: The first act of Machine Learning.\" ISBN 9798585463570 (2021).](https://rishabhmisra.github.io/Sculpting_Data_for_ML.pdf)\n",
    "\n",
    "The sarcasm data is encoded as a collection of `JSON` records (although it is not directly readable using a JSON parser). Each record has the following fields:\n",
    "* `is_sarcastic`: has a value of `1` if the record is sarcastic; otherwise, `0.`\n",
    "* `headline`: the headline of the article, unstructured text\n",
    "* `article_link`: link to the original news article. Useful in collecting supplementary data\n",
    "\n",
    "We'll load the saved data file that we generated in `L13b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3654a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpusmodel = let\n",
    "\n",
    "    # setup path -\n",
    "    path_to_saved_corpus_file = joinpath(_PATH_TO_DATA, \"L13b-SarcasmSamplesTokenizer-SavedData.jld2\");\n",
    "    saveddata = load(path_to_saved_corpus_file);\n",
    "\n",
    "    # get items from the saveddata -\n",
    "    corpusmodel = saveddata[\"corpus\"];\n",
    "\n",
    "    # return \n",
    "    corpusmodel\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41f6ac4",
   "metadata": {},
   "source": [
    "Fill me in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b39e598",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = corpusmodel.tokens; # vocabulary for the corpus\n",
    "inverse_vocabulary = corpusmodel.inverse; # inverse vocabulary for the corpus\n",
    "N = length(vocabulary); # number of tokens in the vocabulary\n",
    "number_of_hidden_states = 100; # number of hidden states\n",
    "array_of_token_ids = range(1, step=1, length=N) |> collect; # array of token ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00a973c",
   "metadata": {},
   "source": [
    "Fill me in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cae1b609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"thirtysomething scientists unveil doomsday clock of hair loss\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpusmodel.records[1].headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408f6d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = let\n",
    "\n",
    "    # specify the context, and the target -\n",
    "    training_dataset = Vector{Tuple{Vector{Float32}, OneHotVector{UInt32}}}();\n",
    "    context_word_id = vocabulary[\"thirtysomething\"];\n",
    "    target_word_id = vocabulary[\"scientists\"];\n",
    "    \n",
    "    # compute the one-hot encoding for the context and target -\n",
    "    context_one_hot = zeros(Float32, N, 1);\n",
    "    context_one_hot[context_word_id] = 1.0 |> Float32;\n",
    "    target_one_hot = onehot(target_word_id, array_of_token_ids);\n",
    "\n",
    "    D = (context_one_hot, target_one_hot);\n",
    "    push!(training_dataset, D);\n",
    "\n",
    "    # return the training dataset -\n",
    "    training_dataset\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea00abf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29664×1 Matrix{Float32}:\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " ⋮\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x₁"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2576b0",
   "metadata": {},
   "source": [
    "Fill me in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fe429aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Uncomment the code below to build the model!\n",
    "Flux.@layer MyFluxNeuralNetworkModel  trainable=(input, middle, hidden); # create a \"namespaced\" of sorts\n",
    "MyModel() = MyFluxNeuralNetworkModel( # a strange type of constructor\n",
    "    Chain(\n",
    "        input = Dense(N, number_of_hidden_states, tanh_fast),  # layer 1\n",
    "        hidden = Dense(number_of_hidden_states, N, tanh_fast), # layer 2\n",
    "        output = NNlib.softmax) # layer 3 (output layer)\n",
    ");\n",
    "model = MyModel().chain;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4038d42f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23295"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y₁ |> v-> argmax(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46c45661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"scientists\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inverse_vocabulary[23295]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bf38ee",
   "metadata": {},
   "source": [
    "Fill me in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13062bcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29664×1 Matrix{Float32}:\n",
       " 3.373676f-5\n",
       " 3.371541f-5\n",
       " 3.371067f-5\n",
       " 3.372976f-5\n",
       " 3.376852f-5\n",
       " 3.3670567f-5\n",
       " 3.37205f-5\n",
       " 3.3723845f-5\n",
       " 3.368726f-5\n",
       " 3.3717908f-5\n",
       " ⋮\n",
       " 3.369874f-5\n",
       " 3.371874f-5\n",
       " 3.3690172f-5\n",
       " 3.3718115f-5\n",
       " 3.371758f-5\n",
       " 3.3685053f-5\n",
       " 3.3680633f-5\n",
       " 3.3687964f-5\n",
       " 3.3688557f-5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model(x₁)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65ac063c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(ŷ, y) = Flux.Losses.logitcrossentropy(ŷ, y; agg = mean); # loss for training multiclass classifiers, what is the agg?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca50d906",
   "metadata": {},
   "source": [
    "Train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f75fea",
   "metadata": {},
   "outputs": [
    {
     "ename": "MethodError",
     "evalue": "MethodError: no method matching (::var\"#23#24\")(::Chain{@NamedTuple{input::Dense{typeof(tanh_fast), Matrix{Float32}, Vector{Float32}}, hidden::Dense{typeof(tanh_fast), Matrix{Float32}, Vector{Float32}}, output::typeof(softmax)}}, ::Matrix{Float32})\nThe function `#23` exists, but no method is defined for this combination of argument types.\n\nClosest candidates are:\n  (::var\"#23#24\")(::Any, ::Any, !Matched::Any)\n   @ Main ~/Desktop/julia_work/CHEME-5820-SP25/CHEME-5820-Lectures-Spring-2025/lectures/week-14/L14a/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X30sZmlsZQ==.jl:18\n",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching (::var\"#23#24\")(::Chain{@NamedTuple{input::Dense{typeof(tanh_fast), Matrix{Float32}, Vector{Float32}}, hidden::Dense{typeof(tanh_fast), Matrix{Float32}, Vector{Float32}}, output::typeof(softmax)}}, ::Matrix{Float32})\n",
      "The function `#23` exists, but no method is defined for this combination of argument types.\n",
      "\n",
      "Closest candidates are:\n",
      "  (::var\"#23#24\")(::Any, ::Any, !Matched::Any)\n",
      "   @ Main ~/Desktop/julia_work/CHEME-5820-SP25/CHEME-5820-Lectures-Spring-2025/lectures/week-14/L14a/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X30sZmlsZQ==.jl:18\n",
      "\n",
      "\n",
      "Stacktrace:\n",
      "  [1] macro expansion\n",
      "    @ ~/.julia/packages/Zygote/HdT4O/src/compiler/interface2.jl:0 [inlined]\n",
      "  [2] _pullback(::Zygote.Context{false}, ::var\"#23#24\", ::Chain{@NamedTuple{input::Dense{typeof(tanh_fast), Matrix{Float32}, Vector{Float32}}, hidden::Dense{typeof(tanh_fast), Matrix{Float32}, Vector{Float32}}, output::typeof(softmax)}}, ::Matrix{Float32})\n",
      "    @ Zygote ~/.julia/packages/Zygote/HdT4O/src/compiler/interface2.jl:81\n",
      "  [3] _apply(::Function, ::Vararg{Any})\n",
      "    @ Core ./boot.jl:946\n",
      "  [4] adjoint\n",
      "    @ ~/.julia/packages/Zygote/HdT4O/src/lib/lib.jl:199 [inlined]\n",
      "  [5] _pullback\n",
      "    @ ~/.julia/packages/ZygoteRules/CkVIK/src/adjoint.jl:67 [inlined]\n",
      "  [6] #4\n",
      "    @ ~/.julia/packages/Flux/3711C/src/train.jl:117 [inlined]\n",
      "  [7] _pullback(ctx::Zygote.Context{false}, f::Flux.Train.var\"#4#5\"{var\"#23#24\", Tuple{Matrix{Float32}}}, args::Chain{@NamedTuple{input::Dense{typeof(tanh_fast), Matrix{Float32}, Vector{Float32}}, hidden::Dense{typeof(tanh_fast), Matrix{Float32}, Vector{Float32}}, output::typeof(softmax)}})\n",
      "    @ Zygote ~/.julia/packages/Zygote/HdT4O/src/compiler/interface2.jl:0\n",
      "  [8] pullback(f::Function, cx::Zygote.Context{false}, args::Chain{@NamedTuple{input::Dense{typeof(tanh_fast), Matrix{Float32}, Vector{Float32}}, hidden::Dense{typeof(tanh_fast), Matrix{Float32}, Vector{Float32}}, output::typeof(softmax)}})\n",
      "    @ Zygote ~/.julia/packages/Zygote/HdT4O/src/compiler/interface.jl:96\n",
      "  [9] pullback\n",
      "    @ ~/.julia/packages/Zygote/HdT4O/src/compiler/interface.jl:94 [inlined]\n",
      " [10] withgradient(f::Function, args::Chain{@NamedTuple{input::Dense{typeof(tanh_fast), Matrix{Float32}, Vector{Float32}}, hidden::Dense{typeof(tanh_fast), Matrix{Float32}, Vector{Float32}}, output::typeof(softmax)}})\n",
      "    @ Zygote ~/.julia/packages/Zygote/HdT4O/src/compiler/interface.jl:211\n",
      " [11] macro expansion\n",
      "    @ ~/.julia/packages/Flux/3711C/src/train.jl:117 [inlined]\n",
      " [12] macro expansion\n",
      "    @ ~/.julia/packages/ProgressLogging/6KXlp/src/ProgressLogging.jl:328 [inlined]\n",
      " [13] train!(loss::Function, model::Chain{@NamedTuple{input::Dense{typeof(tanh_fast), Matrix{Float32}, Vector{Float32}}, hidden::Dense{typeof(tanh_fast), Matrix{Float32}, Vector{Float32}}, output::typeof(softmax)}}, data::Tuple{Matrix{Float32}, OneHotVector{UInt32}}, opt::@NamedTuple{layers::@NamedTuple{input::@NamedTuple{weight::Optimisers.Leaf{Momentum{Float64, Float64}, Matrix{Float32}}, bias::Optimisers.Leaf{Momentum{Float64, Float64}, Vector{Float32}}, σ::Tuple{}}, hidden::@NamedTuple{weight::Optimisers.Leaf{Momentum{Float64, Float64}, Matrix{Float32}}, bias::Optimisers.Leaf{Momentum{Float64, Float64}, Vector{Float32}}, σ::Tuple{}}, output::Tuple{}}}; cb::Nothing)\n",
      "    @ Flux.Train ~/.julia/packages/Flux/3711C/src/train.jl:114\n",
      " [14] train!(loss::Function, model::Chain{@NamedTuple{input::Dense{typeof(tanh_fast), Matrix{Float32}, Vector{Float32}}, hidden::Dense{typeof(tanh_fast), Matrix{Float32}, Vector{Float32}}, output::typeof(softmax)}}, data::Tuple{Matrix{Float32}, OneHotVector{UInt32}}, opt::@NamedTuple{layers::@NamedTuple{input::@NamedTuple{weight::Optimisers.Leaf{Momentum{Float64, Float64}, Matrix{Float32}}, bias::Optimisers.Leaf{Momentum{Float64, Float64}, Vector{Float32}}, σ::Tuple{}}, hidden::@NamedTuple{weight::Optimisers.Leaf{Momentum{Float64, Float64}, Matrix{Float32}}, bias::Optimisers.Leaf{Momentum{Float64, Float64}, Vector{Float32}}, σ::Tuple{}}, output::Tuple{}}})\n",
      "    @ Flux.Train ~/.julia/packages/Flux/3711C/src/train.jl:111\n",
      " [15] top-level scope\n",
      "    @ ~/Desktop/julia_work/CHEME-5820-SP25/CHEME-5820-Lectures-Spring-2025/lectures/week-14/L14a/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X30sZmlsZQ==.jl:17"
     ]
    }
   ],
   "source": [
    "trained_model = let\n",
    "\n",
    "    localmodel = model; # make a local copy of the model\n",
    "\n",
    "    # loss function -\n",
    "    \n",
    "\n",
    "    λ = 0.61; # TODO: maybe change the learning rate (default: 0.61)?\n",
    "    β = 0.10; # TODO: maybe change the momentum parameter (default: 0.10)?\n",
    "    opt_state = Flux.setup(Momentum(λ,β), model);\n",
    "\n",
    "    \n",
    "    # train the model - check out the do block notion: https://docs.julialang.org/en/v1/base/base/#do\n",
    "    Flux.train!(localmodel, training_dataset, opt_state) do m, x, y\n",
    "       loss(m(x), y) # loss function\n",
    "    end\n",
    "\n",
    "    localmodel;\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52884ff7",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Lab: The Skip-Gram Model\n",
    "In lab `L14b`, we'll look at the skip-gram model, which is a neural network-based approach in natural language processing designed to learn word embeddings by predicting the surrounding context words given a target word within a fixed window in a text corpus. \n",
    "* _What is it?_ A skip-gram model consists of a single hidden layer that transforms a one-hot encoded input word into a dense vector representation, optimizing the embedding so that words appearing in similar contexts have similar vector representations. This method effectively captures semantic relationships and contextual similarity between words, making it foundational for many downstream NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c8154a",
   "metadata": {},
   "source": [
    "# Today?\n",
    "That's a wrap! What are some of the interesting things we discussed today?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8db5313",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.5",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
