{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c3374c5-23ad-48f0-85d4-2bdc4c379f2d",
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "source": [
    "# L14a: Natural Language Embedding Models\n",
    "In this lecture, we'll examine natural language models before the advent of transformers. We'll introduce [embedding models](https://en.wikipedia.org/wiki/Word_embedding), which are techniques for representing words in a continuous vector space. These models are crucial for understanding the evolution of natural language processing (NLP) and the development of transformer architectures.\n",
    "\n",
    "The key concepts of this lecture include:\n",
    "* __Embedding Models__: These models represent words as vectors in a continuous space, allowing for the capture of semantic relationships between words. We'll discuss how these embeddings are learned and their applications in various NLP tasks.\n",
    "* __Word2Vec__: A popular embedding model that uses shallow neural networks to learn word representations. We'll explore the two main architectures of Word2Vec: Continuous Bag of Words (CBOW) and Skip-Gram (in L14b).\n",
    "* __Continuous Bag of Words (CBOW)__: This architecture predicts the target word based on its context words. It uses a shallow neural network to learn the embeddings of words in a given context. No positional information is used, and the model is trained to minimize the loss between the predicted and actual target word.\n",
    "\n",
    "The sources for this lecture include:\n",
    "* [Rong, X. (2014). word2vec Parameter Learning Explained. ArXiv, abs/1411.2738.](https://arxiv.org/abs/1411.2738)\n",
    "* [Vaswani, Ashish, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin. “Attention is All You Need.” Neural Information Processing Systems (2017).](https://arxiv.org/abs/1706.03762)\n",
    "* [Ramsauer, H., Schafl, B., Lehner, J., Seidl, P., Widrich, M., Gruber, L., Holzleitner, M., Pavlovi'c, M., Sandve, G.K., Greiff, V., Kreil, D.P., Kopp, M., Klambauer, G., Brandstetter, J., & Hochreiter, S. (2020). Hopfield Networks is All You Need. ArXiv, abs/2008.02217.](https://arxiv.org/abs/2008.02217)\n",
    "* [Phuong, M., & Hutter, M. (2022). Formal Algorithms for Transformers. ArXiv, abs/2207.09238.](https://arxiv.org/abs/2207.09238)\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e81d9b",
   "metadata": {},
   "source": [
    "## Embedding Models\n",
    "The overall goal of embedding models is to represent language sequences, e.g., characters, words, documents, etc., in a continuous vector space, where similar words are _close together_ in the embedding space. Let's look at some of the most popular embedding models: the continuous bag-of-words (CBOW) and skip-gram models. \n",
    "* _Key idea_: These CBOW and Skip-Gram models are based on the idea that words that appear in similar contexts tend to have similar meanings. The CBOW model predicts a target word based on its context, while the skip-gram model does the opposite: it predicts the context given a target word.\n",
    "\n",
    "Before we discuss the details of these models, let's introduce some key concepts, terminology, and notation that will be used throughout this lecture.\n",
    "\n",
    "### Vocabulary, Tokens, and Tokenization\n",
    "Let $\\mathcal{V}$ be the vocabulary of tokens (characters, sub-words, whole words, documents, etc) in our [corpus](https://en.wikipedia.org/wiki/Corpus), and let $N_{\\mathcal{V}} = \\dim\\mathcal{V}$ be the size of the vocabulary. Let $\\mathbf{x}\\equiv \\{x_1, x_2, \\ldots, x_n\\in\\mathcal{V}\\}$ be a sequence of tokens in the corpus i.e., a sentence or document, where $n$ is the length of the sequence, and $x_i$ is the $i$-th token in the sequence. \n",
    "\n",
    "Let's consider a simple example: `My grandma makes the best apple pie.`\n",
    "\n",
    "Tokens are the basic units of text that we will be working with. In this space, tokens can be characters, sub-words, whole words, or documents. Converting a sequence of text into tokens is called _tokenization_.\n",
    "* _Character-level tokenization_. Given the example above, one possible choice is to let the vocabulary $\\mathcal{V}$ be the (English) alphabet (plus punctuation). Thus, we’d get a sequence $\\mathbf{x}\\in\\mathcal{V}$ of length 36: `[‘M’, ‘y’, ‘ ’, ..., ’.’]`. Character-level tokenization tends to yield _very long sequences_.\n",
    "* _Word-level tokenization_. Another possible choice is to let the vocabulary $\\mathcal{V}$ be the set of all words in the corpus. Thus, we’d get a sequence $\\mathbf{x}\\in\\mathcal{V}$ of length 8: `[‘My’, ‘grandma’, ‘makes’, ‘the’, ‘best’, ‘apple’, ‘pie’, ‘.’]`. Word-level tokenization tends to yield _shorter sequences_; however, word-level tokenization tends to require an extensive vocabulary and cannot deal with new words at test time.\n",
    "* _Sub-word tokenization_. A third possible choice is to let the vocabulary $\\mathcal{V}$ be the set of commonly occurring word segments like `cious`, `ing`, `pre`. Common words like `is` are often a separate token, and single characters are also included in the vocabulary $\\mathcal{V}$ to ensure all words are expressible.\n",
    "\n",
    "Given a choice of tokenization/vocabulary, each vocabulary element is assigned a unique index $\\left\\{1, 2,\\dots, N_{\\mathcal{V}}-3\\right\\}$. Several special (control) tokens are then added to the vocabulary, let's use `3`, but there could be more:\n",
    "* $\\texttt{mask} \\rightarrow N_{\\mathcal{V}} - 2$: the `mask` token that is used to mask out a token in the input sequence. This is used in training to predict the masked word.\n",
    "* $\\texttt{bos} \\rightarrow N_{\\mathcal{V}} - 1$: the beginning of the sequence (bos) token is used to indicate the start of a sequence. \n",
    "* $\\texttt{eos} \\rightarrow N_{\\mathcal{V}}$: the end of sequence (eos) token is used to indicate the end of a sequence.\n",
    "\n",
    "A piece of text is represented as a sequence of indices (called token IDs) corresponding to its (sub)words, preceded by $\\texttt{bos}$-token and followed by the $\\texttt{eos}$-token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca0920f2-f7c1-49cd-a599-e66511410561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7-element Vector{SubString{String}}:\n",
       " \"My\"\n",
       " \"grandma\"\n",
       " \"makes\"\n",
       " \"the\"\n",
       " \"best\"\n",
       " \"apple\"\n",
       " \"pie.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "let\n",
    "    sentence = \"My grandma makes the best apple pie.\";\n",
    "    sentence |> s-> split(s, \" \")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee2e76e",
   "metadata": {},
   "source": [
    "### Contextual Continuous Bag of Words (CBOW)\n",
    "The Continuous Bag of Words (CBOW) model is a neural network architecture used for learning word embeddings that was popularized by the [word2vec algorithm](https://arxiv.org/abs/1301.3781). \n",
    "\n",
    "* _What is it?_ The CBOW model predicts the probability of a _target word_ based on its surrounding _context words_. The CBOW is encoded as a feedforward neural network with a single hidden layer. The input (context) vector $\\mathbf{x}\\in\\mathbb{R}^{N_{\\mathcal{V}}}$ is a [one-hot encoded vector](https://en.wikipedia.org/wiki/One-hot) representing the _context words_. The output is a _softmax layer_ that computes the probability of the target word given the context.\n",
    "* See: [Rong, X. (2014). word2vec Parameter Learning Explained. ArXiv, abs/1411.2738.](https://arxiv.org/abs/1411.2738)\n",
    "\n",
    "In the simplest case, the the input context vector $\\mathbf{x}\\in\\mathbb{R}^{N_{\\mathcal{V}}}$ is connected to a hidden layer $\\mathbf{h}\\in\\mathbb{R}^{h}$ which is computed using a linear identity transformation, i.e., with no activation function:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{h} &= \\mathbf{W}_{1} \\cdot \\mathbf{x} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\mathbf{W}_{1}\\in\\mathbb{R}^{h\\times{N_{\\mathcal{V}}}}$ is the (unkown) weight matrix of the hidden layer, and $\\mathbf{x}$ is [the one-hot encoded vector](https://en.wikipedia.org/wiki/One-hot) of context word(s) (the input vector). The hidden layer is then mapped through another linear layer:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{u} &= \\mathbf{W}_{2} \\cdot \\mathbf{h} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "which produces the $\\mathbf{u}\\in\\mathbb{R}^{N_{\\mathcal{V}}}$ vector, where $\\mathbf{W}_{2}\\in\\mathbb{R}^{N_{\\mathcal{V}}\\times{h}}$ is the (unknown) weight matrix for the output layer. The output layer is then passed through a softmax activation function to obtain the probability distribution over the vocabulary:\n",
    "$$\n",
    "\\begin{align*}\n",
    "p(w_{i} | \\mathbf{x}) = y_i &= \\frac{e^{\\mathbf{u}_i}}{\\sum_{j=1}^{N_{\\mathcal{V}}} e^{\\mathbf{u}_j}} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "where $p(w_{i} | \\mathbf{x})$ is the probability of observing the $i$-th token, e.g., character, sub-word, word, document, etc in the vocabulary as the output (target) given the context vector $\\mathbf{x}$, the term $N_{\\mathcal{V}}$ is the size of the vocabulary, and $e^{\\mathbf{u}_i}$ is the exponential function applied to the $i$-th element of the vector $\\mathbf{u}$.\n",
    "\n",
    "#### Training\n",
    "The training objective of the CBOW model is to _maximize_ the likelihood of target word(s) given the context words. This is done by _minimizing_ the negative log-likelihood loss function (in this case, a weighted cross-entropy loss) over the training data. The loss function is defined as:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\min\\mathcal{L} &= -\\sum_{i=1}^{N_{\\mathcal{V}}} y_{i}\\cdot\\log p(w_{i} | \\mathbf{x}) \\\\\n",
    "&= -\\sum_{i=1}^{N_{\\mathcal{V}}} y_{i}\\cdot\\log \\left( \\frac{e^{\\mathbf{u}_i}}{\\sum_{j=1}^{N_{\\mathcal{V}}} e^{\\mathbf{u}_j}} \\right) \\\\\n",
    "&= -\\sum_{i=1}^{N_{\\mathcal{V}}} y_{i}\\cdot\\left( \\mathbf{u}_i - \\log \\left( \\sum_{j=1}^{N_{\\mathcal{V}}} e^{\\mathbf{u}_j} \\right) \\right) \\\\\n",
    "&= \\sum_{i=1}^{N_{\\mathcal{V}}} y_{i}\\cdot\\left(\\log \\left( \\sum_{j=1}^{N_{\\mathcal{V}}} e^{\\mathbf{u}_j} \\right) -  \\mathbf{u}_i\\right)\\quad\\text{substitute}~u_{i} = \\langle \\mathbf{w}_{2}^{(i)},\\mathbf{W}_{1}\\cdot\\mathbf{x}\\rangle \\\\\n",
    "&= \\sum_{i=1}^{N_{\\mathcal{V}}} y_{i}\\cdot\\left(\\log \\left( \\sum_{j=1}^{N_{\\mathcal{V}}} e^{\\langle \\mathbf{w}_{2}^{(j)},\\mathbf{W}_{1}\\cdot\\mathbf{x}\\rangle} \\right) -  \\langle \\mathbf{w}_{2}^{(i)},\\mathbf{W}_{1}\\cdot\\mathbf{x}\\rangle\\right)\\blacksquare\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\mathcal{L}$ is the loss function, $y_{i}$ is the $i$-th element of the one-hot encoded vector of the target word(s), $\\mathbf{W}_{1}$ and $\\mathbf{W}_{2}$ are the weight matrices of the hidden and output layers, respectively, and $\\langle \\cdot,\\cdot\\rangle$ is the inner product. Finally, the term $\\mathbf{w}_{2}^{(i)}$ is the $i$-th row of the weight matrix $\\mathbf{W}_{2}$, which corresponds to the target word $w_{i}$.\n",
    "\n",
    "A variety of optimization algorithms can be used to minimize the loss function. Let's implement the CBOW model and mess around with the inputs, hyperparameters, etc, to see how they affect its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8eca43",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Example: CBOW Model of Sarcasm Headlines\n",
    "Let's set up the computational environment for our example, e.g., importing the necessary libraries (and codes), etc, by including the `Include.jl` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd6cf2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"Include.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccb607c",
   "metadata": {},
   "source": [
    "### Sarcasm Data\n",
    "We'll load a public dataset of headlines curated as either sarcastic or not sarcastic. The dataset we'll use is available on [Kaggle](https://www.kaggle.com/datasets/rmisra/news-headlines-dataset-for-sarcasm-detection) and is also discussed in the publications:\n",
    "1. [Misra, Rishabh and Prahal Arora. \"Sarcasm Detection using News Headlines Dataset.\" AI Open (2023).](https://www.sciencedirect.com/science/article/pii/S2666651023000013?via%3Dihub)\n",
    "2. [Misra, Rishabh and Jigyasa Grover. \"Sculpting Data for ML: The first act of Machine Learning.\" ISBN 9798585463570 (2021).](https://rishabhmisra.github.io/Sculpting_Data_for_ML.pdf)\n",
    "\n",
    "The sarcasm data is encoded as a collection of `JSON` records (although it is not directly readable using a JSON parser). Each record has the following fields:\n",
    "* `is_sarcastic`: has a value of `1` if the record is sarcastic; otherwise, `0.`\n",
    "* `headline`: the headline of the article, unstructured text\n",
    "* `article_link`: link to the original news article. Useful in collecting supplementary data\n",
    "\n",
    "We'll load the saved data file that we generated in `L13b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3654a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpusmodel = let\n",
    "\n",
    "    # setup path -\n",
    "    path_to_saved_corpus_file = joinpath(_PATH_TO_DATA, \"L13b-SarcasmSamplesTokenizer-SavedData.jld2\");\n",
    "    saveddata = load(path_to_saved_corpus_file);\n",
    "\n",
    "    # get items from the saveddata -\n",
    "    corpusmodel = saveddata[\"corpus\"];\n",
    "\n",
    "    # return \n",
    "    corpusmodel\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3eda9b19-5a7b-4a29-9093-6d72f46e783f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MySarcasmRecordModel(true, \"thirtysomething scientists unveil doomsday clock of hair loss\", \"https://www.theonion.com/thirtysomething-scientists-unveil-doomsday-clock-of-hai-1819586205\")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpusmodel.records[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41f6ac4",
   "metadata": {},
   "source": [
    "__Constants__. Next, we'll set some constants that will be used throughout the code. See the comment next to each constant for a description of its purpose, permissible values, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b39e598",
   "metadata": {},
   "outputs": [],
   "source": [
    "θ = 0.10; # What percentage of record do we want to train?\n",
    "number_of_records = length(corpusmodel.records)\n",
    "number_of_training_samples = Int64(round(θ*number_of_records)); # θ of the data will be used for training\n",
    "number_of_test_samples = number_of_records - number_of_training_samples; # the rest will be used for testing\n",
    "vocabulary = corpusmodel.tokens; # vocabulary for the corpus\n",
    "inverse_vocabulary = corpusmodel.inverse; # inverse vocabulary for the corpus\n",
    "N = length(vocabulary); # number of tokens in the vocabulary\n",
    "number_of_hidden_states = 100; # number of hidden states\n",
    "number_of_epochs = 20; # number of epochs that we'll use for training\n",
    "array_of_token_ids = range(1, step=1, length=N) |> collect; # array of token ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42a6e03a-7998-4fb3-83b4-9076bcbde176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"why these people of faith are marching for women this weekend\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpusmodel.records[600].headline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00a973c",
   "metadata": {},
   "source": [
    "__Select a context__. Let's set up a context to train the model. Select a headline for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cae1b609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"inclement weather prevents liar from getting to work\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpusmodel.records[4].headline # used 1 and 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434608c7",
   "metadata": {},
   "source": [
    "Next, let's build the training dataset. This will be contained in the `training_dataset::Vector{Tuple{Vector{Float32}, OneHotVector{UInt32}}}` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "408f6d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = let\n",
    "\n",
    "    # specify the context, and the target -\n",
    "    training_dataset = Vector{Tuple{Vector{Float32}, OneHotVector{UInt32}}}();\n",
    "    \n",
    "    # build list of training examples -\n",
    "    for i ∈ 1:number_of_training_samples # use the first number_of_training_samples for records\n",
    "        words = corpusmodel.records[i].headline |> s-> split(s, \" \");\n",
    "\n",
    "        # How many words are in this sentence?\n",
    "        number_of_words = length(words);\n",
    "        idx_target_word = rand(1:number_of_words); # select a random word\n",
    "        idx_context = 1:number_of_words |> collect |> v-> setdiff(v,idx_target_word); # get all the words, excluding the random word\n",
    "\n",
    "        # What is the token_id for the target word?\n",
    "        target_word_token_id = vocabulary[words[idx_target_word]]; # this is the target word\n",
    "        if target_word_token_id == 0 # hack:\n",
    "            target_word_token_id = 1;\n",
    "        end\n",
    "        target_one_hot = onehot(target_word_token_id, array_of_token_ids);\n",
    "       \n",
    "        # build the context vector x\n",
    "        C = length(idx_context);\n",
    "        tmp = zeros(Float32, N, C); # temporary vector for the context words\n",
    "        for j ∈ 1:C\n",
    "            word = words[idx_context[j]]; # get the context word\n",
    "            context_word_id = vocabulary[word]; # get the context word id\n",
    "\n",
    "            # becuase we use 0?\n",
    "            if (context_word_id == 0)\n",
    "                context_word_id = 1;\n",
    "            end\n",
    "            \n",
    "            tmp[context_word_id, j] = 1.0 |> Float32; # set the context word id to 1.0 \n",
    "        end\n",
    "        context_one_hot = (1/C)*sum(tmp, dims=2) |> vec; # sum the context words vector\n",
    "        \n",
    "        D = (context_one_hot, target_one_hot);\n",
    "        push!(training_dataset, D);\n",
    "    end\n",
    "    \n",
    "    # return the training dataset -\n",
    "    training_dataset\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "40020914-839b-43b1-95d0-83b0aa9d7141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2862-element Vector{Tuple{Vector{Float32}, OneHotVector{UInt32}}}:\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
       " ⋮\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6bc09b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7-element Vector{Int64}:\n",
       "  5553\n",
       "  8295\n",
       " 12047\n",
       " 15828\n",
       " 18533\n",
       " 23295\n",
       " 27980"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findall(x-> x!= 0.0, training_dataset[1][1]) # context words vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6f4c886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"loss\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverse_vocabulary[15828]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2576b0",
   "metadata": {},
   "source": [
    "__Setup model__: We will use [the `Flux.jl` package](https://github.com/FluxML/Flux.jl) to encode the CBOW model. The model is a simple feedforward neural network with a single hidden layer. The input layer is a one-hot encoded vector of the context words (average for multiple words), and the output layer is a softmax layer that computes the probability of the target word given the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6fe429aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Uncomment the code below to build the model!\n",
    "Flux.@layer MyFluxNeuralNetworkModel  trainable=(input, hidden); # create a \"namespaced\" of sorts\n",
    "MyModel() = MyFluxNeuralNetworkModel( # a strange type of constructor\n",
    "    Chain(\n",
    "        input = Dense(N, number_of_hidden_states, identity),  # layer 1\n",
    "        hidden = Dense(number_of_hidden_states, N, identity), # layer 2\n",
    "        output = NNlib.softmax) # layer 3 (output layer)\n",
    ");\n",
    "model = MyModel().chain;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f0501a3-783e-4e1b-b82c-3b5828501ba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"00003\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverse_vocabulary[67]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdb7ae5-9261-4272-aacf-90701c9bf53d",
   "metadata": {},
   "source": [
    "What does the untrained model give?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ddd91583-5c7e-4552-b2f5-7abe748a1653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"satin\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "let\n",
    "    x = onehot(67, array_of_token_ids); # give some random word\n",
    "    y = model(x) |> v-> argmax(v) |> i-> inverse_vocabulary[i]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7c6054",
   "metadata": {},
   "source": [
    "__Loss__: The loss function is [the logit cross entropy function](https://fluxml.ai/Flux.jl/stable/reference/models/losses/#Flux.Losses.logitcrossentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65ac063c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(ŷ, y) = Flux.Losses.logitcrossentropy(ŷ, y; agg = mean); # loss for training multiclass classifiers, what is the agg?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca50d906",
   "metadata": {},
   "source": [
    "__Train__: We'll train the model using the gradient descent with momentum optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "24f75fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Epoch $(i) of $(number_of_epochs) completed\" = \"Epoch 10 of 20 completed\"\n",
      "\"Epoch $(i) of $(number_of_epochs) completed\" = \"Epoch 20 of 20 completed\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  input = Dense(29664 => 100),          \u001b[90m# 2_966_500 parameters\u001b[39m\n",
       "  hidden = Dense(100 => 29664),         \u001b[90m# 2_996_064 parameters\u001b[39m\n",
       "  output = NNlib.softmax,\n",
       ") \u001b[90m                  # Total: 4 arrays, \u001b[39m5_962_564 parameters, 22.746 MiB."
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainedmodel = let\n",
    "\n",
    "    localmodel = model; # make a local copy of the model\n",
    "\n",
    "    # setup the optimizer\n",
    "    λ = 0.64; # TODO: maybe change the learning rate (default: 0.61)?\n",
    "    β = 0.10; # TODO: maybe change the momentum parameter (default: 0.10)?\n",
    "    opt_state = Flux.setup(Momentum(λ,β), model);\n",
    "\n",
    "    # training loop -\n",
    "    for i ∈ 1:number_of_epochs\n",
    "        # train the model - check out the do block notion: https://docs.julialang.org/en/v1/base/base/#do\n",
    "        Flux.train!(localmodel, training_dataset, opt_state) do m, x, y\n",
    "            loss(m(x), y) # loss function\n",
    "        end\n",
    "\n",
    "        if (rem(i,10) == 0)\n",
    "            @show \"Epoch $i of $number_of_epochs completed\" # print the epoch number\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # return the trained model -\n",
    "    localmodel;\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b329d75d",
   "metadata": {},
   "source": [
    "__Check__: If we give the context vector to the trained model, and the training was good, we should get the target word back. Let's check this by passing the context vector to the model and checking the output. The output should be a probability distribution over the vocabulary, with the target word having the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ab0d327e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"to\""
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ŷ = trainedmodel(training_dataset[1000][1]); # get the predicted word\n",
    "ŷ |> v-> argmax(v) |> i-> inverse_vocabulary[i] # get the predicted word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3786ca",
   "metadata": {},
   "source": [
    "__Hmmm__. What happens if we change the context? Let's try a few different contexts and see how the model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ccafe900",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_context_example = let\n",
    "\n",
    "    # initialize -\n",
    "    context_words_vector = zeros(Float32, N); # context words vector\n",
    "    list_of_context_words = [\"hair\", \"loss\", \"unveil\"];\n",
    "    C = length(list_of_context_words); # number of context words\n",
    "\n",
    "    tmp = zeros(Float32, N, C); # temporary vector for the context words\n",
    "    for i ∈ eachindex(list_of_context_words)\n",
    "        word = list_of_context_words[i]; # get the context word\n",
    "        context_word_id = vocabulary[word]; # get the context word id\n",
    "        tmp[context_word_id, i] = 1.0; # set the context word id to 1.0        \n",
    "    end\n",
    "    context_one_hot = (1/C)*sum(tmp, dims=2) |> vec .|> Float32; # sum the context words vector\n",
    "\n",
    "    # return -\n",
    "    context_one_hot;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae874c9",
   "metadata": {},
   "source": [
    "What are going to see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2c04173f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"to\""
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "let\n",
    "    ŷ = trainedmodel(test_context_example); # get the predicted word\n",
    "    ŷ |> v-> argmax(v) |> i-> inverse_vocabulary[i] # get the predicted word\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52884ff7",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Lab: The Skip-Gram Model\n",
    "In lab `L14b`, we'll examine the skip-gram model, a neural network-based approach to natural language processing designed to learn word embeddings by predicting the _surrounding context words_ given a _target word_ within a fixed window in a text corpus. \n",
    "* _What is it?_ A skip-gram model consists of a single hidden layer that transforms a one-hot encoded input word into a dense vector representation, optimizing the embedding so that words appearing in similar contexts have similar vector representations. Imagine you're reading a sentence and can guess the words that come before and after a particular word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c8154a",
   "metadata": {},
   "source": [
    "# Today?\n",
    "That's a wrap! What are some of the interesting things we discussed today?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8db5313",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.5",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
