{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "680153d9-f769-4e75-a5b2-e4faa27ca9a4",
   "metadata": {},
   "source": [
    "# L14c: Introduction to Attention Mechanisms\n",
    "This lecture will explore the concept of _attention mechanisms_ in deep learning. Attention mechanisms have become a fundamental component in various neural network architectures, particularly in natural language processing (NLP), because of their role in _transformer_ models. \n",
    "\n",
    "The key concepts in this lecture include:\n",
    "\n",
    "* __Transformer__: Transformers are a neural network architecture that has revolutionized natural language processing by using _attention mechanisms_ to capture relationships between words in a sequence, regardless of distance.\n",
    "* __Scaled Dot-Product Attention__: A specific type of attention mechanism that computes the attention scores using the dot product of _query_ and key _vectors_, scaled by the square root of the dimension of the key vectors. This helps to stabilize gradients during training.\n",
    "\n",
    "This lecture is based on the papers:\n",
    "* [Rong, X. (2014). word2vec Parameter Learning Explained. ArXiv, abs/1411.2738.](https://arxiv.org/abs/1411.2738)\n",
    "* [Vaswani, Ashish, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin. “Attention is All You Need.” Neural Information Processing Systems (2017).](https://arxiv.org/abs/1706.03762)\n",
    "* [Ramsauer, H., Schafl, B., Lehner, J., Seidl, P., Widrich, M., Gruber, L., Holzleitner, M., Pavlovi'c, M., Sandve, G.K., Greiff, V., Kreil, D.P., Kopp, M., Klambauer, G., Brandstetter, J., & Hochreiter, S. (2020). Hopfield Networks is All You Need. ArXiv, abs/2008.02217.](https://arxiv.org/abs/2008.02217)\n",
    "* [Phuong, M., & Hutter, M. (2022). Formal Algorithms for Transformers. ArXiv, abs/2207.09238.](https://arxiv.org/abs/2207.09238)\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92537abc",
   "metadata": {},
   "source": [
    "## What is a Transformer?\n",
    "The transformer is a neural network architecture that was introduced in [the paper \"Attention is All You Need\" by Vaswani et al. in 2017](https://arxiv.org/abs/1706.03762). It has since become the foundation for many state-of-the-art natural language processing (NLP) models and other domains.\n",
    "* _Core innovation_: The Transformer's core innovation lies in its departure from traditional architectures like Recurrent Neural Networks (RNNs) and Long-Short-Term Memory networks (LSTMs), which process sequences step-by-step, making it inherently difficult to capture long-range dependencies and hindering parallel computation.\n",
    "\n",
    "The Transformer architecture employed a novel approach, dispensing entirely with recurrence, called _attention_ to relate different sequence positions to compute a sequence representation. \n",
    "\n",
    "Let's watch a short video from Google Research that explains the Transformer architecture: [Click me!](https://www.yout-ube.com/watch?v=SZorAJ4I-sA&t=5s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c017908",
   "metadata": {},
   "source": [
    "## What is Attention? \n",
    "Attention is a technique that allows a model to focus on specific parts of the input sequence when making predictions. It helps the model weigh the importance of different input elements dynamically. This is particularly useful in tasks where the input sequence can be long and complex, such as machine translation or text summarization.\n",
    "\n",
    "Let's watch a short video from Google Research that introduces the concept of attention on a sequence-to-sequence problem. [Click me!](https://www.yout-ube.com/watch?v=fjJOgb-E41w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241fc4ad",
   "metadata": {},
   "source": [
    "### Scaled Dot-Product Attention\n",
    "The scaled dot-product attention mechanism computes the attention scores using the dot product of query and key vectors, scaled by the square root of the dimension of the key vectors:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Attention}(Q, K, V) &= \\text{softmax}\\left(\\frac{QK^{\\top}}{\\sqrt{d_k}}\\right)V\n",
    "\\end{align*}\n",
    "$$\n",
    "where: $\\mathbf{Q}$ is the query matrix, $\\mathbf{K}$ is the key matrix, $\\mathbf{V}$ is the value matrix, and $d_k$ is the dimension of the key vectors.\n",
    "\n",
    "* __Query matrix__: The query matrix $\\mathbf{Q}$ contains the queries (embeddings) for which we want to compute the attention scores, i.e., the sequence of tokens we are looking at. \n",
    "* __Key matrix__: The key matrix $\\mathbf{K}$ contains the keys (embeddings) we want to compare against the queries. \n",
    "* __Value matrix__: The value matrix $\\mathbf{V}$ contains the values we want to retrieve based on the attention scores.\n",
    "\n",
    "#### What??\n",
    "Ok, but what is in the query, key, and value matrices? The query, key, and value matrices are obtained by applying linear transformations to the input embeddings. For example, if we have an input sequence of word embeddings $\\mathbf{X}\\in\\mathbb{R}^{T\\times{h}}$, where $T$ is the number of tokens (words) in the sequence and $h$ is the dimension of the embeddings, then we can obtain the query, key and value matrices as follows:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{Q} &= \\mathbf{X}\\mathbf{W}_Q \\\\\n",
    "\\mathbf{K} &= \\mathbf{X}\\mathbf{W}_K \\\\\n",
    "\\mathbf{V} &= \\mathbf{X}\\mathbf{W}_V\n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\mathbf{W}_Q$, $\\mathbf{W}_K$, and $\\mathbf{W}_V$ are (unknown) weight matrices for the query, key, and value transformations, respectively. \n",
    "\n",
    "So, how do we calculate the attention scores (assuming we have the weights)? [Let's check out algorithm 4 of Phuong and Hutter (2022)](https://arxiv.org/abs/2207.09238)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2104716d",
   "metadata": {},
   "source": [
    "## Lab: Modeling Attention\n",
    "In lab `L14d`, we will model the attention-scaled dot-product attention mechanism and show that the update rule in [a Modern Hopfield Network](https://arxiv.org/abs/2008.02217) is equivalent to the scaled-dot product attention mechanism. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4958f30a",
   "metadata": {},
   "source": [
    "# Today?\n",
    "That's a wrap! What are some of the interesting things we discussed today?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
