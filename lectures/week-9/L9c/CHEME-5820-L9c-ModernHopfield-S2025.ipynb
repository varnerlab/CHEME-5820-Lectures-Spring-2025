{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74021a3c-bd9e-4bd3-b5dd-c3f8103d294e",
   "metadata": {},
   "source": [
    "# L9c: Modern Hopfield Networks\n",
    "In this lecture, we will discuss the modern Hopfield networks, which are a type of recurrent neural network that can be used for associative memory tasks. Modern Hopfield networks are a generalization of the original Hopfield network. The modern Hopfield network extends the original concept to continuous data and allows for the much more storage than the original approach. The key ideas of this lecture are:\n",
    "\n",
    "* __Modern Hopfield networks__, also known as Dense Associative Memories, are generalizations of classical Hopfield networks that expand their capabilities to store and retrieve high-dimensional discrete and continuous data. They are designed to overcome the limitations of classical Hopfield networks, which are constrained by linear scaling relationships between the number of input features and the number of stored memories, discrete representations, and sensitivity to correlated patterns.\n",
    "* __Non-linear energy functions__: Modern Hopfield networks utilize strongly non-linear energy functions. These functions allow for the storage of a larger number of patterns than classical Hopfield networks, which are limited by linear scaling relationships. Further, they allow the storage and retrieval of high-dimensional continuous (potentially correlated, noisy) data.\n",
    "* __Theorectical guarantees__: The theoretical guarantees of modern Hopfield networks are based on the concept of _attractor dynamics_. This means that the network will converge to a stored pattern when presented with a noisy or incomplete version of that pattern. Further, the network will convergence after one update step (each update step corresponds to visiting each neuron once).\n",
    "\n",
    "Background reading for this lecture (and the associated lab) can be found from the following sources:\n",
    "* [Krotov, D., & Hopfield, J.J. (2016). Dense Associative Memory for Pattern Recognition. ArXiv, abs/1606.01164.](https://arxiv.org/abs/1606.01164)\n",
    "* [Demircigil, M., Heusel, J., LÃ¶we, M., Upgang, S., & Vermet, F. (2017). On a Model of Associative Memory with Huge Storage Capacity. Journal of Statistical Physics, 168, 288 - 299.](https://arxiv.org/abs/1702.01929)\n",
    "* [Ramsauer, H., Schafl, B., Lehner, J., Seidl, P., Widrich, M., Gruber, L., Holzleitner, M., Pavlovi'c, M., Sandve, G.K., Greiff, V., Kreil, D.P., Kopp, M., Klambauer, G., Brandstetter, J., & Hochreiter, S. (2020). Hopfield Networks is All You Need. ArXiv, abs/2008.02217.](https://arxiv.org/abs/2008.02217)\n",
    "\n",
    "Today, we are going to walkthrough the following blog post: [Hopfield Networks is All You Need Blog, GitHub.io](https://ml-jku.github.io/hopfield-layers/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca32950-f6d0-4ca2-b1ce-7ca8142798ca",
   "metadata": {},
   "source": [
    "## Lab\n",
    "In Lab `L9d` we will implement a modern Hopfield network and revisit the image storage and retrival task discussed in `L9a` and `L9b`. However, we'll use [the Simpsons MNIST dataset](https://github.com/alvarobartt/simpsons-mnist) instead of the handwritten images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30391152",
   "metadata": {},
   "source": [
    "# Today?\n",
    "That's a wrap! What are some of the interesting things we discussed today?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2dd037",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.4",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
