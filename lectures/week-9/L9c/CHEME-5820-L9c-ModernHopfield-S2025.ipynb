{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74021a3c-bd9e-4bd3-b5dd-c3f8103d294e",
   "metadata": {},
   "source": [
    "# L9c: Modern Hopfield Networks\n",
    "In this lecture, we will discuss the modern Hopfield networks, which are a type of recurrent neural network that can be used for associative memory tasks. Modern Hopfield networks are a generalization of the original Hopfield network. The modern Hopfield network extends the original concept to continuous data and allows for the much more storage than the original approach. The key ideas of this lecture are:\n",
    "\n",
    "* __Modern Hopfield networks__, also known as Dense Associative Memories, are generalizations of classical Hopfield networks that break the linear scaling relationship between the number of input features and the number of stored memories. These networks introduce non-linearities in their energy functions, enabling them to achieve super-linear (or even exponential) memory storage capacity as a function of the number of input features.\n",
    "* __Modern Hopfield networks store continuous-valued inputs__: by employing energy-minimizing dynamics in continuous state spaces, where synaptic interactions between _feature neurons_ and _memory neurons_ drive the system toward fixed points representing stored patterns. These networks achieve exponential memory capacity by using strong non-linearities in their energy functions, such as exponential interaction terms (which enables efficient storage and retrieval of high-dimensional continuous data through update rules similar to transformer attention mechanisms).\n",
    "* __Different neuron types__: Feature neurons represent the _input patterns_, while memory neurons store the _learned patterns_ and facilitate the retrieval process. The interaction between feature and memory neurons, governed by non-linear energy functions and update rules, enables modern networks to overcome the limitations of classical Hopfield models.\n",
    "\n",
    "\n",
    "The source for the lecture (and associated lab) was taken from the following sources:\n",
    "* [Krotov, D., & Hopfield, J.J. (2016). Dense Associative Memory for Pattern Recognition. ArXiv, abs/1606.01164.](https://arxiv.org/abs/1606.01164)\n",
    "* [Demircigil, M., Heusel, J., LÃ¶we, M., Upgang, S., & Vermet, F. (2017). On a Model of Associative Memory with Huge Storage Capacity. Journal of Statistical Physics, 168, 288 - 299.](https://arxiv.org/abs/1702.01929)\n",
    "* [Ramsauer, H., Schafl, B., Lehner, J., Seidl, P., Widrich, M., Gruber, L., Holzleitner, M., Pavlovi'c, M., Sandve, G.K., Greiff, V., Kreil, D.P., Kopp, M., Klambauer, G., Brandstetter, J., & Hochreiter, S. (2020). Hopfield Networks is All You Need. ArXiv, abs/2008.02217.](https://arxiv.org/abs/2008.02217)\n",
    "* [ Hopfield Networks is All You Need Blog, GitHib.io](https://ml-jku.github.io/hopfield-layers/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca32950-f6d0-4ca2-b1ce-7ca8142798ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.4",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
