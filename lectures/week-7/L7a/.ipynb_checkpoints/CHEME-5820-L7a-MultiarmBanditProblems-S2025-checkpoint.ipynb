{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ad1b5f5",
   "metadata": {},
   "source": [
    "# L7a: Multiarm Bandit Problems\n",
    "In this lecture, we continue our discussion of online learning and will focus on the multiarm bandit problem. This lecture will introduce the following key concepts:\n",
    "* __Bandit problems__ are a class of online learning problems where an agent must decide which arm (choice) to pull from a set of possible options to maximize its reward during each turn of the game. Alternatively, the agent must decide which choice to make to minimize its regret. This is similar to the multiplicative weights algorithm we discussed in the previous lecture. In a bandit problem, the agent is the aggregator, the arms (choices) are the experts, and the adversary is nature.\n",
    "* __Regret__ is the difference between the reward the agent would have received if it had chosen the best arm every turn (in hindsight) and the reward the agent _actually_ received. Thus, the goal of an agent in a bandit problem is to maximize its return, while the objective of the algorithm designer is to minimize the agent's regret. If we only had a time machine, we could return and make better choices.\n",
    "* __Exploration vs. exploitation__ is the key tradeoff in bandit problems. The agent must decide whether to explore (try new arms) or exploit (choose the arm that has been the best so far) during each round. This is a key tradeoff in many online learning problems. If the agent always exploits, it may miss out on better arms. However, if the agent constantly explores, it may not take advantage of the best arm it has found so far.\n",
    "\n",
    "### Notes for this week\n",
    "* The lecture notes for this week were taken from Chapter 1 of \"Introduction to Multi-Armed Bandits\" by Aleksandrs Slivkins. This is an excellent resource (albeit quite technical) for learning more about bandit problems. [The book is available online](https://arxiv.org/abs/1904.07272) and is hosted on our [course GitHub repository](https://github.com/varnerlab/CHEME-5820-Lectures-Spring-2025/blob/main/lectures/week-7/L7a/docs/Slivkins-Bandits-MSFT-Research-2024.pdf). We also drew material from the [Bandit problem Thompson sampling tutorial by Russo et al., 2020](https://arxiv.org/abs/1707.02038)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a912bec5",
   "metadata": {},
   "source": [
    "## What is a Bandit Problem?\n",
    "A bandit problem is another example of an online learning problem, i.e., a sequential decision problem in which an agent (us) makes a choice.\n",
    "* The agent chooses from $K$ alternatives (somehow) and executes the chosen action. A positive reward is given if the choice at time $t$ was _good_ (compared to an adversary, i.e., nature). Alternatively, if the choice is _bad_ a penalty is given. The agent must decide which arm to pull (choice to make) to maximize its reward.\n",
    "\n",
    "Bandit problems are used in a _wide_ range of applications. Here are a few examples:\n",
    "* __Clinical Trials__: Multi-armed bandits optimize the allocation of patients to different treatments, balancing the need to gather information about new therapies with minimizing patient losses. This is a real-life example of the exploration-exploitation tradeoff.\n",
    "* __Financial Portfolio Design__: This approach helps allocate investments dynamically across different assets to maximize returns while exploring new investment opportunities.\n",
    "* __Adaptive Routing__: Bandit algorithms optimize routing decisions in networks, minimizing delays by balancing the exploration of new routes with the exploitation of known efficient paths. For us, networks could be the Internet, a metabolic network, a production network, etc.\n",
    "* __Recommendation systems__: Multi-armed bandits personalize recommendations by iteratively selecting items to display and balancing exploring new items with exploiting well-performing ones. We encounter this in applications like Netflix, Amazon, etc.\n",
    "\n",
    "For more information on the applications of bandit problems, [see the survey by Bouneffouf and Rish, 2019](https://arxiv.org/abs/1904.10040)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c30183",
   "metadata": {},
   "source": [
    "## Stochastic Multi-Armed Bandits\n",
    "In the stochastic multi-armed bandit problem, the agent must choose an action $a$ from the set of all possible actions $\\mathcal{A}$, where $\\dim\\mathcal{A} = K$ during each round $t = 1,2,\\dots, T$ of the game or task. The agent receives a reward $r_{a}$ from the environment, where $r_{a}$ is sampled from some unknown distribution $\\mathcal{D}_{a}$\n",
    "\n",
    "For $t = 1,2,\\dots,T$:\n",
    "1. _Aggregator_: The agent picks an action $a_{t} \\in \\mathcal{A}$. How the agent makes this choice is one of the main differences between the different algorithms for solving this problem. \n",
    "2. _Adeversary_: The agent implements action $a_{t}$ and receives a reward $r_{t}\\in\\left[0,1\\right]$ sampled from the (unknown) distribution $\\mathcal{D}_{a}\\mid a = a_{t}$.\n",
    "3. Agent observes $r_{t}$, but nothing else. It cannot see the distribution $\\mathcal{D}_{a}$; only the _adversary_ can see this.\n",
    "\n",
    "The agent is interested in learning the mean of the reward distribution of each arm, $\\mu(a) = \\mathbb{E}\\left[r_{t}\\sim\\mathcal{D}_{a}\\right]$, by experimenting against the world (adversary). The goal of the agent is to maximize the total reward. However, the goal of the algorithm designer is to minimize the _regret_ of the algorithm that the agent uses to choose $a\\in\\mathcal{A}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6860468",
   "metadata": {},
   "source": [
    "## Regret\n",
    "Regret measures the difference between what could have been achieved by always making the best decision, i.e., the decision that maximizes reward (in hindsight), and what the agent actually chooses to do during the round. \n",
    "* __Perspective__: Regret is a property of the algorithm, not the agent (which only cares about the reward). Each decision-making framework the agent employs may lead to a different bound on the regret. Thus, the goal of the algorithm designer is to minimize the regret of the agent's algorithm.\n",
    "\n",
    "__Definition__: _Regret_. Let $\\mu^{\\star}$ be the mean of the best arm, i.e., $\\mu^{\\star} = \\max_{a\\in\\mathcal{A}}\\mu(a)$ after playing the game for $T$ rounds. The regret $R(T)$ of an algorithm after $T$ rounds is defined as:\n",
    "$$\n",
    "\\begin{align*}\n",
    "R(T) = T\\cdot\\mu^{\\star} - \\sum_{t=1}^{T}\\mu(a_{t})\n",
    "\\end{align*}\n",
    "$$\n",
    "The first term is the reward that would have been obtained if the best arm was always chosen over the $T$ rounds.cThe second term is the mean reward obtained by the agent over the $T$ rounds, where $a_{t}$ is the action chosen by the agent at round $t$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d86484",
   "metadata": {},
   "source": [
    "## Uniform Exploration\n",
    "A straightforward approach to the multi-armed bandit problem is to explore each arm equally. This is called _uniform exploration_ or the explore first algorithm. In this approach, the agent begins with a purely _exploratory phase_, pulling each arm $N$ times. After this exploration phase, the agent selects the arm with the highest mean reward for the rest of the game. This is called the _exploitation phase_.\n",
    "___\n",
    "\n",
    "### Explore First Algorithm\n",
    "The agent has $K$ arms, $\\mathcal{A} = \\left\\{1,2,\\dots,K\\right\\}$, and the total number of rounds is $T$. The agent uses the following algorithm to choose which arm to pull during each round:\n",
    "1. _Initialization_: For each arm $a\\in\\mathcal{A}$, set $N_{a} = N$ where $N = (T/K)^{2/3}\\cdot\\mathcal{O}\\left(\\log{T}\\right)^{1/3}$.\n",
    "2. _Exploration_: Play each arm $a\\in\\mathcal{A}$ for $N_{a}$ rounds and record the rewards. After the exploration phase, select the arm $a^{\\star}$ with the highest mean reward (break ties arbitrarily).\n",
    "3. _Explotation_: Play arm $a^{\\star}$ for the remaining rounds.\n",
    "\n",
    "__Theorem__: The _expected_ regret over $T$ rounds of the _uniform exploration_ algorithm is bounded by $\\mathbb{E}\\left[R(T)\\right]\\leq{T}^{2/3}\\times\\mathcal{O}\\left(K\\cdot\\log{T}\\right)^{1/3}$, where $K$ is the number of arms, $T$ is the total number of rounds and $N = (T/K)^{2/3}\\cdot\\mathcal{O}\\left(\\log{T}\\right)^{1/3}$ is the number of rounds in the exploration phase for each action (choice).\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04374a71",
   "metadata": {},
   "source": [
    "## Epsilon-Greedy Exploration\n",
    "One issue with the _uniform exploration_ algorithm is that it may not be the best choice for all problems. For example, the performance in the exploration phase may be _bad_ if many of the arms have a large gap $\\Delta({a})$:\n",
    "* _What is this gap_? Let the (true) mean reward for each arm be $\\mu(a) = \\mathbb{E}\\left[r_{t}\\sim\\mathcal{D}_{a}\\right]$, where $a\\in\\mathcal{A}$. The _best_ mean reward over the actions is $\\mu^{\\star} = \\max_{a\\in\\mathcal{A}}\\mu_{a}$. Then, the gap $\\Delta({a}) = \\mu^{\\star} - \\mu(a)$ is the difference between the mean reward of the best arm and the mean reward of arm $a$. If the gap is _large_, the agent may miss out on many rewards by exploring each arm equally.\n",
    "\n",
    "In a large gap, it may be better to spread out (and interleave) the exploration and exploitation phases of the arms. This is the idea behind the _epsilon-greedy_ algorithm. In this algorithm, the agent chooses the best arm with probability $1-\\epsilon$ and a random arm with probability $\\epsilon$. This allows the agent to explore the arms more evenly and may lead to better performance in cases where the gap is large.\n",
    "\n",
    "While [Slivkins](https://arxiv.org/abs/1904.07272) doesn't give a reference for the epsilon-greedy algorithm, other sources point to (at least in part) to [Thompson and Thompson sampling, proposed in 1933 in the context of drug trials](https://arxiv.org/abs/1707.02038).\n",
    "\n",
    "___\n",
    "### Epsilon-Greedy Algorithm\n",
    "The agent has $K$ arms (choices), $\\mathcal{A} = \\left\\{1,2,\\dots,K\\right\\}$, and the total number of rounds is $T$. The agent uses the following algorithm to choose which arm to pull (which action to take) during each round:\n",
    "\n",
    "For $t = 1,2,\\dots,T$:\n",
    "1. _Initialize_: Roll a random number $p\\in\\left[0,1\\right]$ and compute a threshold $\\epsilon_{t}\\sim{t}^{-1/3}$. Note, in other sources, $\\epsilon$ is a constant, not a function of $t$.\n",
    "2. _Exploration_: If $p\\leq\\epsilon_{t}$, choose a random (uniform) arm $a_{t}\\in\\mathcal{A}$. Execute the action $a_{t}$ and receive a reward $r_{t}$ from the _adversary_ (nature). \n",
    "3. _Exploitation_: Else if $p>\\epsilon_{t}$, choose action $a^{\\star}$ (action with the highest average reward so far, the greedy choice). Execute the action $a^{\\star}_{t}$ and recieve a reward $r_{t}$ from the _adversary_ (nature).\n",
    "4. Update list of rewards for $a_{t}\\in\\mathcal{A}$\n",
    "\n",
    "__Theorem__: The epsilon-greedy algowithm with exploration probability $\\epsilon_{t}={t^{-1/3}}\\cdot\\left(K\\cdot\\log(t)\\right)^{1/3}$ achives a regret bound of $\\mathbb{E}\\left[R(t)\\right]\\leq{t}^{2/3}\\cdot\\left(K\\cdot\\log(t)\\right)^{1/3}$ for each round $t$.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b98b11",
   "metadata": {},
   "source": [
    "## Optimism under uncertainty\n",
    "Let's consider the final approach for solving bandit problems for today: optimism under uncertainty algorithm. The key assumption of this approach:\n",
    "* __Assumption__: each arm is as good as it can be given the observations so far, and choose the best arm based on these optimistic estimates. This intuition leads to the `UCB1` algorithm.\n",
    "\n",
    "Given a history of rewards and the number of pulls for each arm, the `UCB1` algorithm calculates an upper confidence bound (UCB) and uses it to decide which arm to pull. \n",
    "\n",
    "__Definition__: Upper Confidence Bound (UCB). During each round $t =1,2, \\dots, T$ the `UCB1` algorithm maximizes the sum $\\mu(a)+U(a,t)$ where $\\mu(a)$ is the _estimated_ mean return of arm $a\\in\\mathcal{A}$ at time $t$ and $U(a,t)$ is the _upper confidence bound_ of arm $a\\in\\mathcal{A}$ at time $t$:\n",
    "$$\n",
    "\\begin{align*}\n",
    "U(a,t) = \\sqrt{\\frac{2\\log(t)}{N_{a}}}\n",
    "\\end{align*}\n",
    "$$\n",
    "where $N_{a}$ is the number of times that arm $a\\in\\mathcal{A}$ has been pulled up to time $t$. The `UCB1` algorithm chooses the arm $a^{\\star}$ that maximizes the sum $\\bar{\\mu}(a)+U(a,t)$ during each round $t$. The `UCB1` algorithm was originally proposed by Auer, Cesa-Bianchi, and Fischer in 2002:\n",
    "* [Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem. Machine\n",
    "Learning, 47(2-3):235–256, 2002a.](https://people.eecs.berkeley.edu/~russell/classes/cs294/s11/readings/Auer+al:2002.pdf)\n",
    "\n",
    "___\n",
    "### UCB1 Algorithm\n",
    "The agent has $K$ arms (choices), $\\mathcal{A} = \\left\\{1,2,\\dots,K\\right\\}$, and a total number of rounds is $T\\gg{K}$.\n",
    "\n",
    "_Initialization_: Pull each arm $a\\in\\mathcal{A}$ once and record the rewards. For each arm $a\\in\\mathcal{A}$, set $N_{a} = 1$ and $\\bar{\\mu}(a) = r_{a}$.\n",
    "\n",
    "For rounds $t = K+1,K+2,\\dots,T$:\n",
    "1. Compute the upper confidence bound $U(a,t)$ for each arm $a\\in\\mathcal{A}$.\n",
    "2. Choose the best arm $a^{\\star} = \\text{arg}\\max\\,\\left\\{\\bar{\\mu}(a)+U(a,t)\\mid\\,a\\in\\mathcal{A}\\right\\}$ at time $t$.\n",
    "3. Execute the action $a^{\\star}$ and recieve a reward $r_{t}$ from the _adversary_ (nature).\n",
    "4. Update the estimated mean reward $\\bar{\\mu}(a^{\\star})$ and the number of pulls $N_{a^{\\star}}$\n",
    "\n",
    "__Theorem__: The `UCB1` algorithm achieves a regret bound for $K$ arms of $\\mathbb{E}\\left[R(t)\\right]\\leq\\mathcal{O}\\left(\\sqrt{KT\\cdot\\log(T)}\\right)$ \n",
    "over $T$ rounds.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff0f5e8",
   "metadata": {},
   "source": [
    "## Lab\n",
    "In this week's lab, we will implement the `UCB1` algorithm and compare its performance to the `explore-first` and `epsilon-greedy` algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6f05bd",
   "metadata": {},
   "source": [
    "# Today?\n",
    "That's a wrap! What are some of the interesting things we discussed today?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
