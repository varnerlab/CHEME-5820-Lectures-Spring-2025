{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfd1f87b-0e96-4069-b915-6552e2a56a7d",
   "metadata": {},
   "source": [
    "# L15a: What comes after Transformers and LLMs?\n",
    "In this lecture, we'll speculate (wildly) about what might come after Transformers and Large Language Models (LLMs). For more information, see the conversation [between Yann LeCun and Bill Dally | NVIDIA GTC 2025](https://www.youtube.com/watch?v=eyrDM3A_YFc).\n",
    "\n",
    "Transformers have been an enormous success, but they are not the end of the line. Many other architectures and techniques could be used to build even more powerful models. Let's explore a few of these possibilities, [see the review paper Schneider, J. (2024). What comes after Transformers? - A selective survey connecting ideas in deep learning. ArXiv, abs/2408.00386.)](https://arxiv.org/abs/2305.13936) for a more in-depth discussion.\n",
    "\n",
    "The Schneider paper reviews several approaches proposed as alternatives to Transformers, as well as far-out ideas for future architectures. In this lecture, we'll explore a few of these ideas:\n",
    "\n",
    "* __State Space Models (SSMs)__ are an emerging alternative to transformers for sequence modeling, using a fixed-size latent state that enables efficient processing of extremely long inputs, such as entire books or audio streams, without the quadratic computational cost of attention mechanisms. While SSMs like Mamba can match or even outperform transformers at small to medium scale, they are generally less effective than transformers at tasks requiring selective attention or copying from specific parts of the input, due to their reliance on compressing information into a fixed-size state rather than dynamically attending to all previous tokens.\n",
    "* __Spiking Neural Networks (SNNs)__ are brain-inspired models that process information using discrete spikes over time. They offer energy-efficient and biologically plausible alternatives to transformers, especially for tasks with strong temporal dynamics. By leveraging event-driven computation and sparse communication, SNNs can achieve high computational efficiency and are particularly well-suited for deployment on [neuromorphic hardware](https://en.wikipedia.org/wiki/Neuromorphic_computing), addressing some of the limitations of transformer architectures in terms of power consumption and real-time processing.\n",
    "\n",
    "The sources used to prepare this lecture are:\n",
    "* [Schneider, J. (2024). What comes after Transformers? - A selective survey connecting ideas in deep learning. ArXiv, abs/2408.00386](https://arxiv.org/abs/2305.13936)\n",
    "* [Smith, J., Warrington, A., & Linderman, S.W. (2022). Simplified State Space Layers for Sequence Modeling. ArXiv, abs/2208.04933.](https://arxiv.org/abs/2208.04933)\n",
    "* [Limbacher, T., Özdenizci, O., & Legenstein, R.A. (2022). Memory-enriched computation and learning in spiking neural networks through Hebbian plasticity. ArXiv, abs/2205.11276.](https://arxiv.org/abs/2205.11276)\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcea94e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/Desktop/julia_work/CHEME-5820-SP25/CHEME-5820-Lectures-Spring-2025/lectures/week-15/L15a`\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `~/Desktop/julia_work/CHEME-5820-SP25/CHEME-5820-Lectures-Spring-2025/lectures/week-15/L15a/Project.toml`\n",
      "  \u001b[90m[37e2e46d] \u001b[39m\u001b[93m~ LinearAlgebra ⇒ v1.11.0\u001b[39m\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `~/Desktop/julia_work/CHEME-5820-SP25/CHEME-5820-Lectures-Spring-2025/lectures/week-15/L15a/Manifest.toml`\n",
      "  \u001b[90m[56f22d72] \u001b[39m\u001b[92m+ Artifacts v1.11.0\u001b[39m\n",
      "  \u001b[90m[8f399da3] \u001b[39m\u001b[92m+ Libdl v1.11.0\u001b[39m\n",
      "  \u001b[90m[37e2e46d] \u001b[39m\u001b[92m+ LinearAlgebra v1.11.0\u001b[39m\n",
      "  \u001b[90m[e66e0078] \u001b[39m\u001b[92m+ CompilerSupportLibraries_jll v1.1.1+0\u001b[39m\n",
      "  \u001b[90m[4536629a] \u001b[39m\u001b[92m+ OpenBLAS_jll v0.3.27+1\u001b[39m\n",
      "  \u001b[90m[8e850b90] \u001b[39m\u001b[92m+ libblastrampoline_jll v5.11.0+0\u001b[39m\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Desktop/julia_work/CHEME-5820-SP25/CHEME-5820-Lectures-Spring-2025/lectures/week-15/L15a/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Desktop/julia_work/CHEME-5820-SP25/CHEME-5820-Lectures-Spring-2025/lectures/week-15/L15a/Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "include(\"Include.jl\"); # we'll need a couple of packages later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b628213",
   "metadata": {},
   "source": [
    "## Review: S4 models\n",
    "Structured State Space Sequence (S4) models are examples of linear time-invariant (LTI) state space models that represent a system's dynamics over time using a system of linear ordinary differential equations:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\dot{\\mathbf{x}} &= \\mathbf{A} \\mathbf{x} + \\mathbf{B} \\mathbf{u} \\\\\n",
    "\\mathbf{y} &= \\mathbf{C} \\mathbf{x} + \\mathbf{D} \\mathbf{u}\n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\mathbf{x}\\in\\mathbb{R}^{h}$ is an $h$-dimensional state vector, $\\mathbf{u}\\in\\mathbb{R}^{d_{in}}$ is the $d_{in}$ dimensional input vector, $\\mathbf{y}\\in\\mathbb{R}^{d_{out}}$ is the $d_{out}$ dimensional output vector. The LTI system is defined by the system matrices (and the initial state and input):\n",
    "* The $\\mathbf{A}\\in\\mathbb{R}^{h\\times{h}}$ matrix is the state transition matrix, which describes how the state depends upon itself over time.\n",
    "* The $\\mathbf{B}\\in\\mathbb{R}^{h\\times{d_{in}}}$ matrix is the input matrix, which describes how the input vector affects the state.\n",
    "* The $\\mathbf{C}\\in\\mathbb{R}^{d_{out}\\times{h}}$ matrix is the output matrix, which describes how the state affects the output vector.\n",
    "* The $\\mathbf{D}\\in\\mathbb{R}^{d_{out}\\times{d_{in}}}$ matrix is the feedforward matrix, which describes how the input vector affects the output vector.\n",
    "\n",
    "Linear time-invariant state space models have been widely used in control theory, signal processing, and other fields. You may be familiar with these models from your automatic control class, where they are used to model system dynamics. In this lecture, we will focus on the discrete-time version of these models, which are often used in machine learning and signal processing applications.\n",
    "* __Single Input Single Output (SISO)__: The simplest case of a linear time invariant state space model is the single input single output (SISO) case, where there is one input $d_{in} = 1$ and one output $d_{out} = 1$ _per time step_. In this case, the system can be represented by a single transfer function, which describes the relationship between the input and output.\n",
    "* __Multiple Input Multiple Output (MIMO)__: In the multiple input multiple output (MIMO) case, $d_{in}>1$ and $d_{out}>1$. Thus, vectors are input and output at each time step.\n",
    "\n",
    "### SISO S4 Leg-S matrices\n",
    "The S4 Leg-S matrices are a specific type of structured state space model designed to capture long-range dependencies in sequential data efficiently. The Leg-S approach is based on [Legendre polynomials](https://en.wikipedia.org/wiki/Legendre_polynomials), which are a set of _orthogonal polynomials_ that can be used to represent functions over the finite interval $[-1,1]$. The Leg-S HiPPO state transition matrix $a_{ik}\\in\\mathbf{A}$ for a `SISO` problem is constructed as:\n",
    "$$\n",
    "\\begin{align*}\n",
    "a_{ik} &= -\\begin{cases}\n",
    "    \\left(2i+1\\right)^{1/2}\\left(2k+1\\right)^{1/2} & \\text{if } i>k \\\\\n",
    "    \\left(i+1\\right) & \\text{if } i=k \\\\\n",
    "    0 & \\text{if } i<k \\\\\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "$$\n",
    "and the $b_{n}\\in\\mathbf{B}$ input matrix is constructed as:\n",
    "$$\n",
    "\\begin{align*}\n",
    "b_{i} &= \\left(2i+1\\right)^{1/2} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The Leg-S HiPPO matrices are designed to efficiently capture long-range dependencies in sequential data. However, the matrix $\\mathbf{A}$ is not diagonalizable, leading to complications and computational overhead. We'll discuss this in more detail in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1785151",
   "metadata": {},
   "source": [
    "## Two views of the same state space model\n",
    "Linear state space models, either SISO or MIMO, can be operated in two ways: (i) they can process one input token at a time, or (ii) they can process all input tokens simultaneously (up to time $t$). The first approach is called _sequential operation_, and the second is called _convolutional operation_. \n",
    "\n",
    "### Sequential operation\n",
    "Imagine that we have [a queue of input tokens $x\\in\\mathcal{Q}$](https://en.wikipedia.org/wiki/Queue_(abstract_data_type)) that we want to process one at a time. We can use a linear state space model to process each input token in the queue sequentially, and then place the output token in a corresponding [output queue $\\mathcal{O}$](https://en.wikipedia.org/wiki/Queue_(abstract_data_type)). The time steps of the input and output queues are aligned, so that the output token at time $t$ corresponds to the input token at time $t$. Let's look at a simple algorithm for sequential processing of input tokens:\n",
    "\n",
    "__Initialization__: The user provides $\\mathbf{\\bar{A}}$, $\\mathbf{\\bar{B}}$, $\\mathbf{\\bar{C}}$, and $\\mathbf{\\bar{D}}$ matrices (discrete time system matricies). The hidden state is initialized as $\\mathbf{x}_{0} = \\mathbf{0}$. Set $t = 1$. Initialize the input queue $\\mathcal{Q}$, output queue $\\mathcal{O}$ and the hidden state storage $\\mathcal{x}_{0}\\rightarrow\\mathcal{H}$.\n",
    "\n",
    "While $\\mathcal{Q}$ is not empty:\n",
    "1. Read the input token $x\\gets\\mathcal{Q}$ and set $\\mathbf{u}_{t} = x$.\n",
    "2. Get the _previous_ hidden state $\\mathbf{x}_{t-1}\\gets\\mathcal{H}$, and compute the _next_ hidden state $\\mathbf{x}_{t} = \\mathbf{\\bar{A}} \\mathbf{x}_{t-1} + \\mathbf{\\bar{B}} \\mathbf{u}_{t}$.\n",
    "3. Compute the _next_ output token $\\mathbf{y}_{t} = \\mathbf{\\bar{C}} \\mathbf{x}_{t} + \\mathbf{\\bar{D}} \\mathbf{u}_{t}$.\n",
    "4. Write the _next_ output token $\\mathbf{y}_{t}$ to the output queue $\\mathbf{y}_{t}\\rightarrow\\mathcal{O}$.\n",
    "5. Store the new hidden state $\\mathbf{x}_{t}\\rightarrow\\mathcal{H}$, and increment the time index $t \\gets t + 1$.\n",
    "6. If the input queue $\\mathcal{Q}$ is __not empty__ continue, otherwise __stop__.\n",
    "\n",
    "### Convolutional operation\n",
    "In the convolutional operation, we process all $t$ input tokens at once to produce the output tokens $y_{1},y_{2},\\dots,y_{t}$. Let's look at each time step of the sequential operation, where $\\mathbf{x}_{0} = \\mathbf{0}$, and $\\mathbf{\\bar{D}} = 0$. For $i = 0, 1, 2, \\dots, t$ we have the output tokens:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{y}_{0} & = \\mathbf{\\bar{C}}\\mathbf{\\bar{B}}\\mathbf{u}_{0}\\quad |~\\textit{substitute}\\quad\\mathbf{x}_{0} = \\mathbf{\\bar{B}}\\mathbf{u}_{0} \\\\\n",
    "\\mathbf{y}_{1} & = \\mathbf{\\bar{C}}\\mathbf{\\bar{A}}\\mathbf{\\bar{B}}\\mathbf{u}_{0} + \\mathbf{\\bar{C}}\\mathbf{\\bar{B}}\\mathbf{u}_{1}\\quad |~\\textit{substitute}\\quad\\mathbf{x}_{1} = \\mathbf{\\bar{A}}\\mathbf{\\bar{B}}\\mathbf{u}_{0}+\\mathbf{\\bar{B}}\\mathbf{u}_{1}\\\\\n",
    "\\mathbf{y}_{2} & = \\mathbf{\\bar{C}}\\mathbf{\\bar{A}}^{2}\\mathbf{u}_{0} + \\mathbf{\\bar{C}}\\mathbf{\\bar{A}}\\mathbf{\\bar{B}}\\mathbf{u}_{1} + \\mathbf{\\bar{C}}\\mathbf{\\bar{B}}\\mathbf{u}_{2}\\quad |~\\textit{substitute}\\quad\\mathbf{x}_{2} = \\mathbf{\\bar{A}}^{2}\\mathbf{\\bar{B}}\\mathbf{u}_{0} +\n",
    "\\mathbf{\\bar{A}}\\mathbf{\\bar{B}}\\mathbf{u}_{1} + \\mathbf{\\bar{B}}\\mathbf{u}_{2} \\\\\n",
    "\\vdots & \\\\\n",
    "\\mathbf{y}_{t} & = \\mathbf{\\bar{C}}\\mathbf{\\bar{A}}^{t}\\mathbf{u}_{0} + \\mathbf{\\bar{C}}\\mathbf{\\bar{A}}^{t-1}\\mathbf{\\bar{B}}\\mathbf{u}_{1} + \\mathbf{\\bar{C}}\\mathbf{\\bar{A}}^{t-2}\\mathbf{\\bar{B}}\\mathbf{u}_{2} + \\cdots + \\mathbf{\\bar{C}}\\mathbf{\\bar{B}}\\mathbf{u}_{t}\\quad\\blacksquare\n",
    "\\end{align*}\n",
    "$$\n",
    "which we can rewrite as the convolution operation:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{y}_{t} & = \\sum_{i=0}^{t}\\mathbf{\\bar{C}}\\mathbf{\\bar{A}}^{t-i}\\mathbf{\\bar{B}}\\mathbf{u}_{i} \\\\\n",
    "& = \\mathbf{\\bar{C}}\\sum_{i=0}^{t}\\mathbf{\\bar{A}}^{t-i}\\mathbf{\\bar{B}}\\mathbf{u}_{i} \\\\\n",
    "& = \\mathbf{\\bar{C}}\\sum_{i=0}^{t}\\mathbf{\\bar{A}}^{i}\\mathbf{\\bar{B}}\\mathbf{u}_{t-i}\\qquad\\blacksquare\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "The matrix-matrix products and the matrix power operations are expensive. How could we make this cheaper (so we can compute longer sequences?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690ba3a9",
   "metadata": {},
   "source": [
    "## S5: Extension to MIMO Systems\n",
    "The Simplified Structured State Space Sequence (S5) model extends the S4 Leg-S HiPPO model to multiple-input, multiple-output (MIMO) systems and explores approaches for cheaper operations. \n",
    "* See: [Smith, J., Warrington, A., & Linderman, S.W. (2022). Simplified State Space Layers for Sequence Modeling. ArXiv, abs/2208.04933.](https://arxiv.org/abs/2208.04933)\n",
    "\n",
    "The S5 system is similar to the SISO Leg-S HiPPO S4 system, but it uses different matrices for the state transition and input matrices. In particular, in the S5 system, we want the state transition matrix $\\mathbf{A}$ to be a _diagonal matrix_ and $d_{in}>1$. Let's diagonalize the S4 Leg-S HiPPO matrix $\\mathbf{A}$.\n",
    "\n",
    "### Background: Diagonalization of a square matrix\n",
    "A square matrix $\\mathbf{A}\\in\\mathbb{R}^{n\\times{n}}$ is said to be diagonalizable if there exists an invertible matrix $\\mathbf{V}$ and a diagonal matrix $\\mathbf{D}$ such that:\n",
    "$$\n",
    "\\mathbf{A} = \\mathbf{V}\\mathbf{D}\\mathbf{V}^{-1}\n",
    "$$\n",
    "where $\\mathbf{D}$ is a diagonal matrix with the eigenvalues of $\\mathbf{A}$ along the diagonal, and $\\mathbf{V}$ is the matrix of eigenvectors (on the columns) of $\\mathbf{A}$. \n",
    "\n",
    "Let's try this with the S4 Leg-S HiPPO matrix $\\mathbf{A}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c92c57ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100×100 Matrix{Float64}:\n",
       "  -2.0        0.0        0.0        0.0      …     0.0       0.0       0.0\n",
       "  -3.87298   -3.0        0.0        0.0            0.0       0.0       0.0\n",
       "  -4.58258   -5.91608   -4.0        0.0            0.0       0.0       0.0\n",
       "  -5.19615   -6.7082    -7.93725   -5.0            0.0       0.0       0.0\n",
       "  -5.74456   -7.4162    -8.77496   -9.94987        0.0       0.0       0.0\n",
       "  -6.245     -8.06226   -9.53939  -10.8167   …     0.0       0.0       0.0\n",
       "  -6.7082    -8.66025  -10.247    -11.619          0.0       0.0       0.0\n",
       "  -7.14143   -9.21954  -10.9087   -12.3693         0.0       0.0       0.0\n",
       "  -7.54983   -9.74679  -11.5326   -13.0767         0.0       0.0       0.0\n",
       "  -7.93725  -10.247    -12.1244   -13.7477         0.0       0.0       0.0\n",
       "   ⋮                                         ⋱                      \n",
       " -23.5584   -30.4138   -35.9861   -40.8044         0.0       0.0       0.0\n",
       " -23.6854   -30.5778   -36.1801   -41.0244         0.0       0.0       0.0\n",
       " -23.8118   -30.7409   -36.3731   -41.2432         0.0       0.0       0.0\n",
       " -23.9374   -30.9031   -36.565    -41.4608         0.0       0.0       0.0\n",
       " -24.0624   -31.0644   -36.756    -41.6773   …     0.0       0.0       0.0\n",
       " -24.1868   -31.225    -36.9459   -41.8927         0.0       0.0       0.0\n",
       " -24.3105   -31.3847   -37.1349   -42.107        -99.0       0.0       0.0\n",
       " -24.4336   -31.5436   -37.3229   -42.3202      -197.997  -100.0       0.0\n",
       " -24.5561   -31.7017   -37.51     -42.5323      -198.99   -199.997  -101.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "A = let\n",
    "\n",
    "    # initialize -\n",
    "    h = 100; # internal hidden state memory size\n",
    "    A = Array{Float64,2}(undef, h, h); # internal hidden state memory\n",
    "    \n",
    "    # build the A-matrix\n",
    "    for i ∈ 1:h\n",
    "        for k = 1:h\n",
    "            \n",
    "            if (i > k)\n",
    "                A[i,k] = -sqrt((2*i+1))*sqrt((2*k+1));\n",
    "            elseif (i == k)\n",
    "                A[i,k] = -(i+1);\n",
    "            else\n",
    "                A[i,k] = 0.0;\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    A; # return -\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ef27bc",
   "metadata": {},
   "source": [
    "__Blast from the past!__ In [week 2](https://github.com/varnerlab/CHEME-5820-Lectures-Spring-2025/tree/main/lectures/week-2/L2a) we learned about eigendecomposition of a matrix. Let's compute the eigenvalues/eigenvectors using [the built-in `eigen(...)` function](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.eigen), which takes a square array `A` as an argument and returns the eigendecomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26105137",
   "metadata": {},
   "outputs": [],
   "source": [
    "(Λ,V) = let\n",
    "\n",
    "    # initialize -\n",
    "    (n,m) = size(A); # what is the dimension of A?\n",
    "    Λ = Matrix{Float64}(1.0*I, n, n); # builds the I matrix, we'll update with λ -\n",
    "    \n",
    "    # Decompose using the built-in function\n",
    "    F = eigen(A);   # eigenvalues and vectors in F of type Eigen\n",
    "    λ = F.values;   # vector of eigenvalues\n",
    "    V = F.vectors;  # n x n matrix of eigenvectors, each col is an eigenvector\n",
    "\n",
    "    # package the eigenvalues into Λ -\n",
    "    for i ∈ 1:n\n",
    "        Λ[i,i] = λ[i];\n",
    "    end\n",
    "\n",
    "    Λ,V # return\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48344044",
   "metadata": {},
   "source": [
    "__Great!__ Now that we have the eigenvalues and eigenvectors, let's diagonalize the S4 Leg-S HiPPO matrix $\\mathbf{A}$. We can compute the diagonal matrix $\\mathbf{D}$ and the invertible matrix $\\mathbf{V}$:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{A} &= \\mathbf{V}\\mathbf{D}\\mathbf{V}^{-1} \\\\\n",
    "\\mathbf{V}^{-1}\\mathbf{A} &= \\mathbf{D}\\mathbf{V}^{-1} \\\\\n",
    "\\mathbf{V}^{-1}\\mathbf{A}\\mathbf{V} &= \\mathbf{D} \\\\\n",
    "\\mathbf{V}^{-1}\\mathbf{A}\\mathbf{V} &= \\text{diag}(\\lambda_{1},\\lambda_{2},\\dots,\\lambda_{n}) \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\lambda_{i}$ are the eigenvalues of the matrix $\\mathbf{A}$. The diagonal matrix $\\mathbf{D}$ is a diagonal matrix with the eigenvalues of the matrix $\\mathbf{A}$ along the diagonal. The invertible matrix $\\mathbf{V}$ is the matrix of eigenvectors of the matrix $\\mathbf{A}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f66b83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = inv(V)*A*V; # diagonalize A using the eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c05643",
   "metadata": {},
   "source": [
    "__Check__: If this worked, then $\\mathbf{\\Lambda} = \\mathbf{D}$. Let's check this using [the `isapprox(...)` function](https://docs.julialang.org/en/v1/base/math/#Base.isapprox) and [the @assert macro](https://docs.julialang.org/en/v1/base/base/#Base.@assert) to check if the two matrices are approximately equal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20938e93",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "AssertionError: D ≈ Λ",
     "output_type": "error",
     "traceback": [
      "AssertionError: D ≈ Λ\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/Desktop/julia_work/CHEME-5820-SP25/CHEME-5820-Lectures-Spring-2025/lectures/week-15/L15a/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X14sZmlsZQ==.jl:1"
     ]
    }
   ],
   "source": [
    "@assert D ≈ Λ; # check if D is diagonal equal to Λ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e449eb",
   "metadata": {},
   "source": [
    "### Rethinking the $\\mathbf{A}$ and $\\mathbf{B}$ matrices\n",
    "As it turns out, the _original_ S4 Leg-S HiPPO matrix $\\mathbf{A}$ is _not_ diagonalizable. However, [Goel et al 2022](https://arxiv.org/pdf/2202.09729) proposed a fix for this issue: redefine the Leg-S HiPPO matrix $\\mathbf{A}$ as the sum of [a _normal_ matrix](https://en.wikipedia.org/wiki/Normal_matrix) and a low-rank matrix $\\mathbf{P}\\in\\mathbb{R}^{n}$:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{A} &= \\mathbf{A}^{\\text{normal}} - \\mathbf{P}\\mathbf{P}^{\\top} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\mathbf{A}^{\\text{normal}}$ is a [normal matrix](https://en.wikipedia.org/wiki/Normal_matrix), and $\\mathbf{P}$ is a low-rank matrix. The normal and low-rank matrices are defined as follows:\n",
    "$$\n",
    "\\begin{align*}\n",
    "a^{\\text{normal}}_{ik} &= -\\begin{cases}\n",
    "    \\left(i+\\frac{1}{2}\\right)^{1/2}\\left(k+\\frac{1}{2}\\right)^{1/2} & \\text{if } i>k \\\\\n",
    "    \\frac{1}{2} & \\text{if } i=k \\\\\n",
    "    \\left(i+\\frac{1}{2}\\right)^{1/2}\\left(k+\\frac{1}{2}\\right)^{1/2} & \\text{if } i<k \\\\\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\begin{align*}\n",
    "p_{i} & = \\left(i+\\frac{1}{2}\\right)^{1/2}\n",
    "\\end{align*}\n",
    "$$\n",
    "__So let's try this again__. Compute the revised $\\mathbf{A}$ matrix using [the normal](https://en.wikipedia.org/wiki/Normal_matrix) and low-rank matrices (save this in the `Â::Array{Float64,2}` variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "439a97a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×10 Matrix{Float64}:\n",
       " -2.0       -3.87298   -4.58258  …   -7.14143   -7.54983   -7.93725\n",
       " -3.87298   -3.0       -5.91608      -9.21954   -9.74679  -10.247\n",
       " -4.58258   -5.91608   -4.0         -10.9087   -11.5326   -12.1244\n",
       " -5.19615   -6.7082    -7.93725     -12.3693   -13.0767   -13.7477\n",
       " -5.74456   -7.4162    -8.77496     -13.6748   -14.4568   -15.1987\n",
       " -6.245     -8.06226   -9.53939  …  -14.8661   -15.7162   -16.5227\n",
       " -6.7082    -8.66025  -10.247       -15.9687   -16.8819   -17.7482\n",
       " -7.14143   -9.21954  -10.9087       -9.0      -17.9722   -18.8944\n",
       " -7.54983   -9.74679  -11.5326      -17.9722   -10.0      -19.975\n",
       " -7.93725  -10.247    -12.1244      -18.8944   -19.975    -11.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Â = let\n",
    "\n",
    "    # initialize -\n",
    "    h = 10; # internal hidden state memory size\n",
    "    AN = Array{Float64,2}(undef, h, h); # internal hidden state memory\n",
    "    P = Array{Float64,2}(undef, h, 1); # internal hidden state memory\n",
    "\n",
    "    # build the A-matrix\n",
    "    for i ∈ 1:h\n",
    "        for k = 1:h\n",
    "            \n",
    "            if (i > k)\n",
    "                AN[i,k] = -sqrt((i+1/2))*sqrt((k+1/2));\n",
    "\n",
    "            elseif (i == k)\n",
    "                AN[i,k] = -1/2;\n",
    "            else\n",
    "                AN[i,k] = -sqrt((i+1/2))*sqrt((k+1/2));\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # build the B-matrix\n",
    "    for i ∈ 1:h\n",
    "        P[i,1] = sqrt((i+1/2));\n",
    "    end\n",
    "\n",
    "   \n",
    "    # compute -\n",
    "    A = AN - P*P'; # A = A + P*P'\n",
    "   \n",
    "    A; # return -\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feff557b",
   "metadata": {},
   "source": [
    "Then compute the eigendecomposition of the _revised_ matrix $\\mathbf{A}$ using [the `eigen(...)` function](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.eigen). Save the eigenvalues in the `Λ̂::Array{Float64,2}` variable and the eigenvectors in the `V̂::Array{Float64,2}` variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62c57824",
   "metadata": {},
   "outputs": [],
   "source": [
    "(Λ̂, V̂) = let\n",
    "\n",
    "    # initialize -\n",
    "    (n,m) = size(Â); # what is the dimension of A?\n",
    "    Λ = Matrix{Float64}(1.0*I, n, n); # builds the I matrix, we'll update with λ -\n",
    "    \n",
    "    # Decompose using the built-in function\n",
    "    F = eigen(Â);   # eigenvalues and vectors in F of type Eigen\n",
    "    λ = F.values;   # vector of eigenvalues\n",
    "    V = F.vectors;  # n x n matrix of eigenvectors, each col is an eigenvector\n",
    "\n",
    "    # package the eigenvalues into Λ -\n",
    "    for i ∈ 1:n\n",
    "        Λ[i,i] = λ[i];\n",
    "    end\n",
    "\n",
    "    # @assert Â ≈ V*Λ*V'; # check if Â is diagonal equal to Λ\n",
    "\n",
    "    Λ, V;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160185f4",
   "metadata": {},
   "source": [
    "Finally, we can run the test again to check of the _revised_ matrix $\\mathbf{A}$ is diagonalizable. If this worked, then $\\mathbf{\\Lambda} = \\mathbf{D}$, let's check this using [the `isapprox(...)` function](https://docs.julialang.org/en/v1/base/math/#Base.isapprox) and [the @assert macro](https://docs.julialang.org/en/v1/base/base/#Base.@assert) to check if the two matrices are approximately equal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c38959a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@assert inv(V̂)*Â*V̂ ≈ Λ̂ # diagonalize A using the eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e876e2",
   "metadata": {},
   "source": [
    "#### What about the $\\mathbf{B}$ matrix?\n",
    "The input matrix $\\mathbf{B}$ maps the input the hidden state. We are going to use a simple form for the matrix $\\mathbf{B}$, namely, we repeat the columns from the S4 model for each input channel of the input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0371603a",
   "metadata": {},
   "source": [
    "### S5: Putting it all together\n",
    "Now that we have the matrices $\\mathbf{A}$ and $\\mathbf{B}$, we can put them together to form the S5 model. First, let's rewrite the hidden state as $\\mathbf{x} = \\mathbf{V}\\mathbf{h}$, where $\\mathbf{h}$ is the hidden state of the S5 model, and $\\mathbf{V}$ is the matrix of eigenvectors of the matrix $\\mathbf{A}$. \n",
    "* _Helpful_: We know that $\\mathbf{V}$ is constant, so $\\mathbf{x} = \\mathbf{V}\\mathbf{h}$ can differentiated with respect to time $\\dot{\\mathbf{x}} = \\mathbf{V}\\dot{\\mathbf{h}}$.\n",
    "\n",
    "Rewrite the _hidden_ state of the S5 model as:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\dot{\\mathbf{x}} &= \\mathbf{A}\\,\\mathbf{x} + \\mathbf{B}\\,\\mathbf{u}\\quad |~\\textit{substitute}\\quad\\mathbf{x} = \\mathbf{V}\\,\\mathbf{h}\\\\\n",
    "\\mathbf{V}\\dot{\\mathbf{h}} &= \\mathbf{A}\\,\\mathbf{V}\\mathbf{h} + \\mathbf{B}\\,\\mathbf{u}\\quad |~\\textit{multiply by }\\mathbf{V}^{-1}\\\\\n",
    "\\dot{\\mathbf{h}} &= \\underbrace{\\mathbf{V}^{-1}\\mathbf{A} \\mathbf{V}}_{=\\mathbf{\\Lambda}\\,\\text{diagonal}}\\mathbf{h} + \\mathbf{V}^{-1}\\mathbf{B} \\mathbf{u}\\\\\n",
    "\\dot{\\mathbf{h}} &= \\mathbf{\\Lambda}\\mathbf{h} + \\underbrace{\\mathbf{V}^{-1}\\mathbf{B}}_{=\\mathbf{\\tilde{B}}}\\, \\mathbf{u}\\\\\n",
    "\\dot{\\mathbf{h}} &= \\mathbf{\\Lambda}\\mathbf{h} + \\mathbf{\\tilde{B}}\\, \\mathbf{u}\\quad\\blacksquare\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "The output is given by:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{y} &= \\mathbf{C}\\,\\mathbf{x} + \\mathbf{D}\\,\\mathbf{u}\\quad |~\\textit{substitute}\\quad\\mathbf{x} = \\mathbf{V}\\,\\mathbf{h}\\\\\n",
    "\\mathbf{y} &= \\mathbf{C}\\,\\mathbf{V}\\,\\mathbf{h} + \\mathbf{D}\\,\\mathbf{u}\\\\\n",
    "\\mathbf{y} &= \\underbrace{\\mathbf{C}\\,\\mathbf{V}}_{=\\mathbf{\\tilde{C}}}\\,\\mathbf{h} + \\mathbf{D}\\,\\mathbf{u}\\\\\n",
    "\\mathbf{y} &= \\mathbf{\\tilde{C}}\\,\\mathbf{h} + \\mathbf{D}\\,\\mathbf{u}\\quad\\blacksquare\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\mathbf{\\tilde{B}} = \\mathbf{V}^{-1}\\mathbf{B}$ and $\\mathbf{\\tilde{C}} = \\mathbf{C}\\,\\mathbf{V}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086427d4",
   "metadata": {},
   "source": [
    "#### Discretization\n",
    "Whether it is SISO or MIMO, to speed up the calculations, we discretize the continuous-time state space model and use the discrete variables of the hidden state in all calculations.\n",
    "The discrete-time state space S5 model is given by:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{h}_{t} &= \\mathbf{\\bar{\\Lambda}}\\,\\mathbf{h}_{t-1} + \\mathbf{\\bar{B}}\\,\\mathbf{u}_{t} \\\\\n",
    "\\mathbf{y}_{t} &= \\mathbf{\\bar{C}}\\,\\mathbf{h}_{t} + \\mathbf{\\bar{D}}\\,\\mathbf{u}_{t}\n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\mathbf{h}_{t}$ is the hidden state at time $t$, $\\mathbf{u}_{t}$ is the input at time $t$, and $\\mathbf{y}_{t}$ is the output at time $t$. The discretized matrices $\\mathbf{\\bar{\\Lambda}}$, $\\mathbf{\\bar{B}}$, and $\\mathbf{\\bar{C}}$ can be obtained from a variety of methods, such as the bilinear method (shown below for the S5 model):\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{\\bar{\\Lambda}} &= \\left(\\mathbf{I}-\\left(\\Delta/2\\right)\\cdot\\mathbf{\\Lambda}\\right)^{-1}\\left(\\mathbf{I}+\\left(\\Delta/2\\right)\\cdot\\mathbf{\\Lambda}\\right) \\\\\n",
    "\\mathbf{\\bar{B}} &= \\left(\\mathbf{I}-\\left(\\Delta/2\\right)\\cdot\\mathbf{\\Lambda}\\right)^{-1}\\left(\\Delta\\cdot\\mathbf{\\tilde{B}}\\right) \\\\\n",
    "\\mathbf{\\bar{C}} &= \\mathbf{\\tilde{C}} \\\\\n",
    "\\mathbf{\\bar{D}} &= \\mathbf{D} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\Delta$ is the time step size (sampling frequency), and $\\mathbf{I}$ is the identity matrix. There are alternatives to the bilinear method, such as the [zero-order hold (ZOH) method](https://hazyresearch.stanford.edu/blog/2022-06-11-simplifying-s4).\n",
    "* _Simplification_: In most applications, we set $\\mathbf{D} = 0$, which means that the output is only dependent on the hidden state and not on the input, thus $\\mathbf{\\bar{D}} = 0$. If this were not the case, we set $\\mathbf{\\bar{D}} = \\mathbf{D}$, which means that the output is dependent on both the hidden state and the input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba53dd9c-504d-49ab-9cb7-911c24c13de0",
   "metadata": {},
   "source": [
    "## Spiking Neural Networks (SNNs)\n",
    "Spiking neural networks (SNNs) simulate biological neural processes by modeling how neurons communicate through discrete electrical impulses called spikes. These networks differ fundamentally from traditional artificial neural networks (ANNs) in their computational approach, information encoding, and biological plausibility. \n",
    "\n",
    "* _Key point_: Unlike ANNs, which use continuous activation values, e.g., the output $\\texttt{tanh}$ function and an artificial structure, the neurons in an SNN generate binary spikes with timing-dependent information, and attempt to get back to the biology. There are several types of neurons. We'll look at [Leaky Integrate-and-Fire (LIF) neurons](https://www.nature.com/articles/s41598-017-07418-y) in the lab.\n",
    "\n",
    "Let's look at an example paper that uses LIF neurons:\n",
    "* [Limbacher, T., Özdenizci, O., & Legenstein, R.A. (2022). Memory-enriched computation and learning in spiking neural networks through Hebbian plasticity. ArXiv, abs/2205.11276.](https://arxiv.org/abs/2205.11276)\n",
    "\n",
    "### Computational and Practical Challenges of SNNs\n",
    "* __Training Complexity__: Due to non-differentiable spikes, SNNs require specialized training methods (e.g., [Spike-Timing-Dependent Plasticity](https://en.wikipedia.org/wiki/Spike-timing-dependent_plasticity)), whereas ANNs use backpropagation.\n",
    "* __Hardware Implementation__: While SNNs can exploit event-based neuromorphic hardware (e.g., [Intel Loihi](https://www.intel.com/content/www/us/en/research/neuromorphic-computing.html), or efforts at [IBM](https://www.ibm.com/think/topics/neuromorphic-computing)), SNNs often underperform ANNs in energy efficiency unless spikes are sparse.\n",
    "* __Applications__: SNNs excel in tasks benefiting from temporal processing (e.g., robotics, real-time sensory data), while ANNs dominate static data domains (e.g., image classification).\n",
    "  \n",
    "In summary, SNNs offer biologically inspired computation with temporal precision but face challenges in training and efficiency compared to ANNs. The LIF model exemplifies the trade-off between simplicity and biological fidelity, while advances in coding schemes (e.g., temporal vs. rate-based) and hardware will determine their future viability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe978a4",
   "metadata": {},
   "source": [
    "## Lab: Spiking Neural Networks and S5\n",
    "In lab `L15b`, we will implement an embedding layer composed of a Spiking Neural Network (SNN) with LIF neurons and then play around with the S5 model for a few text classification tasks. Should be fun!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1449a592",
   "metadata": {},
   "source": [
    "# Today?\n",
    "That's a wrap! What are some of the interesting things we discussed today?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.5",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
