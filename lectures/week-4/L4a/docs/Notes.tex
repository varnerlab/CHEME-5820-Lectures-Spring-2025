\documentclass{article}[11pt]
\usepackage{fullpage,graphicx, setspace, latexsym, cite,amsmath,amssymb,xcolor,subfigure}
%\usepackage{epstopdf}
%\DeclareGraphicsExtensions{.pdf,.eps,.png,.jpg,.mps} 
\usepackage{amssymb} %maths
\usepackage{amsmath} %maths
\usepackage{amsthm, comment}
\usepackage[round,comma,sort,numbers]{natbib}

% \bibliographystyle{plain}
\bibliographystyle{plos2015}

\newtheorem{theorem}{Theorem}
\newtheorem{prop}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{defn}{Definition}
\newtheorem{ex}{Example}
\usepackage{float}

\newcommand*{\underuparrow}[1]{\underset{\uparrow}{#1}}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage[dvipsnames]{xcolor}
\usepackage{algorithmicx}
\usepackage{algorithm} %http://ctan.org/pkg/algorithms
\usepackage{algpseudocode} %http://ctan.org/pkg/algorithmicx
\usepackage{enumitem}
\usepackage{simplemargins}
\usepackage{hyperref}

\usepackage{mdframed}
\definecolor{lgray}{rgb}{0.92,0.92,0.92}
\definecolor{lsalmon}{rgb}{0.9921568627450981,0.9411764705882353, 0.9254901960784314}

\renewcommand{\bibnumfmt}[1]{#1.}
\setlist{noitemsep} % or \setlist{noitemsep} to leave space around whole list
\setallmargins{1in}
\linespread{1.1}

\newcommand{\brows}[1]{%
  \begin{bmatrix}
  \begin{array}{@{\protect\rotvert\;}c@{\;\protect\rotvert}}
  #1
  \end{array}
  \end{bmatrix}
}
\newcommand{\rotvert}{\rotatebox[origin=c]{90}{$\vert$}}
\newcommand{\rowsvdots}{\multicolumn{1}{@{}c@{}}{\vdots}}


\def\R{\mathbb{R}}
\def\Eps{\mathcal{E}}
\def\E{\mathbb{E}}
\def\V{\mathbb{V}}
\def\F{\mathcal{F}}
\def\G{\mathcal{G}}
\def\H{\mathcal{H}}
\def\S{\mathcal{S}}
\def\D{\mathcal{D}}
\def\P{\mathbb{P}}
\def\1{\mathbf{1}}
\def\n{\nappa}
\def\h{\mathbf{w}}
\def\v{\mathbf{v}}
\def\x{\mathbf{x}}
\def\X{\mathcal{X}}
\def\Y{\mathcal{Y}}
\def\eps{\epsilon}
\def\y{\mathbf{y}}
\def\e{\mathbf{e}}
\newcommand{\norm}[1]{\left|\left|#1\right|\right|}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\lecture}[4]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   % \setcounter{lecnum}{#1}
   \setcounter{page}{1}
   \setlength{\headsep}{10mm}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf CHEME 5820: Machine Learning for Engineers
   \hfill Spring 2025} }
       \vspace{4mm}
       \hbox to 6.28in { {\Large \hfill Lecture #1: #2  \hfill} }
       \vspace{2mm}
       \hbox to 6.28in { {\it Lecturer: #3 \hfill #4} }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{Lecture #1: #2}{Lecture #1: #2}

   \noindent{\bf Disclaimer}: {\it These notes have not been subjected to the
   usual scrutiny reserved for formal publications. }
   \vspace*{4mm}
}
\begin{document}
\lecture{4c}{Kernel Functions and Kernelized Regression}{Jeffrey Varner}{}


\begin{mdframed}[backgroundcolor=lgray]
   In this lecture, we will discuss the following topics:
   \begin{itemize}[leftmargin=16pt]
      \item{\textbf{Positive definite kernel functions}: A positive definite kernel function $k:\mathbb{R}^{\star}\times\mathbb{R}^{\star}\to\mathbb{R}$ is a function that takes two vector arguments and returns a scalar that is in some sense a \textit{similarity} measure of the two input vectors. A positive definite kernel function produces a kernel matrix $\mathbf{K}$ that is positive (semi)definite.}
      \item{\textbf{Kernel machines}: A kernel machine is a class of machine learning algorithms that uses kernel functions to implicitly transform input data into a high-dimensional feature space, enabling the solution of non-linear problems using linear classifiers without explicitly computing the coordinates in that space.}
      \item{\textbf{Kernel regression}: Kernel regression is a technique that uses kernel functions to estimate (potentially) non-linear relationships between variables by assigning weights to data points based on their proximity to a point of interest, allowing for flexible modeling without assuming a specific functional form.}
   \end{itemize}
\end{mdframed}


\section{Introduction}
In this lecture, we will discuss kernel functions and kernel regression.
Kernel functions are mathematical tools that enable algorithms to operate in high-dimensional spaces without explicitly computing the coordinates in those spaces.
Kernel functions are used in a variety of machine learning algorithms, including Support Vector Machines (SVMs), kernelized regression, and kernelized clustering.
Today, we will focus on kernel regression, which is a non-parametric regression technique that uses kernel functions to estimate the relationship between the input and output variables.
Let's start by discussing the basic concepts of kernel functions and then consider kernel regression.


% In this lecture, we will discuss Support Vector Machines (SVMs). 
% SVMs are a powerful class of supervised learning algorithms that can be used for classification and regression tasks. 
% SVMs are based on the concept of decision planes that define decision boundaries. A decision plane is a hyperplane that separates the data into classes. 
% The goal of SVMs is to find the optimal decision plane that maximizes the margin between the classes. 
% The margin is the distance between the decision plane and the closest data points from each class. 
% SVMs are particularly useful for high-dimensional data and can handle non-linear decision boundaries using the kernel trick. 
% In this lecture, we will discuss the basic concepts of SVMs, including the underlying optimization problem, the kernel trick, and the soft margin SVM. 

% \section{Basic Concepts of SVMs}
% Support Vector Machines (SVMs) are a class of supervised learning algorithms that can be used for classification and regression tasks.
% The goal of SVMs is to find the optimal decision plane that maximizes the margin between the classes. 
% The decision plane is defined by the equation:
% \begin{equation}
% \mathbf{w}^T\mathbf{x} + b = 0
% \end{equation}
% where $\mathbf{w}$ is the weight vector, $\mathbf{x}$ is the data point, and $b$ is the bias term. 
% To estimate the decision plane, SVMs use the training data to find the optimal weight vector $\mathbf{w}$ and bias term $b$ that separate the data into classes.
% These parameters are estimated by solving an optimization problem that minimizes the norm of the weight vector subject to the constraints that the data points are correctly classified.
% The optimization problem for SVMs can be formulated as:
% \begin{equation}
% \min_{\mathbf{w},b} \frac{1}{2} \norm{\mathbf{w}}^2
% \end{equation}
% subject to the constraints:
% \begin{equation}
% y_i(\mathbf{w}^T\mathbf{x}_i + b) \geq 1, \quad \forall i = 1, \ldots, n
% \end{equation}
% where $\mathbf{w}$ is the weight vector, $b$ is the bias term, $\mathbf{x}_i$ is the $i$-th data point, 
% $y_i$ is the label of the $i$-th data point, and $n$ is the number of data points.

% \subsection{Soft Margin SVM}
% The soft margin SVM is an extension of the basic SVM that allows for some misclassification errors.
% The soft margin SVM introduces a slack variable $\xi_i$ for each data point, which measures the distance of the data point from the decision plane.
% The optimization problem for the soft margin SVM can be formulated as follows:
% \begin{equation}
% \min_{\mathbf{w},b,\xi} \frac{1}{2} \norm{\mathbf{w}}^2 + C\sum_{i=1}^{n} \xi_i
% \end{equation}
% subject to the constraints:
% \begin{equation}
% y_i(\mathbf{w}^T\mathbf{x}_i + b) \geq 1 - \xi_i, \quad \forall i = 1, \ldots, n
% \end{equation}

\section{Kernel Functions}
Kernel functions in machine learning are mathematical tools that enable algorithms to operate in high-dimensional spaces without explicitly computing the coordinates in those spaces.
Kernel functions are used in a variety of machine learning algorithms, including Support Vector Machines (SVMs), kernelized regression, and kernelized clustering.
Kernel functions have a few different interpretations. 
For example, kernel functions can be thought of as similarity measures, i.e., they quantify the similarity between pairs of data points in a high-dimensional space.
They are also implict mappings of data into a high-dimensional space, where the data becomes linearly separable (which is useful for classification algorithms).
Thus, kernel functions are powerful tools that we are going to use for many applications in machine learning.

A kernel function $k:\mathbb{R}^{\star}\times\R^{\star}\to\R$ is a function that takes a pair of vectors 
$\mathbf{v}_i\in\R^{\star}$ and $\mathbf{v}_j\in\R^{\star}$ as arguments, 
e.g., a pair of feature vectors, a feature vector and a paramter vector, or any two vectors of compatible size 
and computes a scalar value that represents the similarity (in some sense) between the two vector arguments.
For example, the linear kernel function computes the dot product between two vectors:
\begin{equation}
k(\mathbf{v}_i, \mathbf{v}_j) = \mathbf{v}_i^{\top}\mathbf{v}_j
\end{equation}
On the other hand, a polynomial kernel is defined as:
\begin{equation}
k_{d}(\mathbf{v}_i, \mathbf{v}_j) = (1+\mathbf{v}_i^{\top}\mathbf{v}_j)^d
\end{equation}
where $d$ is the degree of the polynomial. The radial basis function (RBF) kernel is defined as:
\begin{equation}
k_{\gamma}(\mathbf{v}_i, \mathbf{v}_j) = \exp(-\gamma \norm{\mathbf{v}_i - \mathbf{v}_j}_{2}^2)
\end{equation}
where $\gamma$ is a scaling factor, and $\norm{\cdot}^{2}_{2}$ is the squared Euclidean norm;
if we define $\gamma$ as ${1}/{2\sigma^2}$, the RBF kernel looks like a Gaussian function, without the normalization constant.
Of course, not all functions are kernel functions (Defn. \ref{def:kernel-valid}).
\begin{defn}{(Valid Kernel Function)}\label{def:kernel-valid}
A function $k:\mathbb{R}^{\star}\times\R^{\star}\to\R$ is a valid kernel function if and only if the Kernel matrix $\mathbf{K}$ is positive 
semidefinite for all possible choices of the data points $\mathbf{v}_i$, where $K_{ij} = k(\mathbf{v}_i, \mathbf{v}_j)$.
This is equivalent to saying that all eigenvalues of the Kernel matrix $\mathbf{K}$ are non-negative.
Further, for all real value vectors $\mathbf{x}$, the Kernel matrix $\mathbf{K}$ must satisfy $\mathbf{x}^{\top}\mathbf{K}\mathbf{x} \geq 0$.
\end{defn}
Kernel functions can also be combined to create more complex kernel functions using the concept of kernel composition.
For example, the sum of two valid kernel functions is also a valid kernel function. 
The product of two valid kernel functions is also a valid kernel function. 
Multiplying a kernel function by a scalar is also a valid kernel function, etc.
See \href{https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote14.html}{the CS 4780 Lecture Notes (Fall 2018)} for more details on kernel composition.

\section{Kernel Regression}
Kernel regression is a non-parametric regression technique that uses kernel functions to estimate the relationship between the input and output variables.
Kernel regression is a powerful tool for modeling complex relationships in data and is widely used in machine learning applications.
Suppose we have a dataset $\D = \{(\mathbf{x}_{i},y_{i}) \mid i = 1,2,\dots,n\}$, where the features $\mathbf{x}_i \in \mathbb{R}^{m}$ 
are $m$-dimensional vectors and the target variables are continous values $y_i \in \mathbb{R}$, e.g., the price of a house, the temperature, etc.
The basic idea behind kernel regression is to estimate the output variable $y$ as a weighted average of the output variables of the training data points, 
where the weights are determined by the kernel functions. The kernel regression function is defined as:
\begin{equation}
f(\mathbf{x}) = \sum_{i=1}^{n} \alpha_i k(\mathbf{x}_i, \mathbf{x})
\end{equation}
where $\alpha_i$ are the weights, and $k(\mathbf{x}_i, \mathbf{x})$ is the kernel function that measures the similarity between the input features $\mathbf{x}_i$ and $\mathbf{x}$.

\subsection{Kernel Ridge Regression}
Suppose we consider a linear regression problem of the form:
\begin{equation}
\hat{\mathbf{y}} = \hat{\mathbf{X}}\theta
\end{equation}
where $\hat{\mathbf{X}}\in\R^{n\times{p}}$ is a data matrix with the transpose of the augmented feature vectors $\hat{\mathbf{x}}^{\top}\in\R^{p}$ on the rows, and $\theta$ is an unknown parameter vector $\theta\in\mathbb{R}^{p}$ 
where $p = m+1$. We can estimate the (expected value) of the parameter vector $\theta$ by minimizing the least squares loss function:
\begin{equation}
\hat{\theta} = \argmin_{\theta} \norm{\mathbf{y} - \hat{\mathbf{X}}\theta}_{2}^{2} + \lambda\norm{\theta}_{2}^{2}
\end{equation}
where $\mathbf{y}$ is the target variable vector, and $\lambda\geq{0}$ is a regularization parameter. 
When we include the L2 penalty term, this is referred to as ridge regression, but we'll refer to it as regularized least squares regression.
If we include a different penalty, we would have a different type of regression, e.g., L1 penalty would be \href{https://en.wikipedia.org/wiki/Lasso_(statistics)}{Lasso regression}.
The (regularized) least squares solution for the (expected value) of the parameters $\theta$ is given by:
\begin{equation}
\hat{\mathbf{\theta}}_{\lambda} = \left(\hat{\mathbf{X}}^{\top}\hat{\mathbf{X}}+\lambda\,\mathbf{I}\right)^{-1}\hat{\mathbf{X}}^{\top}\mathbf{y}
\end{equation}
where $\lambda$ is the regularization parameter, and $\mathbf{I}$ is the identity matrix.

The basic idea of kernel regression is to rewrite the parameter vector $\hat{\theta}_{\lambda}$ 
as a weighted sum of the augmented feature variables: 
\begin{equation}
\hat{\theta}_{\lambda} \equiv \sum_{i=1}^{n}\alpha_{i}\hat{\mathbf{x}}_{i}
\end{equation}
where $\alpha_{i}$ are the weights (that we need to estimate), and $\hat{\mathbf{x}}_{i}$ are the augmented feature vectors.
Then for some (new) feature vector $\hat{\mathbf{z}}$, i.e., a vector not in the training set, the predicted output $\hat{y}$ is given by:
\begin{align*}
\hat{y} & = \hat{\mathbf{z}}^{\top}\theta = \sum_{i=1}^{n}\alpha_{i}\left<\hat{\mathbf{z}},\mathbf{x}_{i}\right>\quad\text{|\,Replace inner product with kernel}\\
        & = \hat{\mathbf{z}}^{\top}\theta \simeq \sum_{i=1}^{n}\alpha_{i}\,k(\hat{\mathbf{z}},\mathbf{x}_{i})
\end{align*}
where $k(\hat{\mathbf{z}},\mathbf{x}_{i})$ denotes a kernel function (similarity score) between a new (augmented) feature vector and $\hat{\mathbf{z}}$
and the (known) training feature vector $\hat{\mathbf{x}}_{i}$. The question is then how to estimate the weights $\alpha_{i}$.

As it turns out the weights $\alpha_{i}$ have an analytical solution. However, before we go through this derivation
we need to introduce a slightly different form for the orignal $\theta_{\lambda}$ solution (Lemma \ref{lemma:ridge-solution}).

\begin{lemma}\label{lemma:ridge-solution}
   First, for any data matrix $\mathbf{X}$, output vector $\mathbf{y}$ and regularization parameter $\lambda\geq{0}$, we can show:
   \begin{equation*}
   \hat{\mathbf{X}}^{\top}\left(\hat{\mathbf{X}}\hat{\mathbf{X}}^{\top}+\lambda\,\mathbf{I}\right)^{-1}\mathbf{y} = \left(\hat{\mathbf{X}}^{\top}\hat{\mathbf{X}}+\lambda\,\mathbf{I}\right)^{-1}\hat{\mathbf{X}}^{\top}\mathbf{y}
   \end{equation*}
   Thus. we can rewrite the regularized least squares solution for the (expected value) of the parameters $\theta$ as:
   \begin{equation*}
   \hat{\mathbf{\theta}}_{\lambda} = \hat{\mathbf{X}}^{\top}\left(\hat{\mathbf{X}}\hat{\mathbf{X}}^{\top}+\lambda\,\mathbf{I}\right)^{-1}\mathbf{y}
   \end{equation*}
\end{lemma}


\section{Summary and Conclusions}
In this lecture, we discussed kernel functions and kernel regression.
Kernel functions are mathematical tools that enable algorithms to operate in high-dimensional spaces without explicitly computing the coordinates in those spaces.
Kernel functions are used in a variety of machine learning algorithms, including Support Vector Machines (SVMs), kernelized regression, and kernelized clustering.
They have several interpretations, including similarity measures and implicit mappings of data into high-dimensional spaces.
We introduced the linear kernel, polynomial kernel, and radial basis function (RBF) kernel as examples of kernel functions, 
and discussed the properties of valid kernel functions.
Finally, we discussed kernel regression, a non-parametric regression technique that uses kernel functions to estimate the relationship between the input and output variables.
Kernel regression is a powerful tool for modeling complex relationships in data and is widely used in machine learning applications.

\bibliography{References-L3a.bib}

\end{document}