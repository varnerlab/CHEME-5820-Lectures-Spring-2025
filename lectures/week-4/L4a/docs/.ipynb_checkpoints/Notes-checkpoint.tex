\documentclass{article}[11pt]
\usepackage{fullpage,graphicx, setspace, latexsym, cite,amsmath,amssymb,xcolor,subfigure}
%\usepackage{epstopdf}
%\DeclareGraphicsExtensions{.pdf,.eps,.png,.jpg,.mps} 
\usepackage{amssymb} %maths
\usepackage{amsmath} %maths
\usepackage{amsthm, comment}
\usepackage[round,comma,sort,numbers]{natbib}

% \bibliographystyle{plain}
\bibliographystyle{plos2015}

\newtheorem{theorem}{Theorem}
\newtheorem{prop}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{defn}{Definition}
\newtheorem{ex}{Example}
\usepackage{float}

\newcommand*{\underuparrow}[1]{\underset{\uparrow}{#1}}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage[dvipsnames]{xcolor}
\usepackage{algorithmicx}
\usepackage{algorithm} %http://ctan.org/pkg/algorithms
\usepackage{algpseudocode} %http://ctan.org/pkg/algorithmicx
\usepackage{enumitem}
\usepackage{simplemargins}
\usepackage{hyperref}

\usepackage{mdframed}
\definecolor{lgray}{rgb}{0.92,0.92,0.92}
\definecolor{lsalmon}{rgb}{0.9921568627450981,0.9411764705882353, 0.9254901960784314}

\renewcommand{\bibnumfmt}[1]{#1.}
\setlist{noitemsep} % or \setlist{noitemsep} to leave space around whole list
\setallmargins{1in}
\linespread{1.1}

\newcommand{\brows}[1]{%
  \begin{bmatrix}
  \begin{array}{@{\protect\rotvert\;}c@{\;\protect\rotvert}}
  #1
  \end{array}
  \end{bmatrix}
}
\newcommand{\rotvert}{\rotatebox[origin=c]{90}{$\vert$}}
\newcommand{\rowsvdots}{\multicolumn{1}{@{}c@{}}{\vdots}}


\def\R{\mathbb{R}}
\def\Eps{\mathcal{E}}
\def\E{\mathbb{E}}
\def\V{\mathbb{V}}
\def\F{\mathcal{F}}
\def\G{\mathcal{G}}
\def\H{\mathcal{H}}
\def\S{\mathcal{S}}
\def\D{\mathcal{D}}
\def\P{\mathbb{P}}
\def\1{\mathbf{1}}
\def\n{\nappa}
\def\h{\mathbf{w}}
\def\v{\mathbf{v}}
\def\x{\mathbf{x}}
\def\X{\mathcal{X}}
\def\Y{\mathcal{Y}}
\def\eps{\epsilon}
\def\y{\mathbf{y}}
\def\e{\mathbf{e}}
\newcommand{\norm}[1]{\left|\left|#1\right|\right|}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\lecture}[4]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   % \setcounter{lecnum}{#1}
   \setcounter{page}{1}
   \setlength{\headsep}{10mm}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf CHEME 5820: Machine Learning for Engineers
   \hfill Spring 2025} }
       \vspace{4mm}
       \hbox to 6.28in { {\Large \hfill Lecture #1: #2  \hfill} }
       \vspace{2mm}
       \hbox to 6.28in { {\it Lecturer: #3 \hfill #4} }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{Lecture #1: #2}{Lecture #1: #2}

   \noindent{\bf Disclaimer}: {\it These notes have not been subjected to the
   usual scrutiny reserved for formal publications. }
   \vspace*{4mm}
}
\begin{document}
\lecture{4c}{Kernel Functions and Kernelized Regression}{Jeffrey Varner}{}


\begin{mdframed}[backgroundcolor=lgray]
   In this lecture, we will discuss the following topics:
   \begin{itemize}[leftmargin=16pt]
      \item{\textbf{Positive definite kernel functions}: A positive definite kernel function $k:\mathbb{R}^{\star}\times\mathbb{R}^{\star}\to\mathbb{R}$ is a function that takes two vector arguments and returns a scalar that is in some sense a \textit{similarity} measure of the two input vectors. A positive definite kernel function produces a kernel matrix $\mathbf{K}$ that is positive (semi)definite.}
      \item{\textbf{Kernel machines}: A kernel machine is a class of machine learning algorithms that uses kernel functions to implicitly transform input data into a high-dimensional feature space, enabling the solution of non-linear problems using linear classifiers without explicitly computing the coordinates in that space.}
      \item{\textbf{Kernel regression}: Kernel regression is a technique that uses kernel functions to estimate (potentially) non-linear relationships between variables by assigning weights to data points based on their proximity to a point of interest, allowing for flexible modeling without assuming a specific functional form.}
   \end{itemize}
\end{mdframed}


\section{Introduction}
In this lecture, we will discuss kernel functions and kernel regression.
Kernel functions are mathematical tools that enable algorithms to operate in high-dimensional spaces without explicitly computing the coordinates in those spaces.
Kernel functions are used in various machine learning algorithms, including Support Vector Machines (SVMs), kernelized regression, and kernelized clustering.
Today, we will focus on kernel regression, a non-parametric regression technique that uses kernel functions to estimate the relationship between the input and output variables.
Let's start by discussing the basic concepts of kernel functions and then consider kernel regression.


\section{Kernel Functions}
Kernel functions are mathematical tools in machine learning that enable algorithms to operate in high-dimensional spaces without explicitly computing the coordinates of those spaces.
Kernel functions are used in various machine learning algorithms, including Support Vector Machines (SVMs), kernelized regression, and kernelized clustering.
Kernel functions have a few different interpretations. 
For example, kernel functions can be considered similarity measures, quantifying the similarity between pairs of data points in a high-dimensional space.
They also implicitly map data into a high-dimensional space, where the data becomes linearly separable (which is helpful for classification algorithms).
Thus, kernel functions are powerful tools that we will use for many machine learning applications.

A kernel function $k:\mathbb{R}^{\star}\times\R^{\star}\to\R$ is a function that takes a pair of vectors 
$\mathbf{v}_i\in\R^{\star}$ and $\mathbf{v}_j\in\R^{\star}$ as arguments, 
e.g., a pair of feature vectors, a feature vector and a parameter vector, or any two vectors of compatible size 
and computes a scalar value that represents the similarity (in some sense) between the two vector arguments.
For example, the linear kernel function computes the dot product between two vectors:
\begin{equation}
k(\mathbf{v}_i, \mathbf{v}_j) = \mathbf{v}_i^{\top}\mathbf{v}_j
\end{equation}
On the other hand, a polynomial kernel is defined as:
\begin{equation}
k_{d}(\mathbf{v}_i, \mathbf{v}_j) = (1+\mathbf{v}_i^{\top}\mathbf{v}_j)^d
\end{equation}
where $d$ is the degree of the polynomial. The radial basis function (RBF) kernel is defined as:
\begin{equation}
k_{\gamma}(\mathbf{v}_i, \mathbf{v}_j) = \exp(-\gamma \norm{\mathbf{v}_i - \mathbf{v}_j}_{2}^2)
\end{equation}
where $\gamma$ is a scaling factor, and $\norm{\cdot}^{2}_{2}$ is the squared Euclidean norm;
If we define $\gamma$ as ${1}/{2\sigma^2}$, the RBF kernel looks like a Gaussian function, without the normalization constant.
Of course, not all functions are kernel functions (Defn. \ref{def:kernel-valid}).
\begin{defn}{(Valid Kernel Function)}\label{def:kernel-valid}
A function $k:\mathbb{R}^{\star}\times\R^{\star}\to\R$ is a valid kernel function if and only if the Kernel matrix $\mathbf{K}$ is positive 
semidefinite for all possible choices of the data points $\mathbf{v}_i$, where $K_{ij} = k(\mathbf{v}_i, \mathbf{v}_j)$.
This is equivalent to saying that all eigenvalues of the Kernel matrix $\mathbf{K}$ are non-negative.
Further, for all real value vectors $\mathbf{x}$, the Kernel matrix $\mathbf{K}$ must satisfy $\mathbf{x}^{\top}\mathbf{K}\mathbf{x} \geq 0$.
\end{defn}
Kernel functions can also be combined to create more complex kernel functions using the concept of kernel composition.
For example, the sum of two valid kernel functions is also a valid kernel function. 
The product of two valid kernel functions is also a valid kernel function. 
Multiplying a kernel function by a scalar is also a valid kernel function, etc.
See \href{https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote14.html}{the CS 4780 Lecture Notes (Fall 2018)} for more details on kernel composition.

\section{Kernel Regression}
Kernel regression is a non-parametric regression technique that uses kernel functions to estimate the relationship between the input and output variables.
Kernel regression is a powerful tool for modeling complex relationships in data and is widely used in machine learning applications.
Suppose we have a dataset $\D = \{(\mathbf{x}_{i},y_{i}) \mid i = 1,2,\dots,n\}$, where the features $\mathbf{x}_i \in \mathbb{R}^{m}$ 
are $m$-dimensional vectors, and the target variables are continuous values $y_i \in\R $, e.g., the price of a house, the temperature, etc.
The basic idea behind kernel regression is to estimate the output variable $y$ as a weighted average of the output variables of the training data points, 
where the kernel functions determine the weights. The kernel regression function is defined as:
\begin{equation}
f(\mathbf{x}) = \sum_{i=1}^{n} \alpha_i k(\mathbf{x}_i, \mathbf{x})
\end{equation}
where $\alpha_i$ are the weights, and $k(\mathbf{x}_i, \mathbf{x})$ is the kernel function that measures the similarity between the input features $\mathbf{x}_i$ and $\mathbf{x}$.

\subsection{Kernel Ridge Regression}
Suppose we consider a linear regression problem of the form:
\begin{equation}
\hat{\mathbf{y}} = \hat{\mathbf{X}}\theta
\end{equation}
where $\hat{\mathbf{X}}\in\R^{n\times{p}}$ is a data matrix with the transpose of the augmented feature vectors $\hat{\mathbf{x}}^{\top}\in\R^{p}$ on the rows, and $\theta$ is an unknown parameter vector $\theta\in\mathbb{R}^{p}$ 
where $p = m+1$. We can estimate the (expected value) of the parameter vector $\theta$ by minimizing the least squares loss function:
\begin{equation}
\hat{\theta} = \argmin_{\theta} \norm{\mathbf{y} - \hat{\mathbf{X}}\theta}_{2}^{2} + \lambda\norm{\theta}_{2}^{2}
\end{equation}
where $\mathbf{y}$ is the target variable vector, and $\lambda\geq{0}$ is a regularization parameter. 
When we include the L2 penalty term, this is referred to as ridge regression, but we'll refer to it as regularized least squares regression.
If we include a different penalty, we would have a different type of regression, e.g., L1 penalty would be \href{https://en.wikipedia.org/wiki/Lasso_(statistics)}{Lasso regression}.
The (regularized) least squares solution for the (expected value) of the parameters $\theta$ is given by:
\begin{equation}\label{eq:ridge-solution}
\hat{\mathbf{\theta}}_{\lambda} = \left(\hat{\mathbf{X}}^{\top}\hat{\mathbf{X}}+\lambda\,\mathbf{I}\right)^{-1}\hat{\mathbf{X}}^{\top}\mathbf{y}
\end{equation}
where $\lambda$ is the regularization parameter, and $\mathbf{I}$ is the identity matrix.

The basic idea of kernel regression is to rewrite the parameter vector $\hat{\theta}_{\lambda}$ 
as a weighted sum of the augmented feature variables: 
\begin{equation}
\hat{\theta}_{\lambda} \equiv \sum_{i=1}^{n}\alpha_{i}\hat{\mathbf{x}}_{i}
\end{equation}
where $\alpha_{i}$ are the weights (that we need to estimate), and $\hat{\mathbf{x}}_{i}$ are the augmented feature vectors.
Then for some (new) feature vector $\hat{\mathbf{z}}$, i.e., a vector not in the training set, the predicted output $\hat{y}$ is given by:
\begin{align*}
\hat{y} & = \hat{\mathbf{z}}^{\top}\theta = \sum_{i=1}^{n}\alpha_{i}\left<\hat{\mathbf{z}},\mathbf{x}_{i}\right>\quad\text{|\,Replace inner product with kernel}\\
        & = \hat{\mathbf{z}}^{\top}\theta \simeq \sum_{i=1}^{n}\alpha_{i}\,k(\hat{\mathbf{z}},\mathbf{x}_{i})
\end{align*}
where $k(\hat{\mathbf{z}},\mathbf{x}_{i})$ denotes a kernel function (similarity score) between a new (augmented) feature vector and $\hat{\mathbf{z}}$
and the (known) training feature vector $\hat{\mathbf{x}}_{i}$. The question is how to estimate the weights $\alpha_{i}$.

As it turns out the weights $\alpha_{i}$ have an analytical solution. However, before we go through this derivation,
we need to introduce a slightly different form for the original $\theta_{\lambda}$ solution (Lemma \ref{lemma:ridge-solution}).
\begin{lemma}\label{lemma:ridge-solution}
   First, for any data matrix $\mathbf{X}$, output vector $\mathbf{y}$ and regularization parameter $\lambda\geq{0}$, we can show:
   \begin{equation*}
   \hat{\mathbf{X}}^{\top}\left(\hat{\mathbf{X}}\hat{\mathbf{X}}^{\top}+\lambda\,\mathbf{I}\right)^{-1}\mathbf{y} = \left(\hat{\mathbf{X}}^{\top}\hat{\mathbf{X}}+\lambda\,\mathbf{I}\right)^{-1}\hat{\mathbf{X}}^{\top}\mathbf{y}
   \end{equation*}
   Thus, we can rewrite the regularized least squares solution for the (expected value) of the parameters $\theta$ as:
   \begin{equation*}
   \hat{\mathbf{\theta}}_{\lambda} = \hat{\mathbf{X}}^{\top}\left(\hat{\mathbf{X}}\hat{\mathbf{X}}^{\top}+\lambda\,\mathbf{I}\right)^{-1}\mathbf{y}
   \end{equation*}
   The proof of this lemma can be found in the \href{https://engineering.purdue.edu/ChanGroup/ECE595/files/Lecture03_kernel.pdf}{ECE595/STAT598 Course Notes, Prof. S Chan, Purdue University}.
\end{lemma}
Starting from Eqn \ref{eq:ridge-solution} and using Lemma \ref{lemma:ridge-solution}, we equate the two expressions for $\hat{\theta}_{\lambda}$:
\begin{align*}
\left(\hat{\mathbf{X}}^{\top}\hat{\mathbf{X}}+\lambda\,\mathbf{I}\right)^{-1}\hat{\mathbf{X}}^{\top}\mathbf{y} & = \sum_{i=1}^{n}\alpha_{i}\hat{\mathbf{x}}_{i}\quad{\mid\text{rewrite right hand side in vector form}}\\
\hat{\mathbf{X}}^{\top}\left(\hat{\mathbf{X}}\hat{\mathbf{X}}^{\top}+\lambda\,\mathbf{I}\right)^{-1}\mathbf{y} & = \hat{\mathbf{X}}^{\top}\alpha\quad\mid\text{multiply by $\hat{\mathbf{X}}$} \\
\hat{\mathbf{X}}\hat{\mathbf{X}}^{\top}\left(\hat{\mathbf{X}}\hat{\mathbf{X}}^{\top}+\lambda\,\mathbf{I}\right)^{-1}\mathbf{y} & = \hat{\mathbf{X}}\hat{\mathbf{X}}^{\top}\alpha\quad\mid\text{substitute $\mathbf{K}^{\prime} = \hat{\mathbf{X}}\hat{\mathbf{X}}^{\top}$}\\
\mathbf{K}^{\prime}\left(\mathbf{K}^{\prime}+\lambda\,\mathbf{I}\right)^{-1}\mathbf{y} & = \mathbf{K}^{\prime}\alpha\quad\mid\text{multiply by the inverse of $\mathbf{K}^{\prime}$}\\
\left(\mathbf{K}^{\prime}+\lambda\,\mathbf{I}\right)^{-1}\mathbf{y} & = \alpha
\end{align*}
For an inner product kernel, the matrix $\mathbf{K}^{\prime}$ is the Gram matrix $\mathbf{K}$ with elements $K_{ij} = \hat{\mathbf{x}}_{i}^{\top}\hat{\mathbf{x}}_{j}$.

\section{Summary and Conclusions}
In this lecture, we discussed kernel functions and kernel regression.
Kernel functions are mathematical tools that enable algorithms to operate in high-dimensional spaces without explicitly computing the coordinates in those spaces.
Kernel functions are used in various machine learning algorithms, including Support Vector Machines (SVMs), kernelized regression, and kernelized clustering.
They have several interpretations, including similarity measures and implicit data mappings into high-dimensional spaces.
We introduced the linear kernel, polynomial kernel, and radial basis function (RBF) kernel as examples of kernel functions, 
and discussed the properties of valid kernel functions.
Finally, we discussed kernel regression, a non-parametric regression technique that uses kernel functions to estimate the relationship between the input and output variables.
Kernel regression is a powerful tool for modeling complex relationships in data and is widely used in machine learning applications.

% \bibliography{References-L3a.bib}

\end{document}