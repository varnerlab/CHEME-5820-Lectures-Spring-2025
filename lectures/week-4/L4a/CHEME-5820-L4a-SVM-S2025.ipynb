{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a31634a-4e99-444b-ad3c-0a4a5a3620c9",
   "metadata": {},
   "source": [
    "# L4a: Kernel Functions and Kernel Machines\n",
    "In this lecture, we'll take a theoretical detour and discuss [positive definite kernel functions](https://en.wikipedia.org/wiki/Positive-definite_kernel) and introduce the ideas behind our first [kernel machine](https://en.wikipedia.org/wiki/Kernel_method), namely [kernel regression](https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote14.html). The key ideas of this lecture include:\n",
    "* __Positive definite kernel functions__: A positive definite kernel function $k:\\mathbb{R}^{\\star}\\times\\mathbb{R}^{\\star}\\to\\mathbb{R}$ is a function that takes two vector arguments and returns a scalar that is in some sense a _similarity_ measure of the two input vectors. A positive definite kernel function produces a kernel matrix $\\mathbf{K}$ that is positive (semi)definite.\n",
    "* __Kernel machines__: A kernel machine is a class of machine learning algorithms that uses kernel functions to implicitly transform input data into a high-dimensional feature space, enabling the solution of non-linear problems using linear classifiers without explicitly computing the coordinates in that space.\n",
    "* __Kernel regression__: Kernel regression is a technique that uses kernel functions to estimate (potentially) non-linear relationships between variables by assigning weights to data points based on their proximity to a point of interest, allowing for flexible modeling without assuming a specific functional form.\n",
    "\n",
    "Lecture notes can be found: [here!](https://github.com/varnerlab/CHEME-5820-Lectures-Spring-2025/blob/main/lectures/week-4/L4a/docs/Notes.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dff6ea4-ae7c-4441-8275-49adf037daa1",
   "metadata": {},
   "source": [
    "## Setup, Data, and Prerequisites\n",
    "We set up the computational environment by including the `Include.jl` file, loading any needed resources, such as sample datasets, and setting up any required constants. The `Include.jl` file loads external packages, various functions that we will use in the exercise, and custom types to model the components of our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34a1146f-5550-425f-8b1a-f7c28b13342f",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"Include.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c90f56-7893-4bbf-a215-9124b8164940",
   "metadata": {},
   "source": [
    "### Data\n",
    "We gathered a daily open-high-low-close `dataset` for each firm in the [S&P500](https://en.wikipedia.org/wiki/S%26P_500) from `01-03-2014` until `02-07-2025`, along with data for a few exchange-traded funds and volatility products during that time. We load the `orignal_dataset` by calling the `MyMarketDataSet()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a524ae8-284a-4c53-876a-9081522a165d",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dataset = MyMarketDataSet() |> x-> x[\"dataset\"];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b90d9f0-b657-4231-a4fd-6e221291f522",
   "metadata": {},
   "source": [
    "\n",
    "__Clean the data__: Not all tickers in our dataset have the maximum number of trading days for various reasons, e.g., acquisition or de-listing events. Let's collect only those tickers with the maximum number of trading days.\n",
    "\n",
    "* First, let's compute the number of records for a company that we know has a maximum value, e.g., `AAPL`, and save that value in the `maximum_number_trading_days` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bd48bcd-e48c-4e0e-ac4a-d87502280055",
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum_number_trading_days = original_dataset[\"AAPL\"] |> nrow;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df21688-df65-493f-9940-be418587ca5a",
   "metadata": {},
   "source": [
    "Now, lets iterate through our data and collect only those tickers that have `maximum_number_trading_days` records. Save that data in the `dataset::Dict{String,DataFrame}` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "155c0808-9e37-4f3c-a114-62a2bcf1cf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = let\n",
    "\n",
    "    dataset = Dict{String,DataFrame}();\n",
    "    for (ticker,data) ∈ original_dataset\n",
    "        if (nrow(data) == maximum_number_trading_days)\n",
    "            dataset[ticker] = data;\n",
    "        end\n",
    "    end\n",
    "    dataset\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdf5d55-eb01-4798-a76d-b042812ac00e",
   "metadata": {},
   "source": [
    "Let's get a list of firms in the cleaned up `dataset` and save it in the `all_tickers` array. We sort the firms alphabetically from `A` to `Z`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88d92a9d-6fa5-4c70-9eaa-67763a39d1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_all_tickers = keys(dataset) |> collect |> sort;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61433052-040a-4c02-a8ef-ecea1425cd58",
   "metadata": {},
   "source": [
    "Compute the expected (annualized) excess log growth rate by passing the `dataset` and the entire list of firms we have in the dataset to the [log_growth_matrix(...) method](src/Compute.jl). The log growth rate between time period $j-1$ to $j$, e.g., yesterday to today is defined as:\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mu_{j,j-1} = \\left(\\frac{1}{\\Delta{t}}\\right)\\ln\\left(\\frac{S_{j}}{S_{j-1}}\\right)\n",
    "\\end{equation}\n",
    "$$\n",
    "where $\\Delta{t}$ denotes the period time step, and $S_{j}$ denote share price in period $j$.\n",
    "* The log growth rates are stored in the `D::Array{Float64,2}` variable, a $T-1\\times{N}$ array of log return values. Each row of the `D` matrix corresponds to a time value, while each column corresponds to a firm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9c50373-b6a3-486f-bc85-9c2ffab00a21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2791×424 Matrix{Float64}:\n",
       " -0.919054     6.23955    -2.91247    …   -0.796891    0.204394  -1.04677\n",
       "  2.77476      1.02999     1.35089         2.09682    -0.84429    0.944968\n",
       "  3.27155      0.814097   -0.036132        0.068377    1.1495    -2.62294\n",
       "  0.604925    17.2184      1.65065         0.233216    3.1178    -0.409728\n",
       "  1.77459      2.53811     3.27774         0.580177   -2.2102     4.36159\n",
       "  0.57233     -4.00534    -0.83428    …   -0.904239   -1.95127   -3.15774\n",
       "  2.81921     -0.525251    4.80423         1.7242     -1.81835   -1.1311\n",
       "  2.00521      0.972004    1.86659         1.63447     4.40834   -0.179319\n",
       "  1.27139      1.63263     0.0657592      -1.54858    -2.17846    1.39634\n",
       "  1.17866      6.08807     0.891078       -1.57352     2.83634   -1.47776\n",
       " -0.479168     4.82859     0.96624    …   -0.362761    9.46677   -3.05023\n",
       "  1.32131      3.57167    -2.38926         0.669113    4.48073    0.299031\n",
       " -4.78054      1.34435    -3.05774        -2.19395    -6.69057    1.36462\n",
       "  ⋮                                   ⋱                          \n",
       " -1.47096    -21.5367      4.50442        -1.68655    -0.759025  -0.770983\n",
       " -0.0717231   -1.64313     3.59808    …   -2.40644     2.10455    2.58079\n",
       " -1.72058      3.09602     4.41392        -9.38229    -2.70206    6.11174\n",
       "  0.885817    -0.261178   -1.37645         0.404962   -1.76972   -0.762716\n",
       " -6.30739     -0.714907    0.885508       -0.459363    0.77441   -2.05469\n",
       "  6.28051     -3.20957     4.7925         -2.73308     2.62737    4.33225\n",
       "  1.40002      1.40186    -5.82424    …   -0.887621   -1.7872    -2.57942\n",
       " -5.44561     -6.80852    -4.27744        -6.80995    -7.53805    0.529063\n",
       " -3.14876      3.44566     8.65486         0.0722037   3.53295    0.340111\n",
       "  1.42848      1.88871    -1.651           1.53426     0.858419   4.29793\n",
       " -0.689024     1.81352    -0.0665703      -5.369       3.36167    0.0911751\n",
       " -2.77586      2.62659   -14.4261     …  -10.239      -3.23907   -4.34116"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = let\n",
    "\n",
    "    # setup some constants -\n",
    "    Δt = (1/252); # 1-trading day in units of years\n",
    "    risk_free_rate = 0.0415; # inferred cc risk-free rate\n",
    "\n",
    "    # compute\n",
    "    μ = log_growth_matrix(dataset, list_of_all_tickers, Δt = Δt, \n",
    "        risk_free_rate = risk_free_rate);\n",
    "\n",
    "    # return to caller\n",
    "    μ\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d89ec7-871e-4ba0-95e7-aaab356f3cc7",
   "metadata": {},
   "source": [
    "Next, let's [z-score center](https://en.wikipedia.org/wiki/Feature_scaling) the continous feature data. In [z-score feature scaling](https://en.wikipedia.org/wiki/Feature_scaling), we subtract off the mean of each feature and then divide by the standard deviation, i.e., $x^{\\prime} = (x - \\mu)/\\sigma$ where $x$ is the unscaled data, and $x^{\\prime}$ is the scaled data. Under this scaling regime, $x^{\\prime}\\leq{0}$ will be values that are less than or equal to the mean value $\\mu$, while $x^{\\prime}>0$ indicate values that are greater than the mean.\n",
    "\n",
    "We save the z-score centered growth data data in the `D̄::Array{Float64,2}` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ce7190a-46f4-4ec7-80d5-c90d4de56f8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2791×424 Matrix{Float64}:\n",
       " -0.270236    0.875241   -0.51285    …  -0.172603    0.0340086  -0.326599\n",
       "  0.735235    0.153683    0.270667       0.365351   -0.156963    0.235764\n",
       "  0.870466    0.123779    0.0157621     -0.0117454   0.206118   -0.771629\n",
       "  0.144598    2.39589     0.325758       0.018899    0.564557   -0.146732\n",
       "  0.462985    0.362567    0.624783       0.0834006  -0.405704    1.20044\n",
       "  0.135725   -0.543745   -0.130921   …  -0.192559   -0.35855    -0.922629\n",
       "  0.747335   -0.0617295   0.905321       0.29608    -0.334345   -0.35041\n",
       "  0.52576     0.145651    0.365443       0.279399    0.799572   -0.0816769\n",
       "  0.326013    0.237151    0.0344876     -0.312346   -0.399924    0.363208\n",
       "  0.30077     0.854261    0.186164      -0.316981    0.513302   -0.448288\n",
       " -0.150497    0.679814    0.199977   …  -0.0918959   1.72074    -0.892273\n",
       "  0.339601    0.505721   -0.416694       0.0999343   0.812754    0.0533844\n",
       " -1.32135     0.197223   -0.539546      -0.432322   -1.22161     0.354251\n",
       "  ⋮                                  ⋱                          \n",
       " -0.420465   -2.97196     0.850221      -0.337995   -0.141436   -0.248732\n",
       " -0.0395886  -0.216563    0.683655   …  -0.471825    0.380037    0.697634\n",
       " -0.488413    0.439841    0.83359       -1.76867    -0.495274    1.69459\n",
       "  0.221058   -0.0251536  -0.23056        0.0508272  -0.325489   -0.246398\n",
       " -1.73696    -0.0879981   0.18514       -0.109855    0.137812   -0.611184\n",
       "  1.68952    -0.433526    0.903164      -0.532549    0.475246    1.19216\n",
       "  0.361025    0.205188   -1.04797    …  -0.18947    -0.328673   -0.759341\n",
       " -1.50238    -0.932005   -0.763702      -1.29046    -1.37594     0.118333\n",
       " -0.877171    0.488268    1.61299       -0.011034    0.640157    0.0649832\n",
       "  0.368773    0.27262    -0.281018       0.260768    0.15311     1.18247\n",
       " -0.20762     0.262206    0.0101682     -1.02258     0.608967   -0.0053034\n",
       " -0.775666    0.374822   -2.62881    …  -1.92793    -0.593067   -1.25676"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D̄ = let\n",
    "\n",
    "    # setup -\n",
    "    number_of_examples = size(D,1);\n",
    "\n",
    "    D̄ = copy(D);\n",
    "    for j ∈ eachindex(list_of_all_tickers)\n",
    "        μ = mean(D[:,j]); # compute the mean\n",
    "        σ = std(D[:,j]); # compute std\n",
    "\n",
    "        # rescale -\n",
    "        for k ∈ 1:number_of_examples\n",
    "            D̄[k,j] = (D[k,j] - μ)/σ;\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    D̄\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894027f7-053f-4623-a854-22f6f2efd3f2",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3a65e7b9-1db1-42be-a13a-3a8553c79e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "γ = 0.0197; # Length scale for the RBF kernel. I estimated this from another source, don't mess with it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140cc400-873d-4742-a1d4-aa01a0545013",
   "metadata": {},
   "source": [
    "## Kernel Functions\n",
    "Kernel functions in machine learning are mathematical tools that enable algorithms to operate in high-dimensional spaces without explicitly computing the coordinates in those spaces. _Huh??_\n",
    "\n",
    "* __What are kernel functions__? A kernel function $k:\\mathbb{R}^{\\star}\\times\\mathbb{R}^{\\star}\\to\\mathbb{R}$ takes a pair of vectors $\\mathbf{v}_i\\in\\mathbb{R}^{\\star}$ and $\\mathbf{v}_j\\in\\mathbb{R}^{\\star}$ as arguments, \n",
    "e.g., a pair of feature vectors, a feature vector and a parameter vector, or any two vectors of compatible size \n",
    ", computes a scalar value that represents the similarity (in some sense) between the two vector arguments.\n",
    "* __Common kernel functions__: Common kernel functions include the linear kernel, which is the dot product between two vectors $k(\\mathbf{z}_i, \\mathbf{z}_j) = \\mathbf{z}_i^{\\top}\\mathbf{z}_j$, the polynomial kernel is defined as: $k_{d}(\\mathbf{z}_i, \\mathbf{z}_j) = (1+\\mathbf{z}_i^{\\top}\\mathbf{z}_j)^d$ and the radial basis function (RBF) kernel $k_{\\gamma}(\\mathbf{z}_i, \\mathbf{z}_j) = \\exp(-\\gamma \\lVert\\mathbf{z}_i - \\mathbf{z}_j\\rVert_{2}^2)$ where $\\gamma$ is a scaling factor, and $\\lVert\\cdot\\rVert^{2}_{2}$ is the squared Euclidean norm\n",
    "* __What do kernel functions do__? Kernel functions are similarity measures, i.e., they quantify the similarity between pairs of points in _some_ high-dimensional space. For example, we could use a kernel function to measure the similarity between augmented feature vectors $\\hat{\\mathbf{x}}_{i}$ and $\\hat{\\mathbf{x}}_{j}$, or we could construct higher dimensional versions of the features, for example, by including $x^{2}, x^{3}, \\dots,$ terms in addition the original features. Increasing the dimension is [sometimes referred to as feature engineering](https://en.wikipedia.org/wiki/Feature_engineering), i.e., transforming raw data into a more effective set of features.\n",
    "\n",
    "Let's look at a couple of example kernels that are [exported by the `KernelFunctions.jl` package](https://github.com/JuliaGaussianProcesses/KernelFunctions.jl) and apply them to our financial dataset.\n",
    "\n",
    "* __Experiment__: If two tickers are similar industries, e.g., `NVDA` and `AMD`, or 'GS' and `JPM`, then they should have a high similarity score. However, if tickers can be anticorrelated, e.g., `SPY` or `QQQ` and `GLD`. Anticorrelated firms will have low similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc42304b-3678-483a-8ee0-ffaeb603d858",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_score, tᵢ, tⱼ = let\n",
    "\n",
    "    # Specify two tickers that we are going to compare -\n",
    "    tᵢ,tⱼ = \"NVDA\", \"WMT\"\n",
    "    i = findfirst(s-> s == tᵢ, list_of_all_tickers); # returns index of ticker 1\n",
    "    j = findfirst(s-> s == tⱼ, list_of_all_tickers); # returns index of ticker 2\n",
    "\n",
    "    # get z-score scaled return vectors for (i,j)\n",
    "    μᵢ,μⱼ = D̄[:,i],D̄[:,j];\n",
    "    \n",
    "    # setup the kernel -\n",
    "    # k = LinearKernel(); # inner product kernel\n",
    "    k = SqExponentialKernel() ∘ ScaleTransform(γ) # RBF kernel w/scaling parameter γ\n",
    "\n",
    "    # compute the similarity -\n",
    "    score = k(μᵢ,μⱼ)\n",
    "\n",
    "    # return -\n",
    "    score, tᵢ, tⱼ\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78b1c6a6-a11c-4763-8041-ab12d7ece6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tickers: (NVDA,WMT) have similarity: 0.3994080617533056\n"
     ]
    }
   ],
   "source": [
    "println(\"Tickers: ($(tᵢ),$(tⱼ)) have similarity: $(similarity_score)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd802c5a-1f1b-41b8-90f4-bea55dcb07eb",
   "metadata": {},
   "source": [
    "## Can any function be a kernel function?\n",
    "No! Not all functions can be kernel functions; there are some rules. \n",
    "* __Rules for a valid kernel function__: A function $k:\\mathbb{R}^{\\star}\\times\\mathbb{R}^{\\star}\\to\\mathbb{R}$ is a _valid kernel function_ if and only if the kernel matrix $\\mathbf{K}\\in\\mathbb{R}^{m\\times{m}}$ is positive semidefinite for all possible choices of the data vectors $\\mathbf{v}_i$, where $K_{ij} = k(\\mathbf{v}_i, \\mathbf{v}_j)$.\n",
    "This is equivalent to saying that all eigenvalues of the kernel matrix $\\mathbf{K}$ are non-negative (and real).\n",
    "Further, for any real valued vector $\\mathbf{x}$, the Kernel matrix $\\mathbf{K}$ must satisfy $\\mathbf{x}^{\\top}\\mathbf{K}\\mathbf{x} \\geq 0$. Finally, when the kernel function is an (untransformed) inner product, the Kernel matrix is equal to [the Gram matrix](https://en.wikipedia.org/wiki/Gram_matrix).\n",
    "\n",
    "* __What is the Gram matrix__? For a set of $n$-vectors, $\\mathbf{v}_{1},\\mathbf{v}_{2},\\dots,\\mathbf{v}_{m}$, [the Gram matrix](https://en.wikipedia.org/wiki/Gram_matrix) $\\mathbf{K}$ is a $m\\times{m}$ matrix with elements  $K_{ij}=\\left<\\mathbf{v}_{i},\\mathbf{v}_{j}\\right>$, where $\\left<\\star,\\star\\right>$ denotes an inner product. If the vectors $\\mathbf{v}_{1},\\mathbf{v}_{2},\\dots,\\mathbf{v}_{m}$ are the _columns_ of the matrix $\\mathbf{X}$, then [the Gram matrix](https://en.wikipedia.org/wiki/Gram_matrix) is given by $\\mathbf{K} = \\mathbf{X}^{\\top}\\mathbf{X}$ (assuming all the entries in $\\mathbf{X}$ are real).\n",
    "\n",
    "Let's check these properties using our financial data. Let's compute [the Gram matrix](https://en.wikipedia.org/wiki/Gram_matrix) $\\mathbf{K}$, and then compute its eigendecomposition [using the `eigen(...)` method exported by the `LinearAlgebra.jl` package](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.eigen).\n",
    "\n",
    "First, let's build a Kernel matrix $\\mathbf{K}$ by computing the $K_{ij} = k(\\mathbf{v}_{i},\\mathbf{v}_{j})$ for all firms in our dataset (the columns). Save this matrix in the `K₁::Array{Float64,2}` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9d72c343-9e5e-4e16-af6f-ede936b5cf3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "424×424 Matrix{Float64}:\n",
       " 2790.0     853.347   742.766  1162.49   …  1261.86    977.207  1430.76\n",
       "  853.347  2790.0     769.093   811.779      924.723  1214.06    764.885\n",
       "  742.766   769.093  2790.0     568.675      738.262   887.252   723.949\n",
       " 1162.49    811.779   568.675  2790.0       1136.34    693.904  1195.26\n",
       "  880.301   437.075   522.057   614.802      521.774   547.608   946.943\n",
       " 1462.46    587.197   591.556  1045.19   …   944.425   667.969  1382.24\n",
       " 1434.56    946.096   826.207  1342.82      1196.96   1049.75   1383.15\n",
       " 1276.66    651.785   549.673  1361.87      1098.5     580.29   1186.05\n",
       " 1377.87   1076.05    724.338  1410.39      1376.32   1045.33   1114.97\n",
       "  869.606   846.611   713.64    655.534      761.013  1116.03    701.958\n",
       " 1227.59    986.172   816.383  1080.43   …  1069.42   1049.42   1255.51\n",
       " 1431.85    983.995   647.768  1304.36      1275.42    891.55   1177.11\n",
       "  688.196   384.223   561.935   566.935      412.38    424.248   835.09\n",
       "    ⋮                                    ⋱                      \n",
       " 1230.26    337.165   522.267   923.399      812.213   493.449  1143.97\n",
       "  978.222   990.421   776.003   829.459      914.137  1157.18    935.853\n",
       " 1239.65   1219.05    938.365  1052.06      1170.04   1222.65   1139.91\n",
       "  932.921  1283.85    658.289   904.679  …   941.287  1067.4     877.813\n",
       "  597.764   271.92    530.814   570.948      288.173   269.609   784.193\n",
       "  809.819   957.202   646.463   698.714      805.133  1280.32    679.499\n",
       "  991.834   859.58    697.742   692.555      707.325  1002.25    949.984\n",
       " 1388.13   1170.99    806.278  1055.2       1160.8    1272.13   1153.81\n",
       "  994.567   885.66    746.998   925.167  …   834.614   779.007  1114.32\n",
       " 1261.86    924.723   738.262  1136.34      2790.0    1071.37   1089.01\n",
       "  977.207  1214.06    887.252   693.904     1071.37   2790.0     734.056\n",
       " 1430.76    764.885   723.949  1195.26      1089.01    734.056  2790.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K₁ = let\n",
    "\n",
    "    k = LinearKernel(); # inner product kernel\n",
    "    # k = SqExponentialKernel() ∘ ScaleTransform(γ) # RBF kernel w/scaling parameter γ\n",
    "    # k(x,y) = norm(x-y).^2; # some rando function that I make up\n",
    "    X = D̄; # we are going to look at the scaled return data\n",
    "    number_of_firms = size(D̄,2); # number of columns in the dataset\n",
    "    K = zeros(number_of_firms, number_of_firms);\n",
    "\n",
    "    for i ∈ 1:number_of_firms\n",
    "        vᵢ = X[:,i];\n",
    "        for j ∈ 1:number_of_firms\n",
    "            vⱼ = X[:,j];\n",
    "            K[i,j] = k(vᵢ,vⱼ);\n",
    "        end\n",
    "    end\n",
    "\n",
    "    K\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10aa1e5-c99e-4d34-9318-203a861c22b3",
   "metadata": {},
   "source": [
    "Next, compute the matrix product $\\mathbf{X}^{\\top}\\mathbf{X}$ and save it to the `K₂::Array{Float64,2}` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "250d2ee9-91a3-4521-b9d6-7b7e66226d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "K₂ = transpose(D̄)*D̄ |> Matrix;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59122c80-34a8-4305-9a60-60b1c72a040c",
   "metadata": {},
   "source": [
    "Is $\\mathbf{K}_{1} = \\mathbf{K}_{2}$? We'll use the shortcut version `≈` of [the `isapprox(...)` method](https://docs.julialang.org/en/v1/base/math/#Base.isapprox) to check if $\\mathbf{K}_{1} = \\mathbf{K}_{2}$.\n",
    "* __Expectation__:  If the kernel function is the inner product, then we expect $\\mathbf{K}_{1} = \\mathbf{K}_{2}$. Otherwise, we expect $\\mathbf{K}_{1} \\neq \\mathbf{K}_{2}$. If $\\mathbf{K}_{1} = \\mathbf{K}_{2}$ then this check will return `true`, if not it returns `false`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00088abd-fb31-4ab3-ac19-1bc1b801a586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K₁ ≈ K₂"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d151b0cd-73c7-48b9-8860-b014a6ded46f",
   "metadata": {},
   "source": [
    "__Eigenvalues of kernel matrix are non-negative__: A valid kernel function will have a kernel matrix whose eigenvalues are all non-negative. Let the kernel matrix be $\\mathbf{K}$ and let $\\lambda_{1},\\lambda_{2},\\dots\\lambda_{m}$ be the eigenvalues of $\\mathbf{K}$. If $k:\\mathbb{R}^{\\star}\\times\\mathbb{R}^{\\star}\\to\\mathbb{R}$ is a valid kernel function, then $\\lambda_{i}\\geq{0}$ and real $i=1,2\\dots,m$.\n",
    "\n",
    "Let's test this idea using the `K₁::Array{Float64,2}` kernel matrix we computed above. We'll compute the eigendecomposition of the kernel matrix [using the `eigen(...)` method exported by the `LinearAlgebra.jl` package](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.eigen)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c0630532-d0cd-4696-b2a6-5ddbb43ab5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "λ,V = let\n",
    "\n",
    "    F = eigen(K₁);\n",
    "    λ = F.values;\n",
    "    V = F.vectors;\n",
    "\n",
    "    λ,V\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48adbe44-f933-48d3-a612-ef6f8148dc65",
   "metadata": {},
   "source": [
    "Are any of the values in the $\\lambda$ vector negative?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6a7bf11b-5dfc-48ec-9099-4d3835f944e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i ∈ eachindex(λ)\n",
    "    λᵢ = λ[i];\n",
    "    @assert λᵢ ≥ 0.0\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80038036-1c66-4862-886e-d54692dab44a",
   "metadata": {},
   "source": [
    "__Random vector check__: Finally, for any real value vector $\\mathbf{x}$, the Kernel matrix $\\mathbf{K}$ must satisfy $\\mathbf{x}^{\\top}\\mathbf{K}\\mathbf{x} \\geq 0$. Let's test this idea using the `K₁::Array{Float64,2}` kernel matrix we computed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f767e663-b064-4580-b2e8-3292f74b4987",
   "metadata": {},
   "outputs": [],
   "source": [
    "value = let\n",
    "    # setup -\n",
    "    number_of_firms = size(D̄,2); # number of columns in the dataset\n",
    "    x = randn(number_of_firms); # generate a random vector\n",
    "\n",
    "    # test -\n",
    "    value = transpose(x)*K₁*x;\n",
    "\n",
    "    # return \n",
    "    value\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "13367c90-3c13-41d1-bb8d-ed07cf58e0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the x^T*K*x value? 1.7592022836452746e6\n"
     ]
    }
   ],
   "source": [
    "println(\"What is the x^T*K*x value? $(value)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00faec09-19c2-49f1-9d03-e0aae6dbac9d",
   "metadata": {},
   "source": [
    "__Are Kernel Matrices Symmetric__? Let's do a quick to show that are a kernel matrix is symmetric, i.e, that $\\mathbf{K}=\\mathbf{K}^{\\top}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "55bba321-7cf6-4c89-9980-44566a777721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transpose(K₁) ≈ K₁"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "34e0c93d-a507-4877-b52d-5dcf7d6dbf90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transpose(K₂) ≈ K₂"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a154ca-94c6-41c8-ac25-261bdcf339b0",
   "metadata": {},
   "source": [
    "## Kernel regression\n",
    "Kernel regression is a non-parametric technique to model (potentially) non-linear relationships between variables. It moves away from having one _global_ model to describe data and instead uses a weighted combination of many _local_ (potentially) nonlinear models to describe the data.\n",
    "* __How does it work__? Kernel regression uses a _kernel function_ to assign weights to new (unseen) data points based on their proximity (similarity) to a known data, allowing for estimating a smooth curve or function that describes the relationship between dependent and independent variables. Thus, we shift to a weighted combination of many _local_ models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5fa2d1-0c35-4c3a-ab50-e632c23b530e",
   "metadata": {},
   "source": [
    "### Theory\n",
    "Suppose we have a dataset $\\mathcal{D} = \\{(\\mathbf{x}_{i},y_{i}) \\mid i = 1,2,\\dots,n\\}$, where the features $\\mathbf{x}_i \\in \\mathbb{R}^{m}$ are $m$-dimensional vectors ($m\\ll{n}$) and the target variables are continuous values $y_i \\in \\mathbb{R}$, e.g., the price of a house, the price of a stock, the temperature, etc. We can model this as a linear regression problem:\n",
    "$$\n",
    "\\hat{\\mathbf{y}} = \\hat{\\mathbf{X}}\\theta\n",
    "$$\n",
    "where $\\hat{\\mathbf{X}}$ is a data matrix with the transpose of the augmented feature vectors $\\hat{\\mathbf{x}}^{\\top}$ on the rows, and $\\theta$ is an unknown parameter vector $\\theta\\in\\mathbb{R}^{p}$ where $p = m+1$. The (regularized) least squares solution for the parameters $\\theta$ is given by:\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\hat{\\mathbf{\\theta}}_{\\lambda} = \\left(\\hat{\\mathbf{X}}^{\\top}\\hat{\\mathbf{X}}+\\lambda\\,\\mathbf{I}\\right)^{-1}\\hat{\\mathbf{X}}^{\\top}\\mathbf{y}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "#### Kernel regression\n",
    "The basic idea of kernel regression is to rewrite the parameter vector $\\hat{\\theta}_{\\lambda}$ as a sum of the _augmented feature variables_: $\\hat{\\theta}_{\\lambda} \\equiv \\sum_{i=1}^{n}\\alpha_{i}\\hat{\\mathbf{x}}_{i}$. Then for some (new) feature vector $\\hat{\\mathbf{z}}$,  the predicted output $\\hat{y}$ is given by:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{y} & = \\hat{\\mathbf{z}}^{\\top}\\theta = \\sum_{i=1}^{n}\\alpha_{i}\\left<\\hat{\\mathbf{z}},\\mathbf{x}_{i}\\right>\\quad\\text{|\\,Replace inner product with kernel}\\\\\n",
    "        & = \\hat{\\mathbf{z}}^{\\top}\\theta \\simeq \\sum_{i=1}^{n}\\alpha_{i}\\,k(\\hat{\\mathbf{z}},\\mathbf{x}_{i})\n",
    "\\end{align}\n",
    "$$\n",
    "where $k(\\hat{\\mathbf{z}},\\mathbf{x}_{i})$ denotes a kernel function (similarity score) between a new (augmented) feature vector and $\\hat{\\mathbf{z}}$ and the (known) training feature vector $\\hat{\\mathbf{x}}_{i}$. We need to estimate the $\\alpha_{i}$ parameters; however, this is not as hard as it may first appear.\n",
    "\n",
    "__How are $\\alpha$ and $\\theta$ related__? \n",
    "The two expression for $\\hat{\\theta}_{\\lambda}$ can be equated:\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\left(\\hat{\\mathbf{X}}^{\\top}\\hat{\\mathbf{X}}+\\lambda\\,\\mathbf{I}\\right)^{-1}\\hat{\\mathbf{X}}^{\\top}\\mathbf{y} = \\hat{\\mathbf{X}}^{\\top}\\alpha\n",
    "\\end{equation}\n",
    "$$\n",
    "After some algebraic manipulation that [is shown in the course notes](https://github.com/varnerlab/CHEME-5820-Lectures-Spring-2025/blob/main/lectures/week-4/L4a/docs/Notes.pdf), this expression can be solved for the expansion coefficients:\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\alpha = \\left(\\mathbf{K}^{\\prime}+\\lambda\\mathbf{I}\\right)^{-1}\\mathbf{y}\n",
    "\\end{equation}\n",
    "$$\n",
    "where $\\mathbf{K}^{\\prime} = \\hat{\\mathbf{X}}\\hat{\\mathbf{X}}^{\\top}$, the matrix $\\mathbf{I}$ denotes the identity matrix, the vector $\\mathbf{y}$ denotes the observed outputs and $\\lambda\\geq{0}$ denotes the regularization parameter. \n",
    "\n",
    "__Hmmm__: That's interesting! In lab tomorrow, we are going to build one of these to model stock prices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0f96a3-4a76-4bf1-a4e3-6e762485f4eb",
   "metadata": {},
   "source": [
    "# Today\n",
    "That's a wrap! What are some things we discussed today?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a8604c-97b9-433d-b225-f5e5cbfd7b85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.3",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
