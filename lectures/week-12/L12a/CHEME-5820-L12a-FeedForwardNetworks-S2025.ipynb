{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9298eb4a-7abb-40a4-a9a0-aff9cfe005f1",
   "metadata": {},
   "source": [
    "# L12a: Feed Forward Neural Networks (FNNs)\n",
    "\n",
    "___\n",
    "\n",
    "In this lecture, we introduce Feed Forward Neural Networks (FNNs), an artificial neural network structure where connections between layers of nodes do not form cycles. This differs from recurrent neural networks (RNNs), where data can flow in cycles (we'll look at recurrent networks next time). The key concepts discussed in this lecture are:\n",
    "\n",
    "* __FNN Architecture__: Feedforward neural networks (FNNs) are foundational artificial neural network architectures in which information flows unidirectionally from input nodes through (potentially many) hidden layers of arbitrary dimension to output nodes without cycles or feedback loops. Each node in the network is a simple processing unit that applies a linear transformation followed by a (potentially) non-linear activation function to its inputs. \n",
    "* __FNN Applications__: FNNs are widely employed for pattern recognition, classification tasks (including for non-linearly separable data), and predictive modeling, such as identifying objects in images, sentiment analysis in text, or forecasting process trends. They also work well in structured data applications like medical diagnostics (classification of patient records) and marketing (personalized recommendations). FFNs also are components of more advanced architectures in fields like computer vision and natural language processing.\n",
    "* __FNN Training__: FNNs are trained using supervised learning, where the model learns to map inputs to outputs by minimizing a loss function. The most common training algorithm [is backpropagation](https://en.wikipedia.org/wiki/Backpropagation), which uses [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) to update the weights (and bias values) of the network based on the error between predicted and actual outputs.\n",
    "\n",
    "The source(s) for this lecture can be found here:\n",
    "* [John Hertz, Anders Krogh, and Richard G. Palmer. 1991. Introduction to the theory of neural computation. Addison-Wesley Longman Publishing Co., Inc., USA.](https://dl.acm.org/doi/10.5555/104000)\n",
    "* [Mehlig, B. (2021). Machine Learning with Neural Networks. Chapter 5: Perceptrons and Chapter 6: Stochastic Gradient Descent](https://arxiv.org/abs/1901.05639v4)\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0d82fb",
   "metadata": {},
   "source": [
    "However, before we do anything, let's set up the computational environment, e.g., importing the necessary libraries (and codes) by including the `Include.jl` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44cf1c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"Include.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bacf60",
   "metadata": {},
   "source": [
    "## Origin story: McCulloch-Pitts Neurons\n",
    "In [their paper, McCulloch and Pitts (1943)](https://link.springer.com/article/10.1007/BF02478259) explored how the brain could produce highly complex patterns by using many [interconnected _basic cells (neurons)_](https://en.wikipedia.org/wiki/Biological_neuron_model). McCulloch and Pitts suggested a _highly simplified model_ of a neuron. Nevertheless, they made a foundational contribution to developing artificial neural networks that we find in wide use today. Let's look at the model of a neuron proposed by McCulloch and Pitts.\n",
    "\n",
    "Suppose we have a neuron that takes an input vector $\\mathbf{n}(t) = (n^{(t)}_1, n^{(t)}_2, \\ldots, n^{(t)}_{m})$, where each component $n_k\\in\\mathbf{n}$ is a binary value (`0` or `1`) which represents the state of other predecessor neurons $n_1,n_2,\\ldots,n_m$ at time $t$. Then, the state of our neuron (say neuron $k$) at time $t+1$ is given by:\n",
    "$$\n",
    "\\begin{align*}\n",
    "n_{k}(t+1) &= \\sigma\\left(\\sum_{j=1}^{m} w_{kj} n_j(t) - \\theta_k\\right) \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\sigma:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ is an _activation function_ that maps the weighted sum of a vector of inputs to a scalar (binary) output. In the original paper, the state of neuron $k$ at time $t+1$ denoted as $n_k(t+1)\\in\\{0,1\\}$, where $w_{kj}$ is the weight of the connection between neuron $k$ to the output of (predecessor) neuron $k$, and $\\theta_k$ is the threshold for neuron $k$. \n",
    "* _Activation function_: In this original McCulloch and Pitts model, the activation function $\\sigma$ is a step function, which means that the output of the neuron is `1` if the weighted sum of inputs exceeds the threshold $\\theta_k$, and `0` otherwise. In other words, the neuron \"fires\" (produces an output of `1`) if the total input to the neuron is greater than or equal to the threshold $\\theta_k$. This is a binary output, simplifying real biological neurons that can produce continuous outputs.\n",
    "* _Parameters_: The weights $w_{kj}\\in\\mathbb{R}$ and the threshold $\\theta_k\\in\\mathbb{R}$ are parameters of the neuron that determine its behavior. The weights can be positive or negative, representing the strength and direction of the influence of the input neurons on the output neuron. The threshold determines how much input the neuron needs to \"fire\" (i.e., produce an output of `1`).\n",
    "\n",
    "While the McCulloch-Pitts neuron model simplifies real biological neurons, it laid the groundwork for the development of more complex artificial neural networks. The key idea is that by combining many simple neurons in a network, we can create complex functions and learn to approximate any continuous function. This idea is at the heart of modern deep learning and neural networks. \n",
    "\n",
    "__Hmmmm__. These ideas _really_ seem familiar. Have we seen this before? Yes! the McCulloch-Pitts Neuron underpins [The Perceptron (Rosenblatt, 1957)](https://en.wikipedia.org/wiki/Perceptron), [Hopfield networks](https://en.wikipedia.org/wiki/Hopfield_network) and [Boltzmann machines](https://en.wikipedia.org/wiki/Boltzmann_machine). Wow!!\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ede978",
   "metadata": {},
   "source": [
    "## Activation functions\n",
    "The activation function $\\sigma:\\mathbb{R}\\rightarrow\\mathbb{R}$ of a neuron is a mathematical function that determines the output of the neuron based on its input. \n",
    "\n",
    "The activation function takes the weighted sum of the inputs to the neuron and applies a non-linear transformation to produce the output. The choice of activation function is important because it affects the learning process and the performance of the neural network. There is a [wide variety of activation functions](https://en.wikipedia.org/wiki/Activation_function#Table_of_activation_functions), each with its own characteristics and applications. \n",
    "\n",
    "Some common activation functions are:\n",
    "\n",
    "* __Sigmoid function__: The sigmoid function is a smooth, S-shaped curve that maps input values to the range (0, 1). It is defined as:\n",
    "$ \\sigma(x) = \\frac{1}{1 + e^{-x}}$. The sigmoid function is often used in the output layer of binary classification problems, as it can be interpreted as a probability. \n",
    "* __Hyperbolic tangent function (tanh)__: The $\\texttt{tanh}$ function is similar to the sigmoid function but maps input values to the range (-1, 1). It is defined as: $\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$. It is often used in hidden layers of neural networks.\n",
    "* __Rectified Linear Unit (ReLU)__: The ReLU function is a piecewise linear function that outputs the input value if it is positive and zero otherwise. It is defined as: $\\text{ReLU}(x) = \\max(0, x)$. The $\\texttt{ReLU}$ function helps mitigate the vanishing gradient problem, a complication in training, making it a popular choice for hidden layers in deep networks. However, it can suffer from [the dying ReLU problem](https://arxiv.org/abs/1903.06733), where neurons can become inactive and stop learning if they output zero for all inputs.\n",
    "* __Softmax function__: The $\\texttt{softmax}$ function is often used in the output layer of multi-class classification problems. It converts a vector of raw scores (logits) into a probability distribution over multiple classes. It is defined as: $\\texttt{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}$ where $z_i$ is the raw score for class $i$, and $K$ is the total number of classes. The softmax function ensures that the output probabilities sum to 1, making it suitable for multi-class classification tasks.\n",
    "\n",
    "Let's take a look at the [activation functions exported by the `NNlib.jl` package](https://fluxml.ai/NNlib.jl/dev/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c2dd6a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip600\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip600)\" d=\"M0 1600 L2400 1600 L2400 8.88178e-14 L0 8.88178e-14  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip601\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip600)\" d=\"M219.866 1423.18 L2352.76 1423.18 L2352.76 47.2441 L219.866 47.2441  Z\" fill=\"#f2f2f2\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip602\">\n",
       "    <rect x=\"219\" y=\"47\" width=\"2134\" height=\"1377\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip602)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"280.231,1423.18 280.231,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip602)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"783.271,1423.18 783.271,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip602)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1286.31,1423.18 1286.31,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip602)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1789.35,1423.18 1789.35,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip602)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"2292.39,1423.18 2292.39,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip602)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"280.231,1423.18 280.231,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip602)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"783.271,1423.18 783.271,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip602)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1286.31,1423.18 1286.31,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip602)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1789.35,1423.18 1789.35,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip602)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"2292.39,1423.18 2292.39,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip602)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"219.866,1246.53 2352.76,1246.53 \"/>\n",
       "<polyline clip-path=\"url(#clip602)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"219.866,905.651 2352.76,905.651 \"/>\n",
       "<polyline clip-path=\"url(#clip602)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"219.866,564.773 2352.76,564.773 \"/>\n",
       "<polyline clip-path=\"url(#clip602)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"219.866,223.895 2352.76,223.895 \"/>\n",
       "<polyline clip-path=\"url(#clip602)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"219.866,1246.53 2352.76,1246.53 \"/>\n",
       "<polyline clip-path=\"url(#clip602)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"219.866,905.651 2352.76,905.651 \"/>\n",
       "<polyline clip-path=\"url(#clip602)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"219.866,564.773 2352.76,564.773 \"/>\n",
       "<polyline clip-path=\"url(#clip602)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"219.866,223.895 2352.76,223.895 \"/>\n",
       "<polyline clip-path=\"url(#clip600)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"219.866,1423.18 2352.76,1423.18 \"/>\n",
       "<polyline clip-path=\"url(#clip600)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"219.866,47.2441 2352.76,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip600)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"280.231,1423.18 280.231,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip600)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"783.271,1423.18 783.271,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip600)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1286.31,1423.18 1286.31,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip600)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1789.35,1423.18 1789.35,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip600)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2292.39,1423.18 2292.39,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip600)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"280.231,47.2441 280.231,66.1417 \"/>\n",
       "<polyline clip-path=\"url(#clip600)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"783.271,47.2441 783.271,66.1417 \"/>\n",
       "<polyline clip-path=\"url(#clip600)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1286.31,47.2441 1286.31,66.1417 \"/>\n",
       "<polyline clip-path=\"url(#clip600)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1789.35,47.2441 1789.35,66.1417 \"/>\n",
       "<polyline clip-path=\"url(#clip600)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2292.39,47.2441 2292.39,66.1417 \"/>\n",
       "<path clip-path=\"url(#clip600)\" d=\"M250.173 1468.75 L279.849 1468.75 L279.849 1472.69 L250.173 1472.69 L250.173 1468.75 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip600)\" d=\"M293.969 1481.64 L310.289 1481.64 L310.289 1485.58 L288.344 1485.58 L288.344 1481.64 Q291.006 1478.89 295.59 1474.26 Q300.196 1469.61 301.377 1468.27 Q303.622 1465.74 304.502 1464.01 Q305.405 1462.25 305.405 1460.56 Q305.405 1457.8 303.46 1456.07 Q301.539 1454.33 298.437 1454.33 Q296.238 1454.33 293.784 1455.09 Q291.354 1455.86 288.576 1457.41 L288.576 1452.69 Q291.4 1451.55 293.854 1450.97 Q296.307 1450.39 298.344 1450.39 Q303.715 1450.39 306.909 1453.08 Q310.104 1455.77 310.104 1460.26 Q310.104 1462.39 309.293 1464.31 Q308.506 1466.2 306.4 1468.8 Q305.821 1469.47 302.719 1472.69 Q299.618 1475.88 293.969 1481.64 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip600)\" d=\"M753.028 1468.75 L782.704 1468.75 L782.704 1472.69 L753.028 1472.69 L753.028 1468.75 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip600)\" d=\"M793.607 1481.64 L801.246 1481.64 L801.246 1455.28 L792.935 1456.95 L792.935 1452.69 L801.199 1451.02 L805.875 1451.02 L805.875 1481.64 L813.514 1481.64 L813.514 1485.58 L793.607 1485.58 L793.607 1481.64 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip600)\" d=\"M1286.31 1454.1 Q1282.7 1454.1 1280.87 1457.66 Q1279.07 1461.2 1279.07 1468.33 Q1279.07 1475.44 1280.87 1479.01 Q1282.7 1482.55 1286.31 1482.55 Q1289.95 1482.55 1291.75 1479.01 Q1293.58 1475.44 1293.58 1468.33 Q1293.58 1461.2 1291.75 1457.66 Q1289.95 1454.1 1286.31 1454.1 M1286.31 1450.39 Q1292.12 1450.39 1295.18 1455 Q1298.26 1459.58 1298.26 1468.33 Q1298.26 1477.06 1295.18 1481.67 Q1292.12 1486.25 1286.31 1486.25 Q1280.5 1486.25 1277.42 1481.67 Q1274.37 1477.06 1274.37 1468.33 Q1274.37 1459.58 1277.42 1455 Q1280.5 1450.39 1286.31 1450.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip600)\" d=\"M1779.73 1481.64 L1787.37 1481.64 L1787.37 1455.28 L1779.06 1456.95 L1779.06 1452.69 L1787.33 1451.02 L1792 1451.02 L1792 1481.64 L1799.64 1481.64 L1799.64 1485.58 L1779.73 1485.58 L1779.73 1481.64 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip600)\" d=\"M2287.04 1481.64 L2303.36 1481.64 L2303.36 1485.58 L2281.42 1485.58 L2281.42 1481.64 Q2284.08 1478.89 2288.66 1474.26 Q2293.27 1469.61 2294.45 1468.27 Q2296.7 1465.74 2297.58 1464.01 Q2298.48 1462.25 2298.48 1460.56 Q2298.48 1457.8 2296.53 1456.07 Q2294.61 1454.33 2291.51 1454.33 Q2289.31 1454.33 2286.86 1455.09 Q2284.43 1455.86 2281.65 1457.41 L2281.65 1452.69 Q2284.47 1451.55 2286.93 1450.97 Q2289.38 1450.39 2291.42 1450.39 Q2296.79 1450.39 2299.98 1453.08 Q2303.18 1455.77 2303.18 1460.26 Q2303.18 1462.39 2302.37 1464.31 Q2301.58 1466.2 2299.47 1468.8 Q2298.9 1469.47 2295.79 1472.69 Q2292.69 1475.88 2287.04 1481.64 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip600)\" d=\"M1302.93 1532.4 L1290.04 1549.74 L1303.59 1568.04 L1296.69 1568.04 L1286.31 1554.04 L1275.93 1568.04 L1269.03 1568.04 L1282.87 1549.39 L1270.21 1532.4 L1277.11 1532.4 L1286.57 1545.1 L1296.02 1532.4 L1302.93 1532.4 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip600)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"219.866,1423.18 219.866,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip600)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2352.76,1423.18 2352.76,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip600)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"219.866,1246.53 238.764,1246.53 \"/>\n",
       "<polyline clip-path=\"url(#clip600)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"219.866,905.651 238.764,905.651 \"/>\n",
       "<polyline clip-path=\"url(#clip600)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"219.866,564.773 238.764,564.773 \"/>\n",
       "<polyline clip-path=\"url(#clip600)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"219.866,223.895 238.764,223.895 \"/>\n",
       "<polyline clip-path=\"url(#clip600)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2352.76,1246.53 2333.86,1246.53 \"/>\n",
       "<polyline clip-path=\"url(#clip600)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2352.76,905.651 2333.86,905.651 \"/>\n",
       "<polyline clip-path=\"url(#clip600)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2352.76,564.773 2333.86,564.773 \"/>\n",
       "<polyline clip-path=\"url(#clip600)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2352.76,223.895 2333.86,223.895 \"/>\n",
       "<path clip-path=\"url(#clip600)\" d=\"M128.288 1232.33 Q124.677 1232.33 122.848 1235.89 Q121.043 1239.43 121.043 1246.56 Q121.043 1253.67 122.848 1257.23 Q124.677 1260.78 128.288 1260.78 Q131.922 1260.78 133.728 1257.23 Q135.556 1253.67 135.556 1246.56 Q135.556 1239.43 133.728 1235.89 Q131.922 1232.33 128.288 1232.33 M128.288 1228.62 Q134.098 1228.62 137.154 1233.23 Q140.232 1237.81 140.232 1246.56 Q140.232 1255.29 137.154 1259.9 Q134.098 1264.48 128.288 1264.48 Q122.478 1264.48 119.399 1259.9 Q116.343 1255.29 116.343 1246.56 Q116.343 1237.81 119.399 1233.23 Q122.478 1228.62 128.288 1228.62 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip600)\" d=\"M148.45 1257.93 L153.334 1257.93 L153.334 1263.81 L148.45 1263.81 L148.45 1257.93 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip600)\" d=\"M167.547 1259.87 L183.866 1259.87 L183.866 1263.81 L161.922 1263.81 L161.922 1259.87 Q164.584 1257.12 169.167 1252.49 Q173.774 1247.84 174.954 1246.49 Q177.2 1243.97 178.079 1242.23 Q178.982 1240.48 178.982 1238.79 Q178.982 1236.03 177.038 1234.29 Q175.116 1232.56 172.014 1232.56 Q169.815 1232.56 167.362 1233.32 Q164.931 1234.09 162.153 1235.64 L162.153 1230.92 Q164.977 1229.78 167.431 1229.2 Q169.885 1228.62 171.922 1228.62 Q177.292 1228.62 180.487 1231.31 Q183.681 1233.99 183.681 1238.48 Q183.681 1240.61 182.871 1242.54 Q182.084 1244.43 179.977 1247.03 Q179.399 1247.7 176.297 1250.92 Q173.195 1254.11 167.547 1259.87 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip600)\" d=\"M126.205 891.45 Q122.593 891.45 120.765 895.014 Q118.959 898.556 118.959 905.686 Q118.959 912.792 120.765 916.357 Q122.593 919.898 126.205 919.898 Q129.839 919.898 131.644 916.357 Q133.473 912.792 133.473 905.686 Q133.473 898.556 131.644 895.014 Q129.839 891.45 126.205 891.45 M126.205 887.746 Q132.015 887.746 135.07 892.352 Q138.149 896.936 138.149 905.686 Q138.149 914.412 135.07 919.019 Q132.015 923.602 126.205 923.602 Q120.394 923.602 117.316 919.019 Q114.26 914.412 114.26 905.686 Q114.26 896.936 117.316 892.352 Q120.394 887.746 126.205 887.746 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip600)\" d=\"M146.366 917.051 L151.251 917.051 L151.251 922.931 L146.366 922.931 L146.366 917.051 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip600)\" d=\"M174.283 892.445 L162.477 910.894 L174.283 910.894 L174.283 892.445 M173.056 888.371 L178.936 888.371 L178.936 910.894 L183.866 910.894 L183.866 914.783 L178.936 914.783 L178.936 922.931 L174.283 922.931 L174.283 914.783 L158.681 914.783 L158.681 910.269 L173.056 888.371 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip600)\" d=\"M126.529 550.572 Q122.918 550.572 121.089 554.137 Q119.283 557.678 119.283 564.808 Q119.283 571.914 121.089 575.479 Q122.918 579.021 126.529 579.021 Q130.163 579.021 131.968 575.479 Q133.797 571.914 133.797 564.808 Q133.797 557.678 131.968 554.137 Q130.163 550.572 126.529 550.572 M126.529 546.868 Q132.339 546.868 135.394 551.474 Q138.473 556.058 138.473 564.808 Q138.473 573.535 135.394 578.141 Q132.339 582.724 126.529 582.724 Q120.718 582.724 117.64 578.141 Q114.584 573.535 114.584 564.808 Q114.584 556.058 117.64 551.474 Q120.718 546.868 126.529 546.868 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip600)\" d=\"M146.691 576.173 L151.575 576.173 L151.575 582.053 L146.691 582.053 L146.691 576.173 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip600)\" d=\"M172.339 562.91 Q169.19 562.91 167.339 565.062 Q165.51 567.215 165.51 570.965 Q165.51 574.692 167.339 576.868 Q169.19 579.021 172.339 579.021 Q175.487 579.021 177.315 576.868 Q179.167 574.692 179.167 570.965 Q179.167 567.215 177.315 565.062 Q175.487 562.91 172.339 562.91 M181.621 548.257 L181.621 552.516 Q179.862 551.683 178.056 551.243 Q176.274 550.803 174.514 550.803 Q169.885 550.803 167.431 553.928 Q165.001 557.053 164.653 563.373 Q166.019 561.359 168.079 560.294 Q170.139 559.206 172.616 559.206 Q177.825 559.206 180.834 562.377 Q183.866 565.525 183.866 570.965 Q183.866 576.289 180.718 579.507 Q177.57 582.724 172.339 582.724 Q166.343 582.724 163.172 578.141 Q160.001 573.535 160.001 564.808 Q160.001 556.613 163.89 551.752 Q167.778 546.868 174.329 546.868 Q176.089 546.868 177.871 547.215 Q179.676 547.562 181.621 548.257 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip600)\" d=\"M126.783 209.694 Q123.172 209.694 121.343 213.259 Q119.538 216.8 119.538 223.93 Q119.538 231.036 121.343 234.601 Q123.172 238.143 126.783 238.143 Q130.417 238.143 132.223 234.601 Q134.052 231.036 134.052 223.93 Q134.052 216.8 132.223 213.259 Q130.417 209.694 126.783 209.694 M126.783 205.99 Q132.593 205.99 135.649 210.597 Q138.728 215.18 138.728 223.93 Q138.728 232.657 135.649 237.263 Q132.593 241.847 126.783 241.847 Q120.973 241.847 117.894 237.263 Q114.839 232.657 114.839 223.93 Q114.839 215.18 117.894 210.597 Q120.973 205.99 126.783 205.99 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip600)\" d=\"M146.945 235.296 L151.829 235.296 L151.829 241.175 L146.945 241.175 L146.945 235.296 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip600)\" d=\"M172.014 224.763 Q168.681 224.763 166.76 226.546 Q164.862 228.328 164.862 231.453 Q164.862 234.578 166.76 236.36 Q168.681 238.143 172.014 238.143 Q175.348 238.143 177.269 236.36 Q179.19 234.555 179.19 231.453 Q179.19 228.328 177.269 226.546 Q175.371 224.763 172.014 224.763 M167.339 222.773 Q164.329 222.032 162.64 219.972 Q160.973 217.911 160.973 214.949 Q160.973 210.805 163.913 208.398 Q166.876 205.99 172.014 205.99 Q177.176 205.99 180.116 208.398 Q183.056 210.805 183.056 214.949 Q183.056 217.911 181.366 219.972 Q179.7 222.032 176.714 222.773 Q180.093 223.56 181.968 225.851 Q183.866 228.143 183.866 231.453 Q183.866 236.476 180.788 239.161 Q177.732 241.847 172.014 241.847 Q166.297 241.847 163.218 239.161 Q160.163 236.476 160.163 231.453 Q160.163 228.143 162.061 225.851 Q163.959 223.56 167.339 222.773 M165.626 215.388 Q165.626 218.074 167.292 219.578 Q168.982 221.083 172.014 221.083 Q175.024 221.083 176.714 219.578 Q178.426 218.074 178.426 215.388 Q178.426 212.703 176.714 211.199 Q175.024 209.694 172.014 209.694 Q168.982 209.694 167.292 211.199 Q165.626 212.703 165.626 215.388 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip600)\" d=\"M33.0032 779.629 Q33.0032 784.467 36.5043 787.077 Q40.1646 789.814 46.212 789.814 Q52.6095 789.814 56.3017 787.108 Q59.9619 784.371 59.9619 779.629 Q59.9619 774.95 56.2698 772.213 Q52.5777 769.475 46.212 769.475 Q40.3874 769.475 36.5043 772.213 Q33.0032 774.727 33.0032 779.629 M28.3562 779.629 L28.3562 760.181 L34.2127 760.181 L34.2127 766.738 Q39.1779 763.269 46.212 763.269 Q54.9649 763.269 59.9301 767.629 Q64.9272 771.99 64.9272 779.629 Q64.9272 787.299 59.9301 791.628 Q54.9649 795.989 46.212 795.989 Q37.3955 795.989 32.4621 791.628 Q28.3562 788.031 28.3562 779.629 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip600)\" d=\"M14.5426 738.061 Q21.8632 742.326 29.0246 744.395 Q36.186 746.463 43.5384 746.463 Q50.8908 746.463 58.1159 744.395 Q65.3091 742.294 72.5979 738.061 L72.5979 743.153 Q65.1182 747.927 57.8931 750.315 Q50.668 752.67 43.5384 752.67 Q36.4406 752.67 29.2474 750.315 Q22.0542 747.959 14.5426 743.153 L14.5426 738.061 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip600)\" d=\"M28.3562 697.065 L45.7028 709.956 L64.0042 696.397 L64.0042 703.304 L49.9996 713.68 L64.0042 724.056 L64.0042 730.963 L45.3526 717.117 L28.3562 729.785 L28.3562 722.878 L41.0558 713.425 L28.3562 703.972 L28.3562 697.065 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip600)\" d=\"M14.5426 689.045 L14.5426 683.952 Q22.0542 679.178 29.2474 676.823 Q36.4406 674.435 43.5384 674.435 Q50.668 674.435 57.8931 676.823 Q65.1182 679.178 72.5979 683.952 L72.5979 689.045 Q65.3091 684.811 58.1159 682.743 Q50.8908 680.642 43.5384 680.642 Q36.186 680.642 29.0246 682.743 Q21.8632 684.811 14.5426 689.045 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip602)\" style=\"stroke:#000080; stroke-linecap:round; stroke-linejoin:round; stroke-width:12; stroke-opacity:1; fill:none\" points=\"280.231,1384.24 300.556,1376.9 320.881,1369.33 341.206,1361.53 361.53,1353.49 381.855,1345.22 402.18,1336.7 422.505,1327.94 442.83,1318.92 463.155,1309.66 483.48,1300.14 503.804,1290.36 524.129,1280.32 544.454,1270.01 564.779,1259.44 585.104,1248.61 605.429,1237.51 625.753,1226.14 646.078,1214.5 666.403,1202.6 686.728,1190.43 707.053,1177.99 727.378,1165.29 747.703,1152.33 768.027,1139.11 788.352,1125.63 808.677,1111.91 829.002,1097.93 849.327,1083.71 869.652,1069.26 889.977,1054.57 910.301,1039.66 930.626,1024.54 950.951,1009.2 971.276,993.668 991.601,977.941 1011.93,962.033 1032.25,945.952 1052.58,929.71 1072.9,913.317 1093.23,896.786 1113.55,880.127 1133.87,863.353 1154.2,846.477 1174.52,829.512 1194.85,812.471 1215.17,795.368 1235.5,778.216 1255.82,761.028 1276.15,743.82 1296.47,726.604 1316.8,709.396 1337.12,692.208 1357.45,675.056 1377.77,657.952 1398.1,640.911 1418.42,623.946 1438.75,607.071 1459.07,590.297 1479.4,573.638 1499.72,557.107 1520.05,540.714 1540.37,524.472 1560.7,508.391 1581.02,492.483 1601.35,476.756 1621.67,461.221 1642,445.886 1662.32,430.76 1682.65,415.851 1702.97,401.166 1723.3,386.711 1743.62,372.494 1763.95,358.519 1784.27,344.791 1804.59,331.315 1824.92,318.094 1845.24,305.133 1865.57,292.433 1885.89,279.997 1906.22,267.826 1926.54,255.922 1946.87,244.285 1967.19,232.916 1987.52,221.815 2007.84,210.981 2028.17,200.412 2048.49,190.108 2068.82,180.067 2089.14,170.287 2109.47,160.765 2129.79,151.5 2150.12,142.486 2170.44,133.723 2190.77,125.206 2211.09,116.932 2231.42,108.897 2251.74,101.097 2272.07,93.528 2292.39,86.1857 \"/>\n",
       "</svg>\n"
      ],
      "text/html": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip650\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip650)\" d=\"M0 1600 L2400 1600 L2400 8.88178e-14 L0 8.88178e-14  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip651\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip650)\" d=\"M219.866 1423.18 L2352.76 1423.18 L2352.76 47.2441 L219.866 47.2441  Z\" fill=\"#f2f2f2\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip652\">\n",
       "    <rect x=\"219\" y=\"47\" width=\"2134\" height=\"1377\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip652)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"280.231,1423.18 280.231,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip652)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"783.271,1423.18 783.271,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip652)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1286.31,1423.18 1286.31,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip652)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1789.35,1423.18 1789.35,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip652)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"2292.39,1423.18 2292.39,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip652)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"280.231,1423.18 280.231,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip652)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"783.271,1423.18 783.271,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip652)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1286.31,1423.18 1286.31,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip652)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1789.35,1423.18 1789.35,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip652)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"2292.39,1423.18 2292.39,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip652)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"219.866,1246.53 2352.76,1246.53 \"/>\n",
       "<polyline clip-path=\"url(#clip652)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"219.866,905.651 2352.76,905.651 \"/>\n",
       "<polyline clip-path=\"url(#clip652)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"219.866,564.773 2352.76,564.773 \"/>\n",
       "<polyline clip-path=\"url(#clip652)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"219.866,223.895 2352.76,223.895 \"/>\n",
       "<polyline clip-path=\"url(#clip652)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"219.866,1246.53 2352.76,1246.53 \"/>\n",
       "<polyline clip-path=\"url(#clip652)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"219.866,905.651 2352.76,905.651 \"/>\n",
       "<polyline clip-path=\"url(#clip652)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"219.866,564.773 2352.76,564.773 \"/>\n",
       "<polyline clip-path=\"url(#clip652)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"219.866,223.895 2352.76,223.895 \"/>\n",
       "<polyline clip-path=\"url(#clip650)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"219.866,1423.18 2352.76,1423.18 \"/>\n",
       "<polyline clip-path=\"url(#clip650)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"219.866,47.2441 2352.76,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip650)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"280.231,1423.18 280.231,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip650)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"783.271,1423.18 783.271,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip650)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1286.31,1423.18 1286.31,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip650)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1789.35,1423.18 1789.35,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip650)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2292.39,1423.18 2292.39,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip650)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"280.231,47.2441 280.231,66.1417 \"/>\n",
       "<polyline clip-path=\"url(#clip650)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"783.271,47.2441 783.271,66.1417 \"/>\n",
       "<polyline clip-path=\"url(#clip650)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1286.31,47.2441 1286.31,66.1417 \"/>\n",
       "<polyline clip-path=\"url(#clip650)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1789.35,47.2441 1789.35,66.1417 \"/>\n",
       "<polyline clip-path=\"url(#clip650)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2292.39,47.2441 2292.39,66.1417 \"/>\n",
       "<path clip-path=\"url(#clip650)\" d=\"M250.173 1468.75 L279.849 1468.75 L279.849 1472.69 L250.173 1472.69 L250.173 1468.75 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip650)\" d=\"M293.969 1481.64 L310.289 1481.64 L310.289 1485.58 L288.344 1485.58 L288.344 1481.64 Q291.006 1478.89 295.59 1474.26 Q300.196 1469.61 301.377 1468.27 Q303.622 1465.74 304.502 1464.01 Q305.405 1462.25 305.405 1460.56 Q305.405 1457.8 303.46 1456.07 Q301.539 1454.33 298.437 1454.33 Q296.238 1454.33 293.784 1455.09 Q291.354 1455.86 288.576 1457.41 L288.576 1452.69 Q291.4 1451.55 293.854 1450.97 Q296.307 1450.39 298.344 1450.39 Q303.715 1450.39 306.909 1453.08 Q310.104 1455.77 310.104 1460.26 Q310.104 1462.39 309.293 1464.31 Q308.506 1466.2 306.4 1468.8 Q305.821 1469.47 302.719 1472.69 Q299.618 1475.88 293.969 1481.64 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip650)\" d=\"M753.028 1468.75 L782.704 1468.75 L782.704 1472.69 L753.028 1472.69 L753.028 1468.75 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip650)\" d=\"M793.607 1481.64 L801.246 1481.64 L801.246 1455.28 L792.935 1456.95 L792.935 1452.69 L801.199 1451.02 L805.875 1451.02 L805.875 1481.64 L813.514 1481.64 L813.514 1485.58 L793.607 1485.58 L793.607 1481.64 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip650)\" d=\"M1286.31 1454.1 Q1282.7 1454.1 1280.87 1457.66 Q1279.07 1461.2 1279.07 1468.33 Q1279.07 1475.44 1280.87 1479.01 Q1282.7 1482.55 1286.31 1482.55 Q1289.95 1482.55 1291.75 1479.01 Q1293.58 1475.44 1293.58 1468.33 Q1293.58 1461.2 1291.75 1457.66 Q1289.95 1454.1 1286.31 1454.1 M1286.31 1450.39 Q1292.12 1450.39 1295.18 1455 Q1298.26 1459.58 1298.26 1468.33 Q1298.26 1477.06 1295.18 1481.67 Q1292.12 1486.25 1286.31 1486.25 Q1280.5 1486.25 1277.42 1481.67 Q1274.37 1477.06 1274.37 1468.33 Q1274.37 1459.58 1277.42 1455 Q1280.5 1450.39 1286.31 1450.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip650)\" d=\"M1779.73 1481.64 L1787.37 1481.64 L1787.37 1455.28 L1779.06 1456.95 L1779.06 1452.69 L1787.33 1451.02 L1792 1451.02 L1792 1481.64 L1799.64 1481.64 L1799.64 1485.58 L1779.73 1485.58 L1779.73 1481.64 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip650)\" d=\"M2287.04 1481.64 L2303.36 1481.64 L2303.36 1485.58 L2281.42 1485.58 L2281.42 1481.64 Q2284.08 1478.89 2288.66 1474.26 Q2293.27 1469.61 2294.45 1468.27 Q2296.7 1465.74 2297.58 1464.01 Q2298.48 1462.25 2298.48 1460.56 Q2298.48 1457.8 2296.53 1456.07 Q2294.61 1454.33 2291.51 1454.33 Q2289.31 1454.33 2286.86 1455.09 Q2284.43 1455.86 2281.65 1457.41 L2281.65 1452.69 Q2284.47 1451.55 2286.93 1450.97 Q2289.38 1450.39 2291.42 1450.39 Q2296.79 1450.39 2299.98 1453.08 Q2303.18 1455.77 2303.18 1460.26 Q2303.18 1462.39 2302.37 1464.31 Q2301.58 1466.2 2299.47 1468.8 Q2298.9 1469.47 2295.79 1472.69 Q2292.69 1475.88 2287.04 1481.64 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip650)\" d=\"M1302.93 1532.4 L1290.04 1549.74 L1303.59 1568.04 L1296.69 1568.04 L1286.31 1554.04 L1275.93 1568.04 L1269.03 1568.04 L1282.87 1549.39 L1270.21 1532.4 L1277.11 1532.4 L1286.57 1545.1 L1296.02 1532.4 L1302.93 1532.4 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip650)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"219.866,1423.18 219.866,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip650)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2352.76,1423.18 2352.76,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip650)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"219.866,1246.53 238.764,1246.53 \"/>\n",
       "<polyline clip-path=\"url(#clip650)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"219.866,905.651 238.764,905.651 \"/>\n",
       "<polyline clip-path=\"url(#clip650)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"219.866,564.773 238.764,564.773 \"/>\n",
       "<polyline clip-path=\"url(#clip650)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"219.866,223.895 238.764,223.895 \"/>\n",
       "<polyline clip-path=\"url(#clip650)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2352.76,1246.53 2333.86,1246.53 \"/>\n",
       "<polyline clip-path=\"url(#clip650)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2352.76,905.651 2333.86,905.651 \"/>\n",
       "<polyline clip-path=\"url(#clip650)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2352.76,564.773 2333.86,564.773 \"/>\n",
       "<polyline clip-path=\"url(#clip650)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2352.76,223.895 2333.86,223.895 \"/>\n",
       "<path clip-path=\"url(#clip650)\" d=\"M128.288 1232.33 Q124.677 1232.33 122.848 1235.89 Q121.043 1239.43 121.043 1246.56 Q121.043 1253.67 122.848 1257.23 Q124.677 1260.78 128.288 1260.78 Q131.922 1260.78 133.728 1257.23 Q135.556 1253.67 135.556 1246.56 Q135.556 1239.43 133.728 1235.89 Q131.922 1232.33 128.288 1232.33 M128.288 1228.62 Q134.098 1228.62 137.154 1233.23 Q140.232 1237.81 140.232 1246.56 Q140.232 1255.29 137.154 1259.9 Q134.098 1264.48 128.288 1264.48 Q122.478 1264.48 119.399 1259.9 Q116.343 1255.29 116.343 1246.56 Q116.343 1237.81 119.399 1233.23 Q122.478 1228.62 128.288 1228.62 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip650)\" d=\"M148.45 1257.93 L153.334 1257.93 L153.334 1263.81 L148.45 1263.81 L148.45 1257.93 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip650)\" d=\"M167.547 1259.87 L183.866 1259.87 L183.866 1263.81 L161.922 1263.81 L161.922 1259.87 Q164.584 1257.12 169.167 1252.49 Q173.774 1247.84 174.954 1246.49 Q177.2 1243.97 178.079 1242.23 Q178.982 1240.48 178.982 1238.79 Q178.982 1236.03 177.038 1234.29 Q175.116 1232.56 172.014 1232.56 Q169.815 1232.56 167.362 1233.32 Q164.931 1234.09 162.153 1235.64 L162.153 1230.92 Q164.977 1229.78 167.431 1229.2 Q169.885 1228.62 171.922 1228.62 Q177.292 1228.62 180.487 1231.31 Q183.681 1233.99 183.681 1238.48 Q183.681 1240.61 182.871 1242.54 Q182.084 1244.43 179.977 1247.03 Q179.399 1247.7 176.297 1250.92 Q173.195 1254.11 167.547 1259.87 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip650)\" d=\"M126.205 891.45 Q122.593 891.45 120.765 895.014 Q118.959 898.556 118.959 905.686 Q118.959 912.792 120.765 916.357 Q122.593 919.898 126.205 919.898 Q129.839 919.898 131.644 916.357 Q133.473 912.792 133.473 905.686 Q133.473 898.556 131.644 895.014 Q129.839 891.45 126.205 891.45 M126.205 887.746 Q132.015 887.746 135.07 892.352 Q138.149 896.936 138.149 905.686 Q138.149 914.412 135.07 919.019 Q132.015 923.602 126.205 923.602 Q120.394 923.602 117.316 919.019 Q114.26 914.412 114.26 905.686 Q114.26 896.936 117.316 892.352 Q120.394 887.746 126.205 887.746 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip650)\" d=\"M146.366 917.051 L151.251 917.051 L151.251 922.931 L146.366 922.931 L146.366 917.051 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip650)\" d=\"M174.283 892.445 L162.477 910.894 L174.283 910.894 L174.283 892.445 M173.056 888.371 L178.936 888.371 L178.936 910.894 L183.866 910.894 L183.866 914.783 L178.936 914.783 L178.936 922.931 L174.283 922.931 L174.283 914.783 L158.681 914.783 L158.681 910.269 L173.056 888.371 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip650)\" d=\"M126.529 550.572 Q122.918 550.572 121.089 554.137 Q119.283 557.678 119.283 564.808 Q119.283 571.914 121.089 575.479 Q122.918 579.021 126.529 579.021 Q130.163 579.021 131.968 575.479 Q133.797 571.914 133.797 564.808 Q133.797 557.678 131.968 554.137 Q130.163 550.572 126.529 550.572 M126.529 546.868 Q132.339 546.868 135.394 551.474 Q138.473 556.058 138.473 564.808 Q138.473 573.535 135.394 578.141 Q132.339 582.724 126.529 582.724 Q120.718 582.724 117.64 578.141 Q114.584 573.535 114.584 564.808 Q114.584 556.058 117.64 551.474 Q120.718 546.868 126.529 546.868 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip650)\" d=\"M146.691 576.173 L151.575 576.173 L151.575 582.053 L146.691 582.053 L146.691 576.173 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip650)\" d=\"M172.339 562.91 Q169.19 562.91 167.339 565.062 Q165.51 567.215 165.51 570.965 Q165.51 574.692 167.339 576.868 Q169.19 579.021 172.339 579.021 Q175.487 579.021 177.315 576.868 Q179.167 574.692 179.167 570.965 Q179.167 567.215 177.315 565.062 Q175.487 562.91 172.339 562.91 M181.621 548.257 L181.621 552.516 Q179.862 551.683 178.056 551.243 Q176.274 550.803 174.514 550.803 Q169.885 550.803 167.431 553.928 Q165.001 557.053 164.653 563.373 Q166.019 561.359 168.079 560.294 Q170.139 559.206 172.616 559.206 Q177.825 559.206 180.834 562.377 Q183.866 565.525 183.866 570.965 Q183.866 576.289 180.718 579.507 Q177.57 582.724 172.339 582.724 Q166.343 582.724 163.172 578.141 Q160.001 573.535 160.001 564.808 Q160.001 556.613 163.89 551.752 Q167.778 546.868 174.329 546.868 Q176.089 546.868 177.871 547.215 Q179.676 547.562 181.621 548.257 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip650)\" d=\"M126.783 209.694 Q123.172 209.694 121.343 213.259 Q119.538 216.8 119.538 223.93 Q119.538 231.036 121.343 234.601 Q123.172 238.143 126.783 238.143 Q130.417 238.143 132.223 234.601 Q134.052 231.036 134.052 223.93 Q134.052 216.8 132.223 213.259 Q130.417 209.694 126.783 209.694 M126.783 205.99 Q132.593 205.99 135.649 210.597 Q138.728 215.18 138.728 223.93 Q138.728 232.657 135.649 237.263 Q132.593 241.847 126.783 241.847 Q120.973 241.847 117.894 237.263 Q114.839 232.657 114.839 223.93 Q114.839 215.18 117.894 210.597 Q120.973 205.99 126.783 205.99 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip650)\" d=\"M146.945 235.296 L151.829 235.296 L151.829 241.175 L146.945 241.175 L146.945 235.296 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip650)\" d=\"M172.014 224.763 Q168.681 224.763 166.76 226.546 Q164.862 228.328 164.862 231.453 Q164.862 234.578 166.76 236.36 Q168.681 238.143 172.014 238.143 Q175.348 238.143 177.269 236.36 Q179.19 234.555 179.19 231.453 Q179.19 228.328 177.269 226.546 Q175.371 224.763 172.014 224.763 M167.339 222.773 Q164.329 222.032 162.64 219.972 Q160.973 217.911 160.973 214.949 Q160.973 210.805 163.913 208.398 Q166.876 205.99 172.014 205.99 Q177.176 205.99 180.116 208.398 Q183.056 210.805 183.056 214.949 Q183.056 217.911 181.366 219.972 Q179.7 222.032 176.714 222.773 Q180.093 223.56 181.968 225.851 Q183.866 228.143 183.866 231.453 Q183.866 236.476 180.788 239.161 Q177.732 241.847 172.014 241.847 Q166.297 241.847 163.218 239.161 Q160.163 236.476 160.163 231.453 Q160.163 228.143 162.061 225.851 Q163.959 223.56 167.339 222.773 M165.626 215.388 Q165.626 218.074 167.292 219.578 Q168.982 221.083 172.014 221.083 Q175.024 221.083 176.714 219.578 Q178.426 218.074 178.426 215.388 Q178.426 212.703 176.714 211.199 Q175.024 209.694 172.014 209.694 Q168.982 209.694 167.292 211.199 Q165.626 212.703 165.626 215.388 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip650)\" d=\"M33.0032 779.629 Q33.0032 784.467 36.5043 787.077 Q40.1646 789.814 46.212 789.814 Q52.6095 789.814 56.3017 787.108 Q59.9619 784.371 59.9619 779.629 Q59.9619 774.95 56.2698 772.213 Q52.5777 769.475 46.212 769.475 Q40.3874 769.475 36.5043 772.213 Q33.0032 774.727 33.0032 779.629 M28.3562 779.629 L28.3562 760.181 L34.2127 760.181 L34.2127 766.738 Q39.1779 763.269 46.212 763.269 Q54.9649 763.269 59.9301 767.629 Q64.9272 771.99 64.9272 779.629 Q64.9272 787.299 59.9301 791.628 Q54.9649 795.989 46.212 795.989 Q37.3955 795.989 32.4621 791.628 Q28.3562 788.031 28.3562 779.629 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip650)\" d=\"M14.5426 738.061 Q21.8632 742.326 29.0246 744.395 Q36.186 746.463 43.5384 746.463 Q50.8908 746.463 58.1159 744.395 Q65.3091 742.294 72.5979 738.061 L72.5979 743.153 Q65.1182 747.927 57.8931 750.315 Q50.668 752.67 43.5384 752.67 Q36.4406 752.67 29.2474 750.315 Q22.0542 747.959 14.5426 743.153 L14.5426 738.061 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip650)\" d=\"M28.3562 697.065 L45.7028 709.956 L64.0042 696.397 L64.0042 703.304 L49.9996 713.68 L64.0042 724.056 L64.0042 730.963 L45.3526 717.117 L28.3562 729.785 L28.3562 722.878 L41.0558 713.425 L28.3562 703.972 L28.3562 697.065 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip650)\" d=\"M14.5426 689.045 L14.5426 683.952 Q22.0542 679.178 29.2474 676.823 Q36.4406 674.435 43.5384 674.435 Q50.668 674.435 57.8931 676.823 Q65.1182 679.178 72.5979 683.952 L72.5979 689.045 Q65.3091 684.811 58.1159 682.743 Q50.8908 680.642 43.5384 680.642 Q36.186 680.642 29.0246 682.743 Q21.8632 684.811 14.5426 689.045 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip652)\" style=\"stroke:#000080; stroke-linecap:round; stroke-linejoin:round; stroke-width:12; stroke-opacity:1; fill:none\" points=\"280.231,1384.24 300.556,1376.9 320.881,1369.33 341.206,1361.53 361.53,1353.49 381.855,1345.22 402.18,1336.7 422.505,1327.94 442.83,1318.92 463.155,1309.66 483.48,1300.14 503.804,1290.36 524.129,1280.32 544.454,1270.01 564.779,1259.44 585.104,1248.61 605.429,1237.51 625.753,1226.14 646.078,1214.5 666.403,1202.6 686.728,1190.43 707.053,1177.99 727.378,1165.29 747.703,1152.33 768.027,1139.11 788.352,1125.63 808.677,1111.91 829.002,1097.93 849.327,1083.71 869.652,1069.26 889.977,1054.57 910.301,1039.66 930.626,1024.54 950.951,1009.2 971.276,993.668 991.601,977.941 1011.93,962.033 1032.25,945.952 1052.58,929.71 1072.9,913.317 1093.23,896.786 1113.55,880.127 1133.87,863.353 1154.2,846.477 1174.52,829.512 1194.85,812.471 1215.17,795.368 1235.5,778.216 1255.82,761.028 1276.15,743.82 1296.47,726.604 1316.8,709.396 1337.12,692.208 1357.45,675.056 1377.77,657.952 1398.1,640.911 1418.42,623.946 1438.75,607.071 1459.07,590.297 1479.4,573.638 1499.72,557.107 1520.05,540.714 1540.37,524.472 1560.7,508.391 1581.02,492.483 1601.35,476.756 1621.67,461.221 1642,445.886 1662.32,430.76 1682.65,415.851 1702.97,401.166 1723.3,386.711 1743.62,372.494 1763.95,358.519 1784.27,344.791 1804.59,331.315 1824.92,318.094 1845.24,305.133 1865.57,292.433 1885.89,279.997 1906.22,267.826 1926.54,255.922 1946.87,244.285 1967.19,232.916 1987.52,221.815 2007.84,210.981 2028.17,200.412 2048.49,190.108 2068.82,180.067 2089.14,170.287 2109.47,160.765 2129.79,151.5 2150.12,142.486 2170.44,133.723 2190.77,125.206 2211.09,116.932 2231.42,108.897 2251.74,101.097 2272.07,93.528 2292.39,86.1857 \"/>\n",
       "</svg>\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "let\n",
    "\n",
    "    # initialize -\n",
    "    x = range(-2, 2, length=100) |> collect .|> Float32; # notice: .|> Float32. What?\n",
    "\n",
    "    # what activation function to use? We'll use the NNlib package for this.\n",
    "    y = x |> NNlib.sigmoid_fast |> collect; # \n",
    "\n",
    "    # plot -\n",
    "    p = plot(bg=\"gray95\", background_color_outside=\"white\", framestyle = :box, fg_legend = :transparent); \n",
    "    plot!(x, y, xlabel=\"x\", ylabel=\"σ(x)\", legend=:topright, label=\"\", lw=3, c=:navy)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1369fabd",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff9f079",
   "metadata": {},
   "source": [
    "<img\n",
    "  src=\"figs/nn-4.svg\"\n",
    "  alt=\"triangle with all three sides equal\"\n",
    "  height=\"400\"\n",
    "  width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e2828e",
   "metadata": {},
   "source": [
    "## Feedforward Neural Networks\n",
    "Let's consider the simple network shown above. The network has three(ish) layers: an input layer (five nodes), a hidden layer (12 nodes), and an output layer (three nodes). \n",
    "\n",
    "* __Hidden Layer__: The hidden layer performs computations on the input data. Each node in the hidden layer takes the input vector $\\mathbf{x}$ and applies a linear transformation followed by a non-linear activation function. The output of each node in the hidden layer is then passed to the next layer (in this case, the output layer). The number of nodes in the hidden layer can vary depending on the complexity of the task. Furthermore, the hidden layer can have multiple layers (i.e., deep neural networks) to learn more complex data representations. The example network has a single hidden layer with 12 nodes.\n",
    "* __Output Layer__: The output layer is the final layer of the network that produces the output of the network $\\mathbf{y} = \\left\\{y_{1},y_{2},\\dots,y_{k}\\right\\}$ where $\\mathbf{y}\\in\\mathbb{R}^{k}$. Each node in the output layer takes the output from the hidden layer and applies a linear transformation followed by a non-linear activation function. In this example, we have three output nodes. For instance, we could predict three classes in a multiclass classification task. In this case, the network output can be interpreted as probabilities for each class, and we can use techniques like softmax to convert the output into probabilities.\n",
    "\n",
    "Let's generalize this example to a more formal definition of a feedforward neural network.\n",
    "\n",
    "### Function Composition\n",
    "Suppose we have a feedforward neural network with $L$ layers. The network has $n$ input nodes (we'll call this layer 0), and $i=1,2,\\dots, L-1$ hidden layers where each hidden layer has $m_{i}$ nodes, and the output (layer $L$) has $d_{out}$ output nodes.  Each hidden layer is fully connected to the previous and subsequent layers (but there are no connections between the nodes inside a layer and no self-connections). Information flows from the input to the output layer, forming a feedforward structure.\n",
    "\n",
    "Let's dig into the states and parameters of the network.\n",
    "\n",
    "* __Inputs and outputs__: Let $\\mathbf{x} = \\left\\{x_{1},x_{2},\\dots,x_{d_{in}},1\\right\\}$ be the _augmented_ input vector, where $x_{i}\\in\\mathbb{R}$ is the $i$-th feature of the input vector. The dimension of the _augmented_ input vector is $d_{in} + 1$, where $d_{in}$ is the number of features in the input vector. The extra `1` is added to the input vector to allow us to include the bias term in the weight vector. This is a common technique used in machine learning to simplify the representation of the model. Further, let $\\mathbf{z}_{i} = \\left\\{z^{(i)}_{1},z^{(i)}_{2},\\dots,z^{(i)}_{m_{i}}\\right\\}$ be the output vector of the $i$-th hidden layer, where $z^{(i)}_{j}\\in\\mathbb{R}$ is the $j$-th component of the output of layer $i$. Finally, let $\\mathbf{y}= \\left\\{y_{1},y_{2},\\dots,y_{d_{out}}\\right\\}$ be the output vector, where $y_{k}\\in\\mathbb{R}$ is the $k$-th component of the output of the network.\n",
    "\n",
    "* __Parameters__: Each node $j=1,2,\\dots,m_{i}$ in layer $j\\geq{1}$ has a parameter vector $\\mathbf{w}^{(i)}_{j} = \\left(w^{(i)}_{j,1},w^{(i)}_{j,2},\\dots,w^{(i)}_{j,m_{i-1}}, b^{(i)}_{j}\\right)$, where $w^{(i)}_{j,k}\\in\\mathbb{R}$ is the weight of the $k$-th input to node $j$ in layer $i$, and $b^{(i)}_{j}\\in\\mathbb{R}$ is the bias term for node $j$ in layer $i$. The weight vector $\\mathbf{w}^{(i)}_{j}$ represents the strength of the connection between node $j$ in layer $i$ and all nodes in layer $i-1$. The bias term $b_{i}$ allows the model to shift the activation function to the left or right.\n",
    "\n",
    "This may seem confusing, so let's think about this differently. A feedforward neural network can be considered a series of function compositions. For example, consider layer $1$ with $m_{1}$ nodes. The output of layer $1$ (given the input vector $\\mathbf{z}_{\\circ}$) is given by:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{z}^{(1)} &= \\begin{bmatrix}\n",
    "\\sigma_{1}\\left(\\mathbf{z}_{\\circ}^{\\top}\\cdot\\mathbf{w}^{(1)}_{1}\\right) \\\\\n",
    "\\sigma_{1}\\left(\\mathbf{z}_{\\circ}^{\\top}\\cdot\\mathbf{w}^{(1)}_{2}\\right) \\\\\n",
    "\\vdots \\\\\n",
    "\\sigma_{1}\\left(\\mathbf{z}_{\\circ}^{\\top}\\cdot\\mathbf{w}^{(1)}_{m_{1}}\\right)\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\mathbf{w}^{(1)}_{j}$ is the weight vector for node $j$ in layer $1$, and $\\sigma_{1}$ is the activation function for layer $1$ (assumed to be the same for nodes in layer $1$). The output of layer $1$ is a vector $\\mathbf{z}^{(1)}\\in\\mathbb{R}^{m_{1}}$ which is then passed to layer $2$, which has $m_{2}$ nodes:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{z}^{(2)} &= \\begin{bmatrix}\n",
    "\\sigma_{2}\\left(\\mathbf{z}_{1}^{\\top}\\cdot\\mathbf{w}^{(2)}_{1}\\right) \\\\\n",
    "\\sigma_{2}\\left(\\mathbf{z}_{1}^{\\top}\\cdot\\mathbf{w}^{(2)}_{2}\\right) \\\\\n",
    "\\vdots \\\\\n",
    "\\sigma_{2}\\left(\\mathbf{z}_{1}^{\\top}\\cdot\\mathbf{w}^{(2)}_{m_{2}}\\right)\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\mathbf{w}^{(2)}_{j}$ is the weight vector for node $j$ in layer $2$, and $\\sigma_{2}$ is the activation function for layer $2$ (assumed to be the same for nodes in layer $2$). However, we can also think of the output of layer $2$ as: $\\mathbf{z}^{(2)} = \\sigma_{2}\\circ\\sigma_{1}\\left(\\mathbf{z}_{\\circ}\\right)$, where $\\sigma_{2}\\circ\\sigma_{1}$ is the composition of the two activation functions, i.e., $\\mathbf{z}^{(2)} = \\sigma_{2}\\left(\\sigma_{1}\\left(\\mathbf{z}_{\\circ}\\right)\\right)$. Putting these ideas together, gives a nice way to think about a feedforward neural network: a series of function compositions:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{\\mathbf{y}} &= f_{\\theta}(\\mathbf{x}) = \\sigma_{L}\\circ\\sigma_{L-1}\\circ\\dots\\circ\\sigma_{1}\\left(\\mathbf{x}\\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\sigma_{L}$ is the activation function for the output layer, $\\mathbf{x}$ is the _augmented_ input vector, and $\\hat{\\mathbf{y}}\\in\\mathbb{R}^{d_{out}}$ is the output of the network. The function $f_{\\theta}(\\mathbf{x})$ represents the mapping from the input vector $\\mathbf{x}=\\mathbf{z}_{\\circ}$ to the output vector $\\hat{\\mathbf{y}}$, and $\\theta$ represents the parameters of the network (i.e., the weights and biases). \n",
    "\n",
    "__Wow!__ A feedforward neural network $f_{\\theta}:\\mathbb{R}^{d_{in}}\\rightarrow\\mathbb{R}^{d_{out}}$ is just _some complicated function_ that takes an input vector $\\mathbf{x}$ and produces an output vector $\\hat{\\mathbf{y}}$. Thus, we can do everything we do with functions: compose them, take their derivatives, and so on. This is a compelling idea because it allows us to use all the tools of calculus and linear algebra to analyze and optimize neural networks.\n",
    "\n",
    "### Parameterization\n",
    "Before we move on, let's take a moment to think about the parameters of the network. The parameters of the network are the weights and biases of each node. Each layer $i$ has $m_{i}$ nodes, and each node in layer $i$ has a _weight vector_ $\\mathbf{w}^{(i)}_{j} = \\left(w^{(i)}_{j,1},w^{(i)}_{j,2},\\dots,w^{(i)}_{j,m_{i-1}}, b^{(i)}_{j}\\right)$, where $w^{(i)}_{j,k}\\in\\mathbb{R}$ is the weight of the $k$-th input to node $j$ in layer $i$, and $b^{(i)}_{j}\\in\\mathbb{R}$ is the bias term for node $j$ in layer $i$. \n",
    "\n",
    "We represent the parameters of _each_ layer $i$ in the matrix $\\mathbf{W}_{i}\\in\\mathbb{R}^{m_{i}\\times(m_{i-1}+1)}$. The weight matrix $\\mathbf{W}_{i}$ contains the weights and biases for all nodes in layer $i$. Thus, we can pack all the parameters $\\left(\\mathbf{W}_{1},\\mathbf{W}_{2},\\dots,\\mathbf{W}_{L}\\right)$ into the $\\theta$ vector:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\theta &\\equiv  \\left(w^{(1)}_{1,1},w^{(1)}_{1,2},\\dots,w^{(1)}_{1,m_{0}}, b^{(1)}_{1}, w^{(1)}_{2,1},w^{(1)}_{2,2},\\dots,w^{(1)}_{2,m_{0}}, b^{(1)}_{2}, \\ldots, w^{(L)}_{k,1},w^{(L)}_{k,2},\\dots,w^{(L)}_{k,m_{L-1}}, b^{(L)}_{k}\\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "where $k=1,2,\\dots,d_{out}$ is the index of the output node. \n",
    "\n",
    "#### How big is $\\theta$?\n",
    "The number of parameters in a feedforward neural network depends on the number of layers and nodes in each layer. Suppose we have a feedforward neural network with $L$ layers, where the $i$-th layer has $m_{i}$ nodes. The number of parameters in the network is given by:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Number of parameters} &= \\sum_{i=1}^{L} m_{i}\\left(m_{i-1}+1\\right) \\\\\n",
    "&= \\sum_{i=1}^{L} \\left(m_{i} m_{i-1} + m_{i}\\right) \\\\\n",
    "&= \\underbrace{\\sum_{i=1}^{L} m_{i} m_{i-1}}_{\\text{weights}} + \\underbrace{\\sum_{i=1}^{L} m_{i}}_{\\text{bias terms}} \\quad \\blacksquare \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "where $m_{0}$ is the number of input nodes, and $m_{L}$ is the number of output nodes. The first term counts the number of weight parameters in the network, while the second term counts the number of biases in the network.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33115fe",
   "metadata": {},
   "source": [
    "### Example\n",
    "Let's do a simple example to illustrate the concepts we've discussed. This network won't win any prizes or do anything useful, but it will help us understand the basic concepts of feedforward neural networks.\n",
    "\n",
    "Start by specifying the input to the network in the `x::Array{Float64}` variable. \n",
    "* The input to the network is a vector of binary or continuous-valued features representing the data we want to process. Each component of the input vector corresponds to a feature of the data. For example, in an image classification task, the input vector could represent the pixel values of an image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "689e5230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11-element Vector{Float32}:\n",
       " -2.0\n",
       " -1.6\n",
       " -1.2\n",
       " -0.8\n",
       " -0.4\n",
       "  0.0\n",
       "  0.4\n",
       "  0.8\n",
       "  1.2\n",
       "  1.6\n",
       "  2.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = range(-2, 2, length=11) |> collect .|> Float32 # notice: .|> Float32. What?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9023cd",
   "metadata": {},
   "source": [
    "Next, let's consider the processing in a single network layer. For example, suppose we have an input dimension of $d_{in} = 11$ and a hidden layer with $m_{1} = 21$ nodes. We'll pick some activation function (e.g., the sigmoid or relu function [from the `NNlib.jl` package](https://fluxml.ai/NNlib.jl/dev/)) and then compute the output of the hidden layer.\n",
    "* _What activation function should we use?_ The choice of activation function is important because it affects the learning process and the neural network's performance. There are various activation functions, each with its own characteristics and applications. We can make this design choice when we build the network. \n",
    "* _What should the weights and biases be?_ The weights and biases are the network parameters we will learn during training. Let's pick some random values for the weights and biases for now. This gets us started, and we can refine the values later. When we build [a `MyLayerModel` instance](src/Types.jl) these parameters will be initialized to random values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f3559e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "z, testmodel = let\n",
    "\n",
    "    # let's build a simple neural network with some hidden layers\n",
    "    n = length(x); # number of inputs (zₒ)\n",
    "    m = 21; # number of hidden neurons\n",
    "\n",
    "    # build  model -\n",
    "    model = build(MyLayerModel, (\n",
    "        n = n, # number of inputs\n",
    "        m = m, # number of hidden neurons\n",
    "        σ = NNlib.relu, # activation function (pick one from NNlib)\n",
    "    ));\n",
    "\n",
    "    # evaluate this layer\n",
    "    z = model(x); # evaluate the model with this input\n",
    "\n",
    "    # return -\n",
    "    z, model\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4302dd21-eabb-4421-9470-e3f71bc95683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21-element Vector{Float64}:\n",
       " 0.5852189092954473\n",
       " 0.3487020580999918\n",
       " 0.0\n",
       " 0.14203364390344145\n",
       " 0.9016406935543316\n",
       " 0.37701185869057846\n",
       " 0.7335371510975494\n",
       " 0.07339924604076373\n",
       " 0.09597524706063207\n",
       " 0.16028580806600037\n",
       " 0.14290996884118481\n",
       " 0.5781189264724963\n",
       " 0.03623396819358016\n",
       " 0.0687302813552244\n",
       " 0.0\n",
       " 0.19603097562654226\n",
       " 0.0\n",
       " 0.10268975314844708\n",
       " 0.0\n",
       " 0.2735201241484355\n",
       " 0.35343646192211936"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871513db",
   "metadata": {},
   "source": [
    "That's sort of interesting! Let's see what multiple layers look like (and how we can construct such a creature with our simple codebase)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "43850809",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z, models = let\n",
    "\n",
    "    # initialize -\n",
    "    L = 3; # How many layers do we want?\n",
    "    σ = [NNlib.tanh_fast, NNlib.relu, NNlib.sigmoid]; # activation function (pick one from NNlib)\n",
    "    n = length(x); # number of inputs (zₒ)\n",
    "\n",
    "    # Dimensions of each layer -\n",
    "    m = Dict{Int, Int}();\n",
    "    m[0] = n; # dimension of the inputs (zₒ) (including the bias at the end)\n",
    "    for i in 1:(L-1)\n",
    "        m[i] = 2*m[0]; # number of hidden neurons, make this some mutiple of the input size\n",
    "    end\n",
    "    m[L] = 1; # number of outputs (zₗ)\n",
    "\n",
    "    # build models dictionary -\n",
    "    models = Dict{Int, MyLayerModel}();\n",
    "    for i in 1:L\n",
    "        \n",
    "        models[i] = build(MyLayerModel, (\n",
    "            n = m[i-1], # number of inputs\n",
    "            m = m[i], # number of hidden neurons\n",
    "            σ = σ[i], # activation function (pick one from NNlib)\n",
    "        ));\n",
    "    end;\n",
    "    \n",
    "    # compute the output of each layer -\n",
    "    z = Dict{Int, Array{Float32}}();\n",
    "    z[0] = x;\n",
    "    for i in 1:L\n",
    "        z[i] = models[i](z[i-1]); # evaluate the layer\n",
    "    end\n",
    "\n",
    "    z, models; # return the output of each layer \n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1c4fba3f-bec7-42c0-9399-e6bf3764f730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{Float32}:\n",
       " 0.47224638"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1da223",
   "metadata": {},
   "source": [
    "__Check__: Is our parameter count correct? Let's check this by hand and then compare it to the code. We'll hardcode the values of the various dimensions of the network in the example above; if you change them, you'll need to change the calculation below.\n",
    "* __Input layer__: $d_{in} = 11$ (11 input nodes)\n",
    "* __Hidden layer 1__: $m_{1} = 22$ (22 hidden nodes)\n",
    "* __Hidden layer 2__: $m_{2} = 22$ (22 hidden nodes)\n",
    "* __Output layer__: $d_{out} = 1$ (1 output node)\n",
    "\n",
    "which gives the following total number of parameters:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Number of parameters} &= \\sum_{i=1}^{L} m_{i}\\left(m_{i-1}+1\\right) \\\\\n",
    "&= \\sum_{i=1}^{L} m_{i} m_{i-1} + \\sum_{i=1}^{L} m_{i} \\\\\n",
    "& = m_{0} m_{1} + m_{1} m_{2} + m_{2}m_{3} + m_{1}+m_{2} + m_{3} \\\\\n",
    "& = \\left(m_{0}+1\\right) \\cdot m_{1} + \\left(m_{1}+1\\right) \\cdot m_{2} + \\left(m_{2}+1\\right) \\cdot m_{3} \\\\\n",
    "&= \\left(11+1\\right) \\cdot 22 + \\left(22+1\\right) \\cdot 22 + \\left(22+1\\right) \\cdot 1 \\\\\n",
    "&= 12 \\cdot 22 + 23 \\cdot 22 + 23 \\cdot 1 \\\\\n",
    "&= 264 + 506 + 23 \\\\\n",
    "&= 793\\quad \\blacksquare \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "Let's check this with our code. We'll count the parameters in each layer and then sum them up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05c1bed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_number_of_parameters = let\n",
    "    \n",
    "    # initialize -\n",
    "    L = 3; # How many layers do we want?\n",
    "\n",
    "    total_number_of_parameters = 0;\n",
    "    for i in 1:L\n",
    "        \n",
    "        # get the model\n",
    "        model = models[i];\n",
    "\n",
    "        # get the number of parameters\n",
    "        mᵢ = size(model.W, 1); # number of hidden neurons (rows)\n",
    "        nᵢ = size(model.W, 2); # number of inputs (columns)\n",
    "\n",
    "        # add to the total\n",
    "        total_number_of_parameters += (nᵢ*mᵢ);\n",
    "    end\n",
    "\n",
    "    total_number_of_parameters;\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "285cb22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@assert total_number_of_parameters == 793 # check the number of parameters. Incorrect? Then ... boooom!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2f5283",
   "metadata": {},
   "source": [
    "__Wow!__ That is a lot of parameters! This is a simple network, but it has a lot of parameters. This is one of the reasons why neural networks are so powerful: they can learn very complex functions by adjusting the weights and biases of the network. However, this also means they can be prone to overfitting, especially if we have a small amount of training data.\n",
    "\n",
    "Speaking of training, let's look at how we can train this network to learn the model's parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2984cb18",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dae27b",
   "metadata": {},
   "source": [
    "## Training\n",
    "Suppose have a training dataset $\\mathcal{D} = \\left\\{(\\mathbf{x}_{1},y_{1}),\\dotsc,(\\mathbf{x}_{n},y_{n})\\right\\}$ with $n$ examples, where \n",
    "$\\mathbf{x}_{i}\\in\\mathbb{R}^{m}$ is the $i$-th feature vector, and $y_{i}\\in\\mathbb{R}$ is the corresponding output. The output can be a discrete label (e.g., in classification tasks) where each example has been labeled by an expert, i.e., a human to be in a category $y_{i}\\in\\{-1,1\\}$, or is some continuous value $y_{i}\\in\\mathbb{R}$, e.g., a real-valued measurement such as temperature, pressure, etc, for regression tasks. \n",
    "\n",
    "Classically, the training of feedforward neural networks is done [using the _backpropagation_ algorithm](https://en.wikipedia.org/wiki/Backpropagation), a common _supervised learning_ algorithm based on gradient descent. \n",
    "* _Do we have to use Gradient descent to train a neural network?_ No! Theoretically, other optimization algorithms like genetic algorithms, particle swarm optimization, and simulated annealing could also be used. However, gradient descent is the most common due to rigid orthodoxy (hot take) and some interesting technical features.\n",
    "\n",
    "Backpropagation computes the gradient for training multi-layer neural networks using gradient descent. It is a supervised learning method applying the chain rule of calculus to evaluate the gradient of the loss function concerning network weights and biases. The algorithm involves two steps:   \n",
    "1. **Forward Pass**: Calculate the network's output for an input by passing it through each layer and applying the activation function at every node, yielding an output vector $\\hat{\\mathbf{y}}$ for the input $\\mathbf{x}$.  \n",
    "2. **Backward Pass**: Determine the gradient of the loss function, which measures the difference between the actual output $\\mathbf{y}$ and predicted output $\\hat{\\mathbf{y}}$, by propagating the error backward using the chain rule.\n",
    "\n",
    "### Forward Pass\n",
    "__Initialization__: Initialize the weights and biases of the network randomly or using some heuristic method. Let $\\mathbf{z}_{\\circ}^{\\top} = \\left(x_{1},x_{2},\\dots,x_{n}, 1\\right)$ be the _augmented input vector_, where the last component is a constant `1` that allows us to include the bias term in the weight vector. \n",
    "\n",
    "For each layer $i=1,2,\\dots,L$ of the network:\n",
    "1. For each node $j=1,2,\\dots,m_{i}$ in layer $i$:\n",
    "      1. Compute the input to the activation function: $a_{j} = \\mathbf{z}_{i-1}^{\\top}\\cdot{\\mathbf{w}^{(i)}_{j}}$, where $\\mathbf{w}^{(i)}_{j}$ is the parameter vector (weights and bias) for node $j$ in layer $i$, and $\\mathbf{z}^{\\top}_{i-1}$ is the transpose of the output vector from the previous layer, i.e., the inpt to layer $i$ is the output of layer $i-1$. \n",
    "      2. Compute the output of the activation function: $z_{j} = \\sigma_{i}(a_{j})$, where $\\sigma:\\mathbb{R}\\rightarrow\\mathbb{R}$ denotes the activation function for layer $i$, and $a_{j}$ denotes the input to the activation function. The activation function can be a $\\texttt{sigmoid}$, $\\texttt{tanh}$, $\\texttt{ReLU}$, or any other non-linear function that introduces non-linearity into the model. \n",
    "2. Store the output of each node in the vector $\\mathbf{z}_{i} = \\left\\{z_{1},z_{2},\\dots,z_{m_{i}}\\right\\}$, where $z_{j}\\in\\mathbb{R}$ is the $j$-th component of the output of layer $i$.\n",
    "4. The output of the last layer is the final model predicted output from the network: $\\hat{\\mathbf{y}} = \\mathbf{z}_{L}$.\n",
    "\n",
    "end\n",
    "\n",
    "### Backward Pass (Gradient Descent)\n",
    "The backward pass computes the gradient of a _loss function_ with respect to each weight and bias in the network by propagating the error backward through the network using the chain rule.\n",
    "* _What is a loss function_? Suppose we have a function $\\mathcal{L}(\\mathbf{y},f_{\\theta}(\\mathbf{x}))$ that measures the difference between the actual output $\\mathbf{y}$ and the model predicted output $\\hat{\\mathbf{y}} = f_{\\theta}(\\mathbf{x})$. The loss is _big_ when the predicted output is far from the actual output and _small_ when the predicted output is close to the actual output. The loss function can be considered a measure of how well the model performs on the training data. The goal of training is to minimize this loss function by adjusting the weights and biases of the network.\n",
    "* _What loss function do we use?_ The loss function depends upon the task we are trying to do. For example, the loss can be the mean squared error (MSE) for regression tasks, or [cross-entropy loss for classification tasks](https://en.wikipedia.org/wiki/Cross-entropy), negative log-likelihood for either classification and regression, etc.\n",
    "\n",
    "We assume $\\mathcal{L}(\\mathbf{y},f_{\\theta}(\\mathbf{x}))$ is _at least once differentiable_ with respect to the parameters, i.e., we can compute the gradient $\\nabla_{\\theta}{\\mathcal{L}}(\\cdot)$. The gradient points in the direction of the steepest increase of the function. Thus, we can iteratively update the parameters to minimize the objective function using the update rule:\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\theta_{k+1} = \\theta_{k} - \\alpha(k)\\cdot\\nabla_{\\theta}\\mathcal{L}(\\theta_{k})\\quad\\text{where}{~k = 0,1,2,\\dots}\n",
    "\\end{equation*}\n",
    "$$\n",
    "where $k$ denotes the iteration index, and $\\nabla_{\\theta}\\mathcal{L}(\\cdot)$ is the gradient of the loss function with respect to the parameters $\\theta$. \n",
    "* _What is $\\alpha(k)$?_ The (hyper) parameter $\\alpha(k)>0$ is the _learning rate_ which can be a function of the iteration count $k$. This is a user-adjustable parameter, and we'll assume it's constant for today.\n",
    "* _Stopping?_ Gradient descent will continue to iterate until a stopping criterion is met, i.e., $\\lVert\\theta_{k+1} - \\theta_{k}\\rVert\\leq\\epsilon$ or the maximum number of iterations is reached, or some other stopping criterion is met, i.e., the gradient is small at the current iteration $\\lVert\\nabla_{\\theta}\\mathcal{L}(\\theta_{k})\\rVert\\leq\\epsilon$.\n",
    "\n",
    "Pusedocode for a naive gradient descent algorithm (for a fixed learning rate) is shown in [the week-3 lecture notes](https://github.com/varnerlab/CHEME-5820-Lectures-Spring-2025/blob/main/lectures/week-3/L3c/docs/Notes.pdf). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd856363",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent\n",
    "Computing the full gradient can be expensive. Stochastic Gradient Descent (SGD) is a less expensive approximation to full gradient descent. Suppose we let $\\mathcal{L}(\\theta)$ denote the overall objective function computed over all $n$ of the training examples. Then, the loss function could be written as:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{L}(\\theta) &= \\frac{1}{n}\\sum_{i=1}^{n}\\mathcal{L}_{i}(\\theta)\n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\mathcal{L}_{i}(\\theta)$ denote the loss on the $i$-th training example. The parameter update rule for training dataset $\\mathcal{D}$ is then given by:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\theta_{k+1} = \\theta_{k} - \\frac{\\alpha(k)}{n}\\cdot\\sum_{i\\in\\mathcal{D}}\\nabla_{\\theta}\\mathcal{L}_{i}(\\theta_{k})\\quad\\text{where}{~k = 0,1,2,\\dots}\n",
    "\\end{align*}\n",
    "$$\n",
    "In _stochastic gradient descent_ (SGD), we _approximate_ the full gradient using only a _single_ training example $\\mathcal{L}_{i}(\\theta)$, i.e., we randomly sample a single training example from the dataset $\\mathcal{D}$ at each iteration. The parameter update rule for SGD is:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\theta \\gets \\theta - \\frac{\\alpha}{n}\\cdot\\nabla_{\\theta}\\mathcal{L}_{i}(\\theta)\n",
    "\\end{align*}\n",
    "$$\n",
    "where $i$ is a randomly selected training example from the dataset $\\mathcal{D}$. The basic SGD algorithm is something like:\n",
    "\n",
    "__Initialize__: Choose an initial value for parameters $\\theta$ and a learning rate (function) $\\alpha$.\n",
    "\n",
    "While _not_ converged:\n",
    "1. Randomly shuffle the order of the training data\n",
    "1. For $i = 1,2,\\dots,n$ do:\n",
    "    1. Compute the update: $\\theta \\gets \\theta - \\alpha\\cdot\\nabla_{\\theta}\\mathcal{L}_{i}(\\theta)$\n",
    "1. Check for convergence: $\\lVert \\theta^{\\prime} - \\theta \\rVert_{2}^{2}\\leq\\epsilon$ then converged.\n",
    "2. If not converged, update the learning rate $\\alpha$.\n",
    "\n",
    "We can also use _mini-batch_ gradient descent, where we randomly sample a small batch of training examples (say $b$) at each iteration. The parameter update rule for mini-batch gradient descent is:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\theta \\gets \\theta - \\frac{\\alpha}{b}\\cdot\\sum_{i=1}^{b}\\nabla_{\\theta}\\mathcal{L}_{i}(\\theta)\n",
    "\\end{align*}\n",
    "$$\n",
    "where $b$ is the size of the mini-batch, and the mini-batch size is a hyperparameter that can be tuned to improve performance.\n",
    "* _What is the mini-batch size?_ The mini-batch size is a hyperparameter that can be tuned to improve performance. A small mini-batch size can lead to faster convergence, while a larger one can lead to more stable convergence (but may be slower). The choice of mini-batch size depends on the specific problem and the available computational resources.\n",
    "\n",
    "The computation of the gradient $\\nabla_{\\theta}\\mathcal{L}_{i}(\\theta)$ is made much easier [using techniques such as Automatic differentiation](https://arxiv.org/abs/1502.05767)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e01c0c",
   "metadata": {},
   "source": [
    "## Strengths and Weaknesses of FNNs\n",
    "Using feedforward neural networks (FNNs) for machine learning tasks has several advantages and disadvantages. Here are some of the key points to consider:\n",
    "\n",
    "### Strengths\n",
    "* _Universal approximation theorem_: [FNNs are universal approximators](https://en.wikipedia.org/wiki/Universal_approximation_theorem), meaning they can approximate any continuous function to arbitrary precision, given enough hidden units and training data. This makes them very powerful for a wide range of tasks.\n",
    "* _Flexibility_: FNNs can be used for various tasks, including classification, regression, and generative modeling. They can also be adapted to work with different data types, such as images, text, and time series. \n",
    "* _Non-linearity_: Using non-linear activation functions, FNNs can learn complex non-linear relationships between inputs and outputs. This allows them to model complex patterns in the data that linear models cannot capture. They can also be used with various data sources, including images, text, and time series data. This allows them to explore non-linearly separable datasets. \n",
    "\n",
    "### Weaknesses\n",
    "* _Overfitting_: FNNs can easily overfit the training data, especially when the model is too complex or the training data is limited. This can lead to poor generalization to new data. Regularization techniques such as dropout, L1/L2 regularization, and early stopping can help mitigate this issue.\n",
    "* _Computationally expensive_: Training FNNs can be computationally expensive, especially for large datasets or deep networks. This can require significant computational resources and time. However, this is less of a concern with modern hardware and software frameworks, but these techniques require (for the most part) expert-level knowledge to implement efficiently.\n",
    "* _Interpretability_: FNNs are often considered `black box` models, meaning it can be difficult to interpret how they make predictions. This can be a disadvantage in applications where interpretability is essential, such as healthcare or finance. However, there are techniques [such as LIME](https://arxiv.org/abs/1602.04938) and [SHAP](https://arxiv.org/abs/1705.07874) that can help improve the interpretability of FNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900ace57",
   "metadata": {},
   "source": [
    "## Lab\n",
    "In Lab `L12b`, we will implement (and train) a feed-forward model for a simple computer vision task. \n",
    "* _Cool, what is this task?_ We'll give handwritten digits to the model and ask it to classify them. The model will be a _deep_ feedforward neural network (a network with potentially many hidden layers). We'll use [the MNIST handwritten image dataset](https://en.wikipedia.org/wiki/MNIST_database), which contains 60,000 images of handwritten digits (0-9). The goal is to train the model to recognize these digits based on the pixel values of the images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fdc635",
   "metadata": {},
   "source": [
    "# Today?\n",
    "That's a wrap! What are some of the interesting things we discussed today?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.4",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
