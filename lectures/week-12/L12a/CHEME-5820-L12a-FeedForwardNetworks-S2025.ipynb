{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9298eb4a-7abb-40a4-a9a0-aff9cfe005f1",
   "metadata": {},
   "source": [
    "# L12a: Feed Forward Neural Networks (FNNs)\n",
    "\n",
    "___\n",
    "\n",
    "In this lecture, we introduce Feed Forward Neural Networks (FNNs), an artificial neural network structure where connections between layers of nodes do not form cycles. This differs from recurrent neural networks (RNNs), where data can flow in cycles (we'll look at recurrent networks next time). The key concepts discussed in this lecture are:\n",
    "\n",
    "* __FNN Architecture__: Feedforward neural networks (FNNs) are foundational artificial neural network architectures in which information flows unidirectionally from input nodes through (potentially many) hidden layers of arbitrary dimension to output nodes without cycles or feedback loops. Each node in the network is a simple processing unit that applies a linear transformation followed by a (potentially) non-linear activation function to its inputs. \n",
    "* __FNN Applications__: FNNs are widely employed for pattern recognition, classification tasks (including for non-linearly separable data), and predictive modeling, such as identifying objects in images, sentiment analysis in text, or forecasting process trends. They also work well in structured data applications like medical diagnostics (classification of patient records) and marketing (personalized recommendations). FFNs also are components of more advanced architectures in fields like computer vision and natural language processing.\n",
    "* __FNN Training__: FNNs are trained using supervised learning, where the model learns to map inputs to outputs by minimizing a loss function. The most common training algorithm [is backpropagation](https://en.wikipedia.org/wiki/Backpropagation), which uses [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) to update the weights (and bias values) of the network based on the error between predicted and actual outputs.\n",
    "\n",
    "The source(s) for this lecture can be found here:\n",
    "* [John Hertz, Anders Krogh, and Richard G. Palmer. 1991. Introduction to the theory of neural computation. Addison-Wesley Longman Publishing Co., Inc., USA.](https://dl.acm.org/doi/10.5555/104000)\n",
    "* [Mehlig, B. (2021). Machine Learning with Neural Networks. Chapter 5: Perceptrons and Chapter 6: Stochastic Gradient Descent](https://arxiv.org/abs/1901.05639v4)\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0d82fb",
   "metadata": {},
   "source": [
    "However, before we do anything, let's set up the computational environment, e.g., importing the necessary libraries (and codes) by including the `Include.jl` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44cf1c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"Include.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bacf60",
   "metadata": {},
   "source": [
    "## Origin story: McCulloch-Pitts Neurons\n",
    "In [their paper, McCulloch and Pitts (1943)](https://link.springer.com/article/10.1007/BF02478259) explored how the brain could produce highly complex patterns by using many [interconnected _basic cells (neurons)_](https://en.wikipedia.org/wiki/Biological_neuron_model). McCulloch and Pitts suggested a _highly simplified model_ of a neuron. Nevertheless, they made a foundational contribution to developing artificial neural networks that we find in wide use today. Let's look at the model of a neuron proposed by McCulloch and Pitts.\n",
    "\n",
    "Suppose we have a neuron that takes an input vector $\\mathbf{n}(t) = (n^{(t)}_1, n^{(t)}_2, \\ldots, n^{(t)}_{m})$, where each component $n_k\\in\\mathbf{n}$ is a binary value (`0` or `1`) which represents the state of other predecessor neurons $n_1,n_2,\\ldots,n_m$ at time $t$. Then, the state of our neuron (say neuron $k$) at time $t+1$ is given by:\n",
    "$$\n",
    "\\begin{align*}\n",
    "n_{k}(t+1) &= \\sigma\\left(\\sum_{j=1}^{m} w_{kj} n_j(t) - \\theta_k\\right) \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\sigma:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ is an _activation function_ that maps the weighted sum of a vector of inputs to a scalar (binary) output. In the original paper, the state of neuron $k$ at time $t+1$ denoted as $n_k(t+1)\\in\\{0,1\\}$, where $w_{kj}$ is the weight of the connection between neuron $k$ to the output of (predecessor) neuron $k$, and $\\theta_k$ is the threshold for neuron $k$. \n",
    "* _Activation function_: In this original McCulloch and Pitts model, the activation function $\\sigma$ is a step function, which means that the output of the neuron is `1` if the weighted sum of inputs exceeds the threshold $\\theta_k$, and `0` otherwise. In other words, the neuron \"fires\" (produces an output of `1`) if the total input to the neuron is greater than or equal to the threshold $\\theta_k$. This is a binary output, simplifying real biological neurons that can produce continuous outputs.\n",
    "* _Parameters_: The weights $w_{kj}\\in\\mathbb{R}$ and the threshold $\\theta_k\\in\\mathbb{R}$ are parameters of the neuron that determine its behavior. The weights can be positive or negative, representing the strength and direction of the influence of the input neurons on the output neuron. The threshold determines how much input the neuron needs to \"fire\" (i.e., produce an output of `1`).\n",
    "\n",
    "While the McCulloch-Pitts neuron model simplifies real biological neurons, it laid the groundwork for the development of more complex artificial neural networks. The key idea is that by combining many simple neurons in a network, we can create complex functions and learn to approximate any continuous function. This idea is at the heart of modern deep learning and neural networks. \n",
    "\n",
    "__Hmmmm__. These ideas _really_ seem familiar. Have we seen this before? Yes! the McCulloch-Pitts Neuron underpins [The Perceptron (Rosenblatt, 1957)](https://en.wikipedia.org/wiki/Perceptron), [Hopfield networks](https://en.wikipedia.org/wiki/Hopfield_network) and [Boltzmann machines](https://en.wikipedia.org/wiki/Boltzmann_machine). Wow!!\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ede978",
   "metadata": {},
   "source": [
    "## Activation functions\n",
    "The activation function $\\sigma:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ of a neuron is a mathematical function that determines the output of the neuron based on its input. \n",
    "\n",
    "The activation function takes the weighted sum of the inputs to the neuron and applies a non-linear transformation to produce the output. The choice of activation function is important because it affects the learning process and the performance of the neural network. There is a [wide variety of activation functions](https://en.wikipedia.org/wiki/Activation_function#Table_of_activation_functions), each with its own characteristics and applications. \n",
    "\n",
    "Some common activation functions are:\n",
    "\n",
    "* __Sigmoid function__: The sigmoid function is a smooth, S-shaped curve that maps input values to the range (0, 1). It is defined as:\n",
    "$ \\sigma(x) = \\frac{1}{1 + e^{-x}}$. The sigmoid function is often used in the output layer of binary classification problems, as it can be interpreted as a probability. \n",
    "* __Hyperbolic tangent function (tanh)__: The $\\texttt{tanh}$ function is similar to the sigmoid function but maps input values to the range (-1, 1). It is defined as: $\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$. It is often used in hidden layers of neural networks.\n",
    "* __Rectified Linear Unit (ReLU)__: The ReLU function is a piecewise linear function that outputs the input value if it is positive and zero otherwise. It is defined as: $\\text{ReLU}(x) = \\max(0, x)$. The $\\texttt{ReLU}$ function helps mitigate the vanishing gradient problem, a complication in training, making it a popular choice for hidden layers in deep networks. However, it can suffer from [the dying ReLU problem](https://arxiv.org/abs/1903.06733), where neurons can become inactive and stop learning if they output zero for all inputs.\n",
    "* __Softmax function__: The $\\texttt{softmax}$ function is often used in the output layer of multi-class classification problems. It converts a vector of raw scores (logits) into a probability distribution over multiple classes. It is defined as: $\\texttt{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}$ where $z_i$ is the raw score for class $i$, and $K$ is the total number of classes. The softmax function ensures that the output probabilities sum to 1, making it suitable for multi-class classification tasks.\n",
    "\n",
    "Let's take a look at the [activation functions exported by the `NNlib.jl` package](https://fluxml.ai/NNlib.jl/dev/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2dd6a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip240\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip240)\" d=\"M0 1600 L2400 1600 L2400 8.88178e-14 L0 8.88178e-14  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip241\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip240)\" d=\"M257.204 1423.18 L2352.76 1423.18 L2352.76 47.2441 L257.204 47.2441  Z\" fill=\"#f2f2f2\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip242\">\n",
       "    <rect x=\"257\" y=\"47\" width=\"2097\" height=\"1377\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip242)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"316.512,1423.18 316.512,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip242)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"810.746,1423.18 810.746,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip242)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1304.98,1423.18 1304.98,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip242)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1799.21,1423.18 1799.21,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip242)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"2293.45,1423.18 2293.45,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip242)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"316.512,1423.18 316.512,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip242)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"810.746,1423.18 810.746,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip242)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1304.98,1423.18 1304.98,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip242)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1799.21,1423.18 1799.21,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip242)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"2293.45,1423.18 2293.45,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip242)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"257.204,1408.46 2352.76,1408.46 \"/>\n",
       "<polyline clip-path=\"url(#clip242)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"257.204,1071.83 2352.76,1071.83 \"/>\n",
       "<polyline clip-path=\"url(#clip242)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"257.204,735.212 2352.76,735.212 \"/>\n",
       "<polyline clip-path=\"url(#clip242)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"257.204,398.59 2352.76,398.59 \"/>\n",
       "<polyline clip-path=\"url(#clip242)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"257.204,61.9674 2352.76,61.9674 \"/>\n",
       "<polyline clip-path=\"url(#clip242)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"257.204,1408.46 2352.76,1408.46 \"/>\n",
       "<polyline clip-path=\"url(#clip242)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"257.204,1071.83 2352.76,1071.83 \"/>\n",
       "<polyline clip-path=\"url(#clip242)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"257.204,735.212 2352.76,735.212 \"/>\n",
       "<polyline clip-path=\"url(#clip242)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"257.204,398.59 2352.76,398.59 \"/>\n",
       "<polyline clip-path=\"url(#clip242)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"257.204,61.9674 2352.76,61.9674 \"/>\n",
       "<polyline clip-path=\"url(#clip240)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"257.204,1423.18 2352.76,1423.18 \"/>\n",
       "<polyline clip-path=\"url(#clip240)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"257.204,47.2441 2352.76,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip240)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"316.512,1423.18 316.512,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip240)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"810.746,1423.18 810.746,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip240)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1304.98,1423.18 1304.98,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip240)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1799.21,1423.18 1799.21,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip240)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2293.45,1423.18 2293.45,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip240)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"316.512,47.2441 316.512,66.1417 \"/>\n",
       "<polyline clip-path=\"url(#clip240)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"810.746,47.2441 810.746,66.1417 \"/>\n",
       "<polyline clip-path=\"url(#clip240)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1304.98,47.2441 1304.98,66.1417 \"/>\n",
       "<polyline clip-path=\"url(#clip240)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1799.21,47.2441 1799.21,66.1417 \"/>\n",
       "<polyline clip-path=\"url(#clip240)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2293.45,47.2441 2293.45,66.1417 \"/>\n",
       "<path clip-path=\"url(#clip240)\" d=\"M286.454 1468.75 L316.13 1468.75 L316.13 1472.69 L286.454 1472.69 L286.454 1468.75 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M330.25 1481.64 L346.57 1481.64 L346.57 1485.58 L324.625 1485.58 L324.625 1481.64 Q327.288 1478.89 331.871 1474.26 Q336.477 1469.61 337.658 1468.27 Q339.903 1465.74 340.783 1464.01 Q341.686 1462.25 341.686 1460.56 Q341.686 1457.8 339.741 1456.07 Q337.82 1454.33 334.718 1454.33 Q332.519 1454.33 330.065 1455.09 Q327.635 1455.86 324.857 1457.41 L324.857 1452.69 Q327.681 1451.55 330.135 1450.97 Q332.588 1450.39 334.625 1450.39 Q339.996 1450.39 343.19 1453.08 Q346.385 1455.77 346.385 1460.26 Q346.385 1462.39 345.574 1464.31 Q344.787 1466.2 342.681 1468.8 Q342.102 1469.47 339 1472.69 Q335.899 1475.88 330.25 1481.64 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M780.503 1468.75 L810.179 1468.75 L810.179 1472.69 L780.503 1472.69 L780.503 1468.75 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M821.082 1481.64 L828.72 1481.64 L828.72 1455.28 L820.41 1456.95 L820.41 1452.69 L828.674 1451.02 L833.35 1451.02 L833.35 1481.64 L840.989 1481.64 L840.989 1485.58 L821.082 1485.58 L821.082 1481.64 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M1304.98 1454.1 Q1301.37 1454.1 1299.54 1457.66 Q1297.73 1461.2 1297.73 1468.33 Q1297.73 1475.44 1299.54 1479.01 Q1301.37 1482.55 1304.98 1482.55 Q1308.61 1482.55 1310.42 1479.01 Q1312.25 1475.44 1312.25 1468.33 Q1312.25 1461.2 1310.42 1457.66 Q1308.61 1454.1 1304.98 1454.1 M1304.98 1450.39 Q1310.79 1450.39 1313.85 1455 Q1316.92 1459.58 1316.92 1468.33 Q1316.92 1477.06 1313.85 1481.67 Q1310.79 1486.25 1304.98 1486.25 Q1299.17 1486.25 1296.09 1481.67 Q1293.04 1477.06 1293.04 1468.33 Q1293.04 1459.58 1296.09 1455 Q1299.17 1450.39 1304.98 1450.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M1789.6 1481.64 L1797.23 1481.64 L1797.23 1455.28 L1788.92 1456.95 L1788.92 1452.69 L1797.19 1451.02 L1801.86 1451.02 L1801.86 1481.64 L1809.5 1481.64 L1809.5 1485.58 L1789.6 1485.58 L1789.6 1481.64 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M2288.1 1481.64 L2304.42 1481.64 L2304.42 1485.58 L2282.48 1485.58 L2282.48 1481.64 Q2285.14 1478.89 2289.72 1474.26 Q2294.33 1469.61 2295.51 1468.27 Q2297.75 1465.74 2298.63 1464.01 Q2299.54 1462.25 2299.54 1460.56 Q2299.54 1457.8 2297.59 1456.07 Q2295.67 1454.33 2292.57 1454.33 Q2290.37 1454.33 2287.92 1455.09 Q2285.48 1455.86 2282.71 1457.41 L2282.71 1452.69 Q2285.53 1451.55 2287.98 1450.97 Q2290.44 1450.39 2292.48 1450.39 Q2297.85 1450.39 2301.04 1453.08 Q2304.23 1455.77 2304.23 1460.26 Q2304.23 1462.39 2303.42 1464.31 Q2302.64 1466.2 2300.53 1468.8 Q2299.95 1469.47 2296.85 1472.69 Q2293.75 1475.88 2288.1 1481.64 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M1321.59 1532.4 L1308.7 1549.74 L1322.26 1568.04 L1315.36 1568.04 L1304.98 1554.04 L1294.6 1568.04 L1287.7 1568.04 L1301.54 1549.39 L1288.87 1532.4 L1295.78 1532.4 L1305.23 1545.1 L1314.69 1532.4 L1321.59 1532.4 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip240)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"257.204,1423.18 257.204,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip240)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2352.76,1423.18 2352.76,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip240)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"257.204,1408.46 276.102,1408.46 \"/>\n",
       "<polyline clip-path=\"url(#clip240)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"257.204,1071.83 276.102,1071.83 \"/>\n",
       "<polyline clip-path=\"url(#clip240)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"257.204,735.212 276.102,735.212 \"/>\n",
       "<polyline clip-path=\"url(#clip240)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"257.204,398.59 276.102,398.59 \"/>\n",
       "<polyline clip-path=\"url(#clip240)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"257.204,61.9674 276.102,61.9674 \"/>\n",
       "<polyline clip-path=\"url(#clip240)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2352.76,1408.46 2333.86,1408.46 \"/>\n",
       "<polyline clip-path=\"url(#clip240)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2352.76,1071.83 2333.86,1071.83 \"/>\n",
       "<polyline clip-path=\"url(#clip240)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2352.76,735.212 2333.86,735.212 \"/>\n",
       "<polyline clip-path=\"url(#clip240)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2352.76,398.59 2333.86,398.59 \"/>\n",
       "<polyline clip-path=\"url(#clip240)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2352.76,61.9674 2333.86,61.9674 \"/>\n",
       "<path clip-path=\"url(#clip240)\" d=\"M114.26 1408.91 L143.936 1408.91 L143.936 1412.84 L114.26 1412.84 L114.26 1408.91 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M154.839 1421.8 L162.477 1421.8 L162.477 1395.44 L154.167 1397.1 L154.167 1392.84 L162.431 1391.18 L167.107 1391.18 L167.107 1421.8 L174.746 1421.8 L174.746 1425.74 L154.839 1425.74 L154.839 1421.8 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M184.19 1419.86 L189.075 1419.86 L189.075 1425.74 L184.19 1425.74 L184.19 1419.86 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M209.26 1394.26 Q205.649 1394.26 203.82 1397.82 Q202.014 1401.36 202.014 1408.49 Q202.014 1415.6 203.82 1419.16 Q205.649 1422.7 209.26 1422.7 Q212.894 1422.7 214.699 1419.16 Q216.528 1415.6 216.528 1408.49 Q216.528 1401.36 214.699 1397.82 Q212.894 1394.26 209.26 1394.26 M209.26 1390.55 Q215.07 1390.55 218.125 1395.16 Q221.204 1399.74 221.204 1408.49 Q221.204 1417.22 218.125 1421.82 Q215.07 1426.41 209.26 1426.41 Q203.449 1426.41 200.371 1421.82 Q197.315 1417.22 197.315 1408.49 Q197.315 1399.74 200.371 1395.16 Q203.449 1390.55 209.26 1390.55 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M115.256 1072.29 L144.931 1072.29 L144.931 1076.22 L115.256 1076.22 L115.256 1072.29 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M165.024 1057.63 Q161.413 1057.63 159.584 1061.2 Q157.778 1064.74 157.778 1071.87 Q157.778 1078.98 159.584 1082.54 Q161.413 1086.08 165.024 1086.08 Q168.658 1086.08 170.464 1082.54 Q172.292 1078.98 172.292 1071.87 Q172.292 1064.74 170.464 1061.2 Q168.658 1057.63 165.024 1057.63 M165.024 1053.93 Q170.834 1053.93 173.889 1058.54 Q176.968 1063.12 176.968 1071.87 Q176.968 1080.6 173.889 1085.2 Q170.834 1089.79 165.024 1089.79 Q159.214 1089.79 156.135 1085.2 Q153.079 1080.6 153.079 1071.87 Q153.079 1063.12 156.135 1058.54 Q159.214 1053.93 165.024 1053.93 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M185.186 1083.23 L190.07 1083.23 L190.07 1089.11 L185.186 1089.11 L185.186 1083.23 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M200.301 1054.55 L218.658 1054.55 L218.658 1058.49 L204.584 1058.49 L204.584 1066.96 Q205.602 1066.61 206.621 1066.45 Q207.639 1066.27 208.658 1066.27 Q214.445 1066.27 217.824 1069.44 Q221.204 1072.61 221.204 1078.03 Q221.204 1083.61 217.732 1086.71 Q214.26 1089.79 207.94 1089.79 Q205.764 1089.79 203.496 1089.42 Q201.25 1089.04 198.843 1088.3 L198.843 1083.61 Q200.926 1084.74 203.149 1085.29 Q205.371 1085.85 207.848 1085.85 Q211.852 1085.85 214.19 1083.74 Q216.528 1081.64 216.528 1078.03 Q216.528 1074.42 214.19 1072.31 Q211.852 1070.2 207.848 1070.2 Q205.973 1070.2 204.098 1070.62 Q202.246 1071.04 200.301 1071.92 L200.301 1054.55 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M164.028 721.011 Q160.417 721.011 158.589 724.575 Q156.783 728.117 156.783 735.247 Q156.783 742.353 158.589 745.918 Q160.417 749.46 164.028 749.46 Q167.663 749.46 169.468 745.918 Q171.297 742.353 171.297 735.247 Q171.297 728.117 169.468 724.575 Q167.663 721.011 164.028 721.011 M164.028 717.307 Q169.839 717.307 172.894 721.913 Q175.973 726.497 175.973 735.247 Q175.973 743.973 172.894 748.58 Q169.839 753.163 164.028 753.163 Q158.218 753.163 155.14 748.58 Q152.084 743.973 152.084 735.247 Q152.084 726.497 155.14 721.913 Q158.218 717.307 164.028 717.307 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M184.19 746.612 L189.075 746.612 L189.075 752.492 L184.19 752.492 L184.19 746.612 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M209.26 721.011 Q205.649 721.011 203.82 724.575 Q202.014 728.117 202.014 735.247 Q202.014 742.353 203.82 745.918 Q205.649 749.46 209.26 749.46 Q212.894 749.46 214.699 745.918 Q216.528 742.353 216.528 735.247 Q216.528 728.117 214.699 724.575 Q212.894 721.011 209.26 721.011 M209.26 717.307 Q215.07 717.307 218.125 721.913 Q221.204 726.497 221.204 735.247 Q221.204 743.973 218.125 748.58 Q215.07 753.163 209.26 753.163 Q203.449 753.163 200.371 748.58 Q197.315 743.973 197.315 735.247 Q197.315 726.497 200.371 721.913 Q203.449 717.307 209.26 717.307 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M165.024 384.388 Q161.413 384.388 159.584 387.953 Q157.778 391.495 157.778 398.624 Q157.778 405.731 159.584 409.296 Q161.413 412.837 165.024 412.837 Q168.658 412.837 170.464 409.296 Q172.292 405.731 172.292 398.624 Q172.292 391.495 170.464 387.953 Q168.658 384.388 165.024 384.388 M165.024 380.685 Q170.834 380.685 173.889 385.291 Q176.968 389.874 176.968 398.624 Q176.968 407.351 173.889 411.958 Q170.834 416.541 165.024 416.541 Q159.214 416.541 156.135 411.958 Q153.079 407.351 153.079 398.624 Q153.079 389.874 156.135 385.291 Q159.214 380.685 165.024 380.685 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M185.186 409.99 L190.07 409.99 L190.07 415.87 L185.186 415.87 L185.186 409.99 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M200.301 381.31 L218.658 381.31 L218.658 385.245 L204.584 385.245 L204.584 393.717 Q205.602 393.37 206.621 393.208 Q207.639 393.023 208.658 393.023 Q214.445 393.023 217.824 396.194 Q221.204 399.365 221.204 404.782 Q221.204 410.36 217.732 413.462 Q214.26 416.541 207.94 416.541 Q205.764 416.541 203.496 416.171 Q201.25 415.8 198.843 415.06 L198.843 410.36 Q200.926 411.495 203.149 412.05 Q205.371 412.606 207.848 412.606 Q211.852 412.606 214.19 410.499 Q216.528 408.393 216.528 404.782 Q216.528 401.171 214.19 399.064 Q211.852 396.958 207.848 396.958 Q205.973 396.958 204.098 397.374 Q202.246 397.791 200.301 398.671 L200.301 381.31 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M154.839 75.3123 L162.477 75.3123 L162.477 48.9467 L154.167 50.6133 L154.167 46.3541 L162.431 44.6874 L167.107 44.6874 L167.107 75.3123 L174.746 75.3123 L174.746 79.2474 L154.839 79.2474 L154.839 75.3123 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M184.19 73.3678 L189.075 73.3678 L189.075 79.2474 L184.19 79.2474 L184.19 73.3678 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M209.26 47.7661 Q205.649 47.7661 203.82 51.3309 Q202.014 54.8726 202.014 62.0022 Q202.014 69.1086 203.82 72.6734 Q205.649 76.215 209.26 76.215 Q212.894 76.215 214.699 72.6734 Q216.528 69.1086 216.528 62.0022 Q216.528 54.8726 214.699 51.3309 Q212.894 47.7661 209.26 47.7661 M209.26 44.0624 Q215.07 44.0624 218.125 48.6689 Q221.204 53.2522 221.204 62.0022 Q221.204 70.729 218.125 75.3354 Q215.07 79.9187 209.26 79.9187 Q203.449 79.9187 200.371 75.3354 Q197.315 70.729 197.315 62.0022 Q197.315 53.2522 200.371 48.6689 Q203.449 44.0624 209.26 44.0624 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M33.0032 779.629 Q33.0032 784.467 36.5043 787.077 Q40.1646 789.814 46.212 789.814 Q52.6095 789.814 56.3017 787.108 Q59.9619 784.371 59.9619 779.629 Q59.9619 774.95 56.2698 772.213 Q52.5777 769.475 46.212 769.475 Q40.3874 769.475 36.5043 772.213 Q33.0032 774.727 33.0032 779.629 M28.3562 779.629 L28.3562 760.181 L34.2127 760.181 L34.2127 766.738 Q39.1779 763.269 46.212 763.269 Q54.9649 763.269 59.9301 767.629 Q64.9272 771.99 64.9272 779.629 Q64.9272 787.299 59.9301 791.628 Q54.9649 795.989 46.212 795.989 Q37.3955 795.989 32.4621 791.628 Q28.3562 788.031 28.3562 779.629 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M14.5426 738.061 Q21.8632 742.326 29.0246 744.395 Q36.186 746.463 43.5384 746.463 Q50.8908 746.463 58.1159 744.395 Q65.3091 742.294 72.5979 738.061 L72.5979 743.153 Q65.1182 747.927 57.8931 750.315 Q50.668 752.67 43.5384 752.67 Q36.4406 752.67 29.2474 750.315 Q22.0542 747.959 14.5426 743.153 L14.5426 738.061 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M28.3562 697.065 L45.7028 709.956 L64.0042 696.397 L64.0042 703.304 L49.9996 713.68 L64.0042 724.056 L64.0042 730.963 L45.3526 717.117 L28.3562 729.785 L28.3562 722.878 L41.0558 713.425 L28.3562 703.972 L28.3562 697.065 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M14.5426 689.045 L14.5426 683.952 Q22.0542 679.178 29.2474 676.823 Q36.4406 674.435 43.5384 674.435 Q50.668 674.435 57.8931 676.823 Q65.1182 679.178 72.5979 683.952 L72.5979 689.045 Q65.3091 684.811 58.1159 682.743 Q50.8908 680.642 43.5384 680.642 Q36.186 680.642 29.0246 682.743 Q21.8632 684.811 14.5426 689.045 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip242)\" style=\"stroke:#000080; stroke-linecap:round; stroke-linejoin:round; stroke-width:12; stroke-opacity:1; fill:none\" points=\"316.512,1384.24 336.481,1382.24 356.45,1380.08 376.419,1377.75 396.388,1375.22 416.357,1372.5 436.326,1369.56 456.295,1366.39 476.264,1362.97 496.234,1359.28 516.203,1355.31 536.172,1351.03 556.141,1346.42 576.11,1341.45 596.079,1336.12 616.048,1330.38 636.017,1324.22 655.986,1317.61 675.955,1310.52 695.924,1302.93 715.893,1294.79 735.862,1286.1 755.831,1276.81 775.8,1266.89 795.769,1256.32 815.738,1245.07 835.707,1233.11 855.676,1220.42 875.645,1206.96 895.614,1192.72 915.584,1177.67 935.553,1161.81 955.522,1145.11 975.491,1127.57 995.46,1109.18 1015.43,1089.95 1035.4,1069.89 1055.37,1049 1075.34,1027.31 1095.3,1004.85 1115.27,981.644 1135.24,957.746 1155.21,933.202 1175.18,908.068 1195.15,882.407 1215.12,856.289 1235.09,829.789 1255.06,802.986 1275.03,775.965 1295,748.811 1314.96,721.613 1334.93,694.459 1354.9,667.438 1374.87,640.635 1394.84,614.135 1414.81,588.017 1434.78,562.356 1454.75,537.222 1474.72,512.678 1494.69,488.78 1514.65,465.579 1534.62,443.116 1554.59,421.426 1574.56,400.538 1594.53,380.472 1614.5,361.243 1634.47,342.857 1654.44,325.316 1674.41,308.617 1694.38,292.752 1714.35,277.707 1734.31,263.465 1754.28,250.008 1774.25,237.311 1794.22,225.351 1814.19,214.101 1834.16,203.532 1854.13,193.617 1874.1,184.326 1894.07,175.629 1914.04,167.497 1934,159.901 1953.97,152.811 1973.94,146.2 1993.91,140.04 2013.88,134.305 2033.85,128.969 2053.82,124.008 2073.79,119.397 2093.76,115.115 2113.73,111.14 2133.7,107.451 2153.66,104.031 2173.63,100.86 2193.6,97.9211 2213.57,95.199 2233.54,92.678 2253.51,90.3442 2273.48,88.1843 2293.45,86.1857 \"/>\n",
       "</svg>\n"
      ],
      "text/html": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip290\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip290)\" d=\"M0 1600 L2400 1600 L2400 8.88178e-14 L0 8.88178e-14  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip291\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip290)\" d=\"M257.204 1423.18 L2352.76 1423.18 L2352.76 47.2441 L257.204 47.2441  Z\" fill=\"#f2f2f2\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip292\">\n",
       "    <rect x=\"257\" y=\"47\" width=\"2097\" height=\"1377\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"316.512,1423.18 316.512,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"810.746,1423.18 810.746,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1304.98,1423.18 1304.98,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1799.21,1423.18 1799.21,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"2293.45,1423.18 2293.45,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"316.512,1423.18 316.512,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"810.746,1423.18 810.746,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1304.98,1423.18 1304.98,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1799.21,1423.18 1799.21,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"2293.45,1423.18 2293.45,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"257.204,1408.46 2352.76,1408.46 \"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"257.204,1071.83 2352.76,1071.83 \"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"257.204,735.212 2352.76,735.212 \"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"257.204,398.59 2352.76,398.59 \"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"257.204,61.9674 2352.76,61.9674 \"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"257.204,1408.46 2352.76,1408.46 \"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"257.204,1071.83 2352.76,1071.83 \"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"257.204,735.212 2352.76,735.212 \"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"257.204,398.59 2352.76,398.59 \"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"257.204,61.9674 2352.76,61.9674 \"/>\n",
       "<polyline clip-path=\"url(#clip290)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"257.204,1423.18 2352.76,1423.18 \"/>\n",
       "<polyline clip-path=\"url(#clip290)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"257.204,47.2441 2352.76,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip290)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"316.512,1423.18 316.512,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip290)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"810.746,1423.18 810.746,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip290)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1304.98,1423.18 1304.98,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip290)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1799.21,1423.18 1799.21,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip290)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2293.45,1423.18 2293.45,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip290)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"316.512,47.2441 316.512,66.1417 \"/>\n",
       "<polyline clip-path=\"url(#clip290)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"810.746,47.2441 810.746,66.1417 \"/>\n",
       "<polyline clip-path=\"url(#clip290)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1304.98,47.2441 1304.98,66.1417 \"/>\n",
       "<polyline clip-path=\"url(#clip290)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1799.21,47.2441 1799.21,66.1417 \"/>\n",
       "<polyline clip-path=\"url(#clip290)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2293.45,47.2441 2293.45,66.1417 \"/>\n",
       "<path clip-path=\"url(#clip290)\" d=\"M286.454 1468.75 L316.13 1468.75 L316.13 1472.69 L286.454 1472.69 L286.454 1468.75 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M330.25 1481.64 L346.57 1481.64 L346.57 1485.58 L324.625 1485.58 L324.625 1481.64 Q327.288 1478.89 331.871 1474.26 Q336.477 1469.61 337.658 1468.27 Q339.903 1465.74 340.783 1464.01 Q341.686 1462.25 341.686 1460.56 Q341.686 1457.8 339.741 1456.07 Q337.82 1454.33 334.718 1454.33 Q332.519 1454.33 330.065 1455.09 Q327.635 1455.86 324.857 1457.41 L324.857 1452.69 Q327.681 1451.55 330.135 1450.97 Q332.588 1450.39 334.625 1450.39 Q339.996 1450.39 343.19 1453.08 Q346.385 1455.77 346.385 1460.26 Q346.385 1462.39 345.574 1464.31 Q344.787 1466.2 342.681 1468.8 Q342.102 1469.47 339 1472.69 Q335.899 1475.88 330.25 1481.64 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M780.503 1468.75 L810.179 1468.75 L810.179 1472.69 L780.503 1472.69 L780.503 1468.75 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M821.082 1481.64 L828.72 1481.64 L828.72 1455.28 L820.41 1456.95 L820.41 1452.69 L828.674 1451.02 L833.35 1451.02 L833.35 1481.64 L840.989 1481.64 L840.989 1485.58 L821.082 1485.58 L821.082 1481.64 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M1304.98 1454.1 Q1301.37 1454.1 1299.54 1457.66 Q1297.73 1461.2 1297.73 1468.33 Q1297.73 1475.44 1299.54 1479.01 Q1301.37 1482.55 1304.98 1482.55 Q1308.61 1482.55 1310.42 1479.01 Q1312.25 1475.44 1312.25 1468.33 Q1312.25 1461.2 1310.42 1457.66 Q1308.61 1454.1 1304.98 1454.1 M1304.98 1450.39 Q1310.79 1450.39 1313.85 1455 Q1316.92 1459.58 1316.92 1468.33 Q1316.92 1477.06 1313.85 1481.67 Q1310.79 1486.25 1304.98 1486.25 Q1299.17 1486.25 1296.09 1481.67 Q1293.04 1477.06 1293.04 1468.33 Q1293.04 1459.58 1296.09 1455 Q1299.17 1450.39 1304.98 1450.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M1789.6 1481.64 L1797.23 1481.64 L1797.23 1455.28 L1788.92 1456.95 L1788.92 1452.69 L1797.19 1451.02 L1801.86 1451.02 L1801.86 1481.64 L1809.5 1481.64 L1809.5 1485.58 L1789.6 1485.58 L1789.6 1481.64 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M2288.1 1481.64 L2304.42 1481.64 L2304.42 1485.58 L2282.48 1485.58 L2282.48 1481.64 Q2285.14 1478.89 2289.72 1474.26 Q2294.33 1469.61 2295.51 1468.27 Q2297.75 1465.74 2298.63 1464.01 Q2299.54 1462.25 2299.54 1460.56 Q2299.54 1457.8 2297.59 1456.07 Q2295.67 1454.33 2292.57 1454.33 Q2290.37 1454.33 2287.92 1455.09 Q2285.48 1455.86 2282.71 1457.41 L2282.71 1452.69 Q2285.53 1451.55 2287.98 1450.97 Q2290.44 1450.39 2292.48 1450.39 Q2297.85 1450.39 2301.04 1453.08 Q2304.23 1455.77 2304.23 1460.26 Q2304.23 1462.39 2303.42 1464.31 Q2302.64 1466.2 2300.53 1468.8 Q2299.95 1469.47 2296.85 1472.69 Q2293.75 1475.88 2288.1 1481.64 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M1321.59 1532.4 L1308.7 1549.74 L1322.26 1568.04 L1315.36 1568.04 L1304.98 1554.04 L1294.6 1568.04 L1287.7 1568.04 L1301.54 1549.39 L1288.87 1532.4 L1295.78 1532.4 L1305.23 1545.1 L1314.69 1532.4 L1321.59 1532.4 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip290)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"257.204,1423.18 257.204,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip290)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2352.76,1423.18 2352.76,47.2441 \"/>\n",
       "<polyline clip-path=\"url(#clip290)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"257.204,1408.46 276.102,1408.46 \"/>\n",
       "<polyline clip-path=\"url(#clip290)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"257.204,1071.83 276.102,1071.83 \"/>\n",
       "<polyline clip-path=\"url(#clip290)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"257.204,735.212 276.102,735.212 \"/>\n",
       "<polyline clip-path=\"url(#clip290)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"257.204,398.59 276.102,398.59 \"/>\n",
       "<polyline clip-path=\"url(#clip290)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"257.204,61.9674 276.102,61.9674 \"/>\n",
       "<polyline clip-path=\"url(#clip290)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2352.76,1408.46 2333.86,1408.46 \"/>\n",
       "<polyline clip-path=\"url(#clip290)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2352.76,1071.83 2333.86,1071.83 \"/>\n",
       "<polyline clip-path=\"url(#clip290)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2352.76,735.212 2333.86,735.212 \"/>\n",
       "<polyline clip-path=\"url(#clip290)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2352.76,398.59 2333.86,398.59 \"/>\n",
       "<polyline clip-path=\"url(#clip290)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2352.76,61.9674 2333.86,61.9674 \"/>\n",
       "<path clip-path=\"url(#clip290)\" d=\"M114.26 1408.91 L143.936 1408.91 L143.936 1412.84 L114.26 1412.84 L114.26 1408.91 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M154.839 1421.8 L162.477 1421.8 L162.477 1395.44 L154.167 1397.1 L154.167 1392.84 L162.431 1391.18 L167.107 1391.18 L167.107 1421.8 L174.746 1421.8 L174.746 1425.74 L154.839 1425.74 L154.839 1421.8 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M184.19 1419.86 L189.075 1419.86 L189.075 1425.74 L184.19 1425.74 L184.19 1419.86 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M209.26 1394.26 Q205.649 1394.26 203.82 1397.82 Q202.014 1401.36 202.014 1408.49 Q202.014 1415.6 203.82 1419.16 Q205.649 1422.7 209.26 1422.7 Q212.894 1422.7 214.699 1419.16 Q216.528 1415.6 216.528 1408.49 Q216.528 1401.36 214.699 1397.82 Q212.894 1394.26 209.26 1394.26 M209.26 1390.55 Q215.07 1390.55 218.125 1395.16 Q221.204 1399.74 221.204 1408.49 Q221.204 1417.22 218.125 1421.82 Q215.07 1426.41 209.26 1426.41 Q203.449 1426.41 200.371 1421.82 Q197.315 1417.22 197.315 1408.49 Q197.315 1399.74 200.371 1395.16 Q203.449 1390.55 209.26 1390.55 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M115.256 1072.29 L144.931 1072.29 L144.931 1076.22 L115.256 1076.22 L115.256 1072.29 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M165.024 1057.63 Q161.413 1057.63 159.584 1061.2 Q157.778 1064.74 157.778 1071.87 Q157.778 1078.98 159.584 1082.54 Q161.413 1086.08 165.024 1086.08 Q168.658 1086.08 170.464 1082.54 Q172.292 1078.98 172.292 1071.87 Q172.292 1064.74 170.464 1061.2 Q168.658 1057.63 165.024 1057.63 M165.024 1053.93 Q170.834 1053.93 173.889 1058.54 Q176.968 1063.12 176.968 1071.87 Q176.968 1080.6 173.889 1085.2 Q170.834 1089.79 165.024 1089.79 Q159.214 1089.79 156.135 1085.2 Q153.079 1080.6 153.079 1071.87 Q153.079 1063.12 156.135 1058.54 Q159.214 1053.93 165.024 1053.93 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M185.186 1083.23 L190.07 1083.23 L190.07 1089.11 L185.186 1089.11 L185.186 1083.23 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M200.301 1054.55 L218.658 1054.55 L218.658 1058.49 L204.584 1058.49 L204.584 1066.96 Q205.602 1066.61 206.621 1066.45 Q207.639 1066.27 208.658 1066.27 Q214.445 1066.27 217.824 1069.44 Q221.204 1072.61 221.204 1078.03 Q221.204 1083.61 217.732 1086.71 Q214.26 1089.79 207.94 1089.79 Q205.764 1089.79 203.496 1089.42 Q201.25 1089.04 198.843 1088.3 L198.843 1083.61 Q200.926 1084.74 203.149 1085.29 Q205.371 1085.85 207.848 1085.85 Q211.852 1085.85 214.19 1083.74 Q216.528 1081.64 216.528 1078.03 Q216.528 1074.42 214.19 1072.31 Q211.852 1070.2 207.848 1070.2 Q205.973 1070.2 204.098 1070.62 Q202.246 1071.04 200.301 1071.92 L200.301 1054.55 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M164.028 721.011 Q160.417 721.011 158.589 724.575 Q156.783 728.117 156.783 735.247 Q156.783 742.353 158.589 745.918 Q160.417 749.46 164.028 749.46 Q167.663 749.46 169.468 745.918 Q171.297 742.353 171.297 735.247 Q171.297 728.117 169.468 724.575 Q167.663 721.011 164.028 721.011 M164.028 717.307 Q169.839 717.307 172.894 721.913 Q175.973 726.497 175.973 735.247 Q175.973 743.973 172.894 748.58 Q169.839 753.163 164.028 753.163 Q158.218 753.163 155.14 748.58 Q152.084 743.973 152.084 735.247 Q152.084 726.497 155.14 721.913 Q158.218 717.307 164.028 717.307 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M184.19 746.612 L189.075 746.612 L189.075 752.492 L184.19 752.492 L184.19 746.612 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M209.26 721.011 Q205.649 721.011 203.82 724.575 Q202.014 728.117 202.014 735.247 Q202.014 742.353 203.82 745.918 Q205.649 749.46 209.26 749.46 Q212.894 749.46 214.699 745.918 Q216.528 742.353 216.528 735.247 Q216.528 728.117 214.699 724.575 Q212.894 721.011 209.26 721.011 M209.26 717.307 Q215.07 717.307 218.125 721.913 Q221.204 726.497 221.204 735.247 Q221.204 743.973 218.125 748.58 Q215.07 753.163 209.26 753.163 Q203.449 753.163 200.371 748.58 Q197.315 743.973 197.315 735.247 Q197.315 726.497 200.371 721.913 Q203.449 717.307 209.26 717.307 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M165.024 384.388 Q161.413 384.388 159.584 387.953 Q157.778 391.495 157.778 398.624 Q157.778 405.731 159.584 409.296 Q161.413 412.837 165.024 412.837 Q168.658 412.837 170.464 409.296 Q172.292 405.731 172.292 398.624 Q172.292 391.495 170.464 387.953 Q168.658 384.388 165.024 384.388 M165.024 380.685 Q170.834 380.685 173.889 385.291 Q176.968 389.874 176.968 398.624 Q176.968 407.351 173.889 411.958 Q170.834 416.541 165.024 416.541 Q159.214 416.541 156.135 411.958 Q153.079 407.351 153.079 398.624 Q153.079 389.874 156.135 385.291 Q159.214 380.685 165.024 380.685 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M185.186 409.99 L190.07 409.99 L190.07 415.87 L185.186 415.87 L185.186 409.99 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M200.301 381.31 L218.658 381.31 L218.658 385.245 L204.584 385.245 L204.584 393.717 Q205.602 393.37 206.621 393.208 Q207.639 393.023 208.658 393.023 Q214.445 393.023 217.824 396.194 Q221.204 399.365 221.204 404.782 Q221.204 410.36 217.732 413.462 Q214.26 416.541 207.94 416.541 Q205.764 416.541 203.496 416.171 Q201.25 415.8 198.843 415.06 L198.843 410.36 Q200.926 411.495 203.149 412.05 Q205.371 412.606 207.848 412.606 Q211.852 412.606 214.19 410.499 Q216.528 408.393 216.528 404.782 Q216.528 401.171 214.19 399.064 Q211.852 396.958 207.848 396.958 Q205.973 396.958 204.098 397.374 Q202.246 397.791 200.301 398.671 L200.301 381.31 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M154.839 75.3123 L162.477 75.3123 L162.477 48.9467 L154.167 50.6133 L154.167 46.3541 L162.431 44.6874 L167.107 44.6874 L167.107 75.3123 L174.746 75.3123 L174.746 79.2474 L154.839 79.2474 L154.839 75.3123 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M184.19 73.3678 L189.075 73.3678 L189.075 79.2474 L184.19 79.2474 L184.19 73.3678 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M209.26 47.7661 Q205.649 47.7661 203.82 51.3309 Q202.014 54.8726 202.014 62.0022 Q202.014 69.1086 203.82 72.6734 Q205.649 76.215 209.26 76.215 Q212.894 76.215 214.699 72.6734 Q216.528 69.1086 216.528 62.0022 Q216.528 54.8726 214.699 51.3309 Q212.894 47.7661 209.26 47.7661 M209.26 44.0624 Q215.07 44.0624 218.125 48.6689 Q221.204 53.2522 221.204 62.0022 Q221.204 70.729 218.125 75.3354 Q215.07 79.9187 209.26 79.9187 Q203.449 79.9187 200.371 75.3354 Q197.315 70.729 197.315 62.0022 Q197.315 53.2522 200.371 48.6689 Q203.449 44.0624 209.26 44.0624 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M33.0032 779.629 Q33.0032 784.467 36.5043 787.077 Q40.1646 789.814 46.212 789.814 Q52.6095 789.814 56.3017 787.108 Q59.9619 784.371 59.9619 779.629 Q59.9619 774.95 56.2698 772.213 Q52.5777 769.475 46.212 769.475 Q40.3874 769.475 36.5043 772.213 Q33.0032 774.727 33.0032 779.629 M28.3562 779.629 L28.3562 760.181 L34.2127 760.181 L34.2127 766.738 Q39.1779 763.269 46.212 763.269 Q54.9649 763.269 59.9301 767.629 Q64.9272 771.99 64.9272 779.629 Q64.9272 787.299 59.9301 791.628 Q54.9649 795.989 46.212 795.989 Q37.3955 795.989 32.4621 791.628 Q28.3562 788.031 28.3562 779.629 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M14.5426 738.061 Q21.8632 742.326 29.0246 744.395 Q36.186 746.463 43.5384 746.463 Q50.8908 746.463 58.1159 744.395 Q65.3091 742.294 72.5979 738.061 L72.5979 743.153 Q65.1182 747.927 57.8931 750.315 Q50.668 752.67 43.5384 752.67 Q36.4406 752.67 29.2474 750.315 Q22.0542 747.959 14.5426 743.153 L14.5426 738.061 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M28.3562 697.065 L45.7028 709.956 L64.0042 696.397 L64.0042 703.304 L49.9996 713.68 L64.0042 724.056 L64.0042 730.963 L45.3526 717.117 L28.3562 729.785 L28.3562 722.878 L41.0558 713.425 L28.3562 703.972 L28.3562 697.065 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M14.5426 689.045 L14.5426 683.952 Q22.0542 679.178 29.2474 676.823 Q36.4406 674.435 43.5384 674.435 Q50.668 674.435 57.8931 676.823 Q65.1182 679.178 72.5979 683.952 L72.5979 689.045 Q65.3091 684.811 58.1159 682.743 Q50.8908 680.642 43.5384 680.642 Q36.186 680.642 29.0246 682.743 Q21.8632 684.811 14.5426 689.045 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip292)\" style=\"stroke:#000080; stroke-linecap:round; stroke-linejoin:round; stroke-width:12; stroke-opacity:1; fill:none\" points=\"316.512,1384.24 336.481,1382.24 356.45,1380.08 376.419,1377.75 396.388,1375.22 416.357,1372.5 436.326,1369.56 456.295,1366.39 476.264,1362.97 496.234,1359.28 516.203,1355.31 536.172,1351.03 556.141,1346.42 576.11,1341.45 596.079,1336.12 616.048,1330.38 636.017,1324.22 655.986,1317.61 675.955,1310.52 695.924,1302.93 715.893,1294.79 735.862,1286.1 755.831,1276.81 775.8,1266.89 795.769,1256.32 815.738,1245.07 835.707,1233.11 855.676,1220.42 875.645,1206.96 895.614,1192.72 915.584,1177.67 935.553,1161.81 955.522,1145.11 975.491,1127.57 995.46,1109.18 1015.43,1089.95 1035.4,1069.89 1055.37,1049 1075.34,1027.31 1095.3,1004.85 1115.27,981.644 1135.24,957.746 1155.21,933.202 1175.18,908.068 1195.15,882.407 1215.12,856.289 1235.09,829.789 1255.06,802.986 1275.03,775.965 1295,748.811 1314.96,721.613 1334.93,694.459 1354.9,667.438 1374.87,640.635 1394.84,614.135 1414.81,588.017 1434.78,562.356 1454.75,537.222 1474.72,512.678 1494.69,488.78 1514.65,465.579 1534.62,443.116 1554.59,421.426 1574.56,400.538 1594.53,380.472 1614.5,361.243 1634.47,342.857 1654.44,325.316 1674.41,308.617 1694.38,292.752 1714.35,277.707 1734.31,263.465 1754.28,250.008 1774.25,237.311 1794.22,225.351 1814.19,214.101 1834.16,203.532 1854.13,193.617 1874.1,184.326 1894.07,175.629 1914.04,167.497 1934,159.901 1953.97,152.811 1973.94,146.2 1993.91,140.04 2013.88,134.305 2033.85,128.969 2053.82,124.008 2073.79,119.397 2093.76,115.115 2113.73,111.14 2133.7,107.451 2153.66,104.031 2173.63,100.86 2193.6,97.9211 2213.57,95.199 2233.54,92.678 2253.51,90.3442 2273.48,88.1843 2293.45,86.1857 \"/>\n",
       "</svg>\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "let\n",
    "\n",
    "    # initialize -\n",
    "    x = range(-2, 2, length=100) |> collect .|> Float32; # notice: .|> Float32. What?\n",
    "\n",
    "    # what activation function to use? We'll use the NNlib package for this.\n",
    "    y = x |> NNlib.tanh_fast |> collect; # \n",
    "\n",
    "    # plot -\n",
    "    p = plot(bg=\"gray95\", background_color_outside=\"white\", framestyle = :box, fg_legend = :transparent); \n",
    "    plot!(x, y, xlabel=\"x\", ylabel=\"σ(x)\", legend=:topright, label=\"\", lw=3, c=:navy)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1369fabd",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff9f079",
   "metadata": {},
   "source": [
    "<img\n",
    "  src=\"figs/nn-4.svg\"\n",
    "  alt=\"triangle with all three sides equal\"\n",
    "  height=\"400\"\n",
    "  width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e2828e",
   "metadata": {},
   "source": [
    "## Feedforward Neural Networks\n",
    "Let's consider the simple network shown above. The network has three layers: an input layer (five nodes), a hidden layer (12 nodes), and an output layer (three nodes). \n",
    "\n",
    "* __Hidden Layer__: The hidden layer performs computations on the input data. Each node in the hidden layer takes the input vector $\\mathbf{x}$ and applies a linear transformation followed by a non-linear activation function. The output of each node in the hidden layer is then passed to the next layer (in this case, the output layer). The number of nodes in the hidden layer can vary depending on the complexity of the task. Furthermore, the hidden layer can have multiple layers (i.e., deep neural networks) to learn more complex data representations. The example network has a single hidden layer with 12 nodes.\n",
    "* __Output Layer__: The output layer is the final layer of the network that produces the output of the network $\\mathbf{y} = \\left\\{y_{1},y_{2},\\dots,y_{k}\\right\\}$ where $\\mathbf{y}\\in\\mathbb{R}^{k}$. Each node in the output layer takes the output from the hidden layer and applies a linear transformation followed by a non-linear activation function. In this example, we have three output nodes. For instance, we could predict three classes in a multiclass classification task. In this case, the network output can be interpreted as probabilities for each class, and we can use techniques like softmax to convert the output into probabilities.\n",
    "\n",
    "Let's generalize this example to a more formal definition of a feedforward neural network.\n",
    "\n",
    "### Function Composition\n",
    "Suppose we have a feedforward neural network with $L$ layers. The network has $n$ input nodes (we'll call this layer 0), and $i=1,2,\\dots, L-1$ hidden layers where each hidden layer has $m_{i}$ nodes, and the output (layer $L$) has $d_{out}$ output nodes.  Each hidden layer is fully connected to the previous and subsequent layers (but there are no connections between the nodes inside a layer and no self-connections). Information flows from the input to the output layer, forming a feedforward structure.\n",
    "\n",
    "Let's dig into the states and parameters of the network.\n",
    "\n",
    "* __Inputs and outputs__: Let $\\mathbf{x} = \\left\\{x_{1},x_{2},\\dots,x_{d_{in}},1\\right\\}$ be the _augmented_ input vector, where $x_{i}\\in\\mathbb{R}$ is the $i$-th feature of the input vector. The dimension of the _augmented_ input vector is $d_{in} + 1$, where $d_{in}$ is the number of features in the input vector. The extra `1` is added to the input vector to allow us to include the bias term in the weight vector. This is a common technique used in machine learning to simplify the representation of the model. Further, let $\\mathbf{z}_{i} = \\left\\{z^{(i)}_{1},z^{(i)}_{2},\\dots,z^{(i)}_{m_{i}}\\right\\}$ be the output vector of the $i$-th hidden layer, where $z^{(i)}_{j}\\in\\mathbb{R}$ is the $j$-th component of the output of layer $i$. Finally, let $\\mathbf{y}_{k} = \\left\\{y_{1},y_{2},\\dots,y_{d_{out}}\\right\\}$ be the output vector, where $y_{k}\\in\\mathbb{R}$ is the $k$-th component of the output of the network.\n",
    "\n",
    "* __Parameters__: Each node $j=1,2,\\dots,m_{i}$ in layer $i\\geq{1}$ has a parameter vector $\\mathbf{w}^{(i)}_{j} = \\left(w^{(i)}_{j,1},w^{(i)}_{j,2},\\dots,w^{(i)}_{j,m_{i-1}}, b^{(i)}_{j}\\right)$, where $w^{(i)}_{j,k}\\in\\mathbb{R}$ is the weight of the $k$-th input to node $j$ in layer $i$, and $b^{(i)}_{j}\\in\\mathbb{R}$ is the bias term for node $j$ in layer $i$. The weight vector $\\mathbf{w}^{(i)}_{j}$ represents the strength of the connection between node $j$ in layer $i$ and all nodes in layer $i-1$. The bias term $b_{i}$ allows the model to shift the activation function to the left or right.\n",
    "\n",
    "This may seem confusing, so let's think about this differently. A feedforward neural network can be considered a series of function compositions. For example, consider layer $1$ with $m_{1}$ nodes. The output of layer $1$ (given the input vector $\\mathbf{z}_{\\circ}$) is given by:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{z}^{(1)} &= \\begin{bmatrix}\n",
    "\\sigma_{1}\\left(\\mathbf{z}_{\\circ}^{\\top}\\cdot\\mathbf{w}^{(1)}_{1}\\right) \\\\\n",
    "\\sigma_{1}\\left(\\mathbf{z}_{\\circ}^{\\top}\\cdot\\mathbf{w}^{(1)}_{2}\\right) \\\\\n",
    "\\vdots \\\\\n",
    "\\sigma_{1}\\left(\\mathbf{z}_{\\circ}^{\\top}\\cdot\\mathbf{w}^{(1)}_{m_{1}}\\right)\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\mathbf{w}^{(1)}_{j}$ is the weight vector for node $j$ in layer $1$, and $\\sigma_{1}$ is the activation function for layer $1$ (assumed to be the same for nodes in layer $1$). The output of layer $1$ is a vector $\\mathbf{z}^{(1)}\\in\\mathbb{R}^{m_{1}}$ which is then passed to layer $2$, which has $m_{2}$ nodes:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{z}^{(2)} &= \\begin{bmatrix}\n",
    "\\sigma_{2}\\left(\\mathbf{z}_{1}^{\\top}\\cdot\\mathbf{w}^{(2)}_{1}\\right) \\\\\n",
    "\\sigma_{2}\\left(\\mathbf{z}_{1}^{\\top}\\cdot\\mathbf{w}^{(2)}_{2}\\right) \\\\\n",
    "\\vdots \\\\\n",
    "\\sigma_{2}\\left(\\mathbf{z}_{1}^{\\top}\\cdot\\mathbf{w}^{(2)}_{m_{2}}\\right)\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\mathbf{w}^{(2)}_{j}$ is the weight vector for node $j$ in layer $2$, and $\\sigma_{2}$ is the activation function for layer $2$ (assumed to be the same for nodes in layer $2$). However, we can also think of the output of layer $2$ as: $\\mathbf{z}^{(2)} = \\sigma_{2}\\circ\\sigma_{1}\\left(\\mathbf{z}_{\\circ}\\right)$, where $\\sigma_{2}\\circ\\sigma_{1}$ is the composition of the two activation functions, i.e., $\\mathbf{z}^{(2)} = \\sigma_{2}\\left(\\sigma_{1}\\left(\\mathbf{z}_{\\circ}\\right)\\right)$. Putting these ideas together, gives a nice way to think about a feedforward neural network: a series of function compositions:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{\\mathbf{y}} &= f_{\\theta}(\\mathbf{x}) = \\sigma_{L}\\circ\\sigma_{L-1}\\circ\\dots\\circ\\sigma_{1}\\left(\\mathbf{x}\\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\sigma_{L}$ is the activation function for the output layer, $\\mathbf{x}$ is the _augmented_ input vector, and $\\hat{\\mathbf{y}}\\in\\mathbb{R}^{d_{out}}$ is the output of the network. The function $f_{\\theta}(\\mathbf{x})$ represents the mapping from the input vector $\\mathbf{x}=\\mathbf{z}_{\\circ}$ to the output vector $\\hat{\\mathbf{y}}$, and $\\theta$ represents the parameters of the network (i.e., the weights and biases). \n",
    "\n",
    "__Wow!__ A feedforward neural network $f_{\\theta}:\\mathbb{R}^{d_{in}}\\rightarrow\\mathbb{R}^{d_{out}}$ is just _some complicated function_ that takes an input vector $\\mathbf{x}$ and produces an output vector $\\hat{\\mathbf{y}}$. Thus, we can do everything we do with functions: compose them, take their derivatives, and so on. This is a compelling idea because it allows us to use all the tools of calculus and linear algebra to analyze and optimize neural networks.\n",
    "\n",
    "### Parameterization\n",
    "Before we move on, let's take a moment to think about the parameters of the network. The parameters of the network are the weights and biases of each node. Each layer $i$ has $m_{i}$ nodes, and each node in layer $i$ has a _weight vector_ $\\mathbf{w}^{(i)}_{j} = \\left(w^{(i)}_{j,1},w^{(i)}_{j,2},\\dots,w^{(i)}_{j,m_{i-1}}, b^{(i)}_{j}\\right)$, where $w^{(i)}_{j,k}\\in\\mathbb{R}$ is the weight of the $k$-th input to node $j$ in layer $i$, and $b^{(i)}_{j}\\in\\mathbb{R}$ is the bias term for node $j$ in layer $i$. \n",
    "\n",
    "We represent the parameters of _each_ layer $i$ in the matrix $\\mathbf{W}_{i}\\in\\mathbb{R}^{m_{i}\\times(m_{i-1}+1)}$. The weight matrix $\\mathbf{W}_{i}$ contains the weights and biases for all nodes in layer $i$. Thus, we can pack all the parameters $\\left(\\mathbf{W}_{1},\\mathbf{W}_{2},\\dots,\\mathbf{W}_{L}\\right)$ into the $\\theta$ vector:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\theta &\\equiv  \\left(w^{(1)}_{1,1},w^{(1)}_{1,2},\\dots,w^{(1)}_{1,m_{0}}, b^{(1)}_{1}, w^{(1)}_{2,1},w^{(1)}_{2,2},\\dots,w^{(1)}_{2,m_{0}}, b^{(1)}_{2}, \\ldots, w^{(L)}_{k,1},w^{(L)}_{k,2},\\dots,w^{(L)}_{k,m_{L-1}}, b^{(L)}_{k}\\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "where $k=1,2,\\dots,d_{out}$ is the index of the output node. \n",
    "\n",
    "#### How big is $\\theta$?\n",
    "The number of parameters in a feedforward neural network depends on the number of layers and nodes in each layer. Suppose we have a feedforward neural network with $L$ layers, where the $i$-th layer has $m_{i}$ nodes. The number of parameters in the network is given by:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Number of parameters} &= \\sum_{i=1}^{L} m_{i}\\left(m_{i-1}+1\\right) \\\\\n",
    "&= \\sum_{i=1}^{L} \\left(m_{i} m_{i-1} + m_{i}\\right) \\\\\n",
    "&= \\underbrace{\\sum_{i=1}^{L} m_{i} m_{i-1}}_{\\text{weights}} + \\underbrace{\\sum_{i=1}^{L} m_{i}}_{\\text{bias terms}} \\quad \\blacksquare \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "where $m_{0}$ is the number of input nodes, and $m_{L}$ is the number of output nodes. The first term counts the number of weight parameters in the network, while the second term counts the number of biases in the network.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33115fe",
   "metadata": {},
   "source": [
    "### Example\n",
    "Let's do a simple example to illustrate the concepts we've discussed. This network won't win any prizes or do anything useful, but it will help us understand the basic concepts of feedforward neural networks.\n",
    "\n",
    "Start by specifying the input to the network in the `x::Array{Float64}` variable. \n",
    "* The input to the network is a vector of binary or continuous-valued features representing the data we want to process. Each component of the input vector corresponds to a feature of the data. For example, in an image classification task, the input vector could represent the pixel values of an image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "689e5230",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(-2, 2, length=11) |> collect .|> Float32; # notice: .|> Float32. What?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9023cd",
   "metadata": {},
   "source": [
    "Next, let's consider the processing in a single network layer. For example, suppose we have an input dimension of $d_{in} = 11$ and a hidden layer with $m_{1} = 21$ nodes. We'll pick some activation function (e.g., the sigmoid or relu function [from the `NNlib.jl` package](https://fluxml.ai/NNlib.jl/dev/)) and then compute the output of the hidden layer.\n",
    "* _What activation function should we use?_ The choice of activation function is important because it affects the learning process and the neural network's performance. There are various activation functions, each with its own characteristics and applications. We can make this design choice when we build the network. \n",
    "* _What should the weights and biases be?_ The weights and biases are the network parameters we will learn during training. Let's pick some random values for the weights and biases for now. This gets us started, and we can refine the values later. When we build [a `MyLayerModel` instance](src/Types.jl) these parameters will be initialized to random values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f3559e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "z, testmodel = let\n",
    "\n",
    "    # let's build a simple neural network with some hidden layers\n",
    "    n = length(x); # number of inputs (zₒ)\n",
    "    m = 21; # number of hidden neurons\n",
    "\n",
    "    # build  model -\n",
    "    model = build(MyLayerModel, (\n",
    "        n = n, # number of inputs\n",
    "        m = m, # number of hidden neurons\n",
    "        σ = NNlib.relu, # activation function (pick one from NNlib)\n",
    "    ));\n",
    "\n",
    "    # evaluate this layer\n",
    "    z = model(x); # evaluate the model with this input\n",
    "\n",
    "    # return -\n",
    "    z, model\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871513db",
   "metadata": {},
   "source": [
    "That's sort of interesting! Let's see what multiple layers look like (and how we can construct such a creature with our simple codebase)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43850809",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z, models = let\n",
    "\n",
    "    # initialize -\n",
    "    L = 3; # How many layers do we want?\n",
    "    σ = [NNlib.tanh_fast, NNlib.relu, NNlib.sigmoid]; # activation function (pick one from NNlib)\n",
    "    n = length(x); # number of inputs (zₒ)\n",
    "\n",
    "    # Dimensions of each layer -\n",
    "    m = Dict{Int, Int}();\n",
    "    m[0] = n; # dimension of the inputs (zₒ) (including the bias at the end)\n",
    "    for i in 1:(L-1)\n",
    "        m[i] = 2*m[0]; # number of hidden neurons, make this some mutiple of the input size\n",
    "    end\n",
    "    m[L] = 1; # number of outputs (zₗ)\n",
    "\n",
    "    # build models dictionary -\n",
    "    models = Dict{Int, MyLayerModel}();\n",
    "    for i in 1:L\n",
    "        \n",
    "        models[i] = build(MyLayerModel, (\n",
    "            n = m[i-1], # number of inputs\n",
    "            m = m[i], # number of hidden neurons\n",
    "            σ = σ[i], # activation function (pick one from NNlib)\n",
    "        ));\n",
    "    end;\n",
    "    \n",
    "    # compute the output of each layer -\n",
    "    z = Dict{Int, Array{Float32}}();\n",
    "    z[0] = x;\n",
    "    for i in 1:L\n",
    "        z[i] = models[i](z[i-1]); # evaluate the layer\n",
    "    end\n",
    "\n",
    "    z, models; # return the output of each layer \n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1da223",
   "metadata": {},
   "source": [
    "__Check__: Is our parameter count correct? Let's check this by hand and then compare it to the code. We'll hardcode the values of the various dimensions of the network in the example above; if you change them, you'll need to change the calculation below.\n",
    "* __Input layer__: $d_{in} = 11$ (11 input nodes)\n",
    "* __Hidden layer 1__: $m_{1} = 22$ (22 hidden nodes)\n",
    "* __Hidden layer 2__: $m_{2} = 22$ (22 hidden nodes)\n",
    "* __Output layer__: $d_{out} = 1$ (1 output node)\n",
    "\n",
    "which gives the following total number of parameters:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Number of parameters} &= \\sum_{i=1}^{L} m_{i}\\left(m_{i-1}+1\\right) \\\\\n",
    "&= \\sum_{i=1}^{L} m_{i} m_{i-1} + \\sum_{i=1}^{L} m_{i} \\\\\n",
    "& = m_{0} m_{1} + m_{1} m_{2} + m_{2}m_{3} + m_{1}+m_{2} + m_{3} \\\\\n",
    "& = \\left(m_{0}+1\\right) \\cdot m_{1} + \\left(m_{1}+1\\right) \\cdot m_{2} + \\left(m_{2}+1\\right) \\cdot m_{3} \\\\\n",
    "&= \\left(11+1\\right) \\cdot 22 + \\left(22+1\\right) \\cdot 22 + \\left(22+1\\right) \\cdot 1 \\\\\n",
    "&= 12 \\cdot 22 + 23 \\cdot 22 + 23 \\cdot 1 \\\\\n",
    "&= 264 + 506 + 23 \\\\\n",
    "&= 793\\quad \\blacksquare \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "Let's check this with our code. We'll count the parameters in each layer and then sum them up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05c1bed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_number_of_parameters = let\n",
    "    \n",
    "    # initialize -\n",
    "    L = 3; # How many layers do we want?\n",
    "\n",
    "    total_number_of_parameters = 0;\n",
    "    for i in 1:L\n",
    "        \n",
    "        # get the model\n",
    "        model = models[i];\n",
    "\n",
    "        # get the number of parameters\n",
    "        mᵢ = size(model.W, 1); # number of hidden neurons (rows)\n",
    "        nᵢ = size(model.W, 2); # number of inputs (columns)\n",
    "\n",
    "        # add to the total\n",
    "        total_number_of_parameters += (nᵢ*mᵢ);\n",
    "    end\n",
    "\n",
    "    total_number_of_parameters;\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "285cb22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@assert total_number_of_parameters == 793 # check the number of parameters. Incorrect? Then ... boooom!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2f5283",
   "metadata": {},
   "source": [
    "__Wow!__ That is a lot of parameters! This is a simple network, but it has a lot of parameters. This is one of the reasons why neural networks are so powerful: they can learn very complex functions by adjusting the weights and biases of the network. However, this also means they can be prone to overfitting, especially if we have a small amount of training data.\n",
    "\n",
    "Speaking of training, let's look at how we can train this network to learn the model's parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2984cb18",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dae27b",
   "metadata": {},
   "source": [
    "## Training\n",
    "Suppose have a training dataset $\\mathcal{D} = \\left\\{(\\mathbf{x}_{1},y_{1}),\\dotsc,(\\mathbf{x}_{n},y_{n})\\right\\}$ with $n$ examples, where \n",
    "$\\mathbf{x}_{i}\\in\\mathbb{R}^{m}$ is the $i$-th feature vector, and $y_{i}\\in\\mathbb{R}$ is the corresponding output. The output can be a discrete label (e.g., in classification tasks) where each example has been labeled by an expert, i.e., a human to be in a category $y_{i}\\in\\{-1,1\\}$, or is some continuous value $y_{i}\\in\\mathbb{R}$, e.g., a real-valued measurement such as temperature, pressure, etc, for regression tasks. \n",
    "\n",
    "Classically, the training of feedforward neural networks is done [using the _backpropagation_ algorithm](https://en.wikipedia.org/wiki/Backpropagation), a common _supervised learning_ algorithm based on gradient descent. \n",
    "* _Do we have to use Gradient descent to train a neural network?_ No! Theoretically, other optimization algorithms like genetic algorithms, particle swarm optimization, and simulated annealing could also be used. However, gradient descent is the most common due to rigid orthodoxy (hot take) and some interesting technical features.\n",
    "\n",
    "Backpropagation computes the gradient for training multi-layer neural networks using gradient descent. It is a supervised learning method applying the chain rule of calculus to evaluate the gradient of the loss function concerning network weights and biases. The algorithm involves two steps:   \n",
    "1. **Forward Pass**: Calculate the network's output for an input by passing it through each layer and applying the activation function at every node, yielding an output vector $\\hat{\\mathbf{y}}$ for the input $\\mathbf{x}$.  \n",
    "2. **Backward Pass**: Determine the gradient of the loss function, which measures the difference between the actual output $\\mathbf{y}$ and predicted output $\\hat{\\mathbf{y}}$, by propagating the error backward using the chain rule.\n",
    "\n",
    "### Forward Pass\n",
    "__Initialization__: Initialize the weights and biases of the network randomly or using some heuristic method. Let $\\mathbf{z}_{\\circ}^{\\top} = \\left(x_{1},x_{2},\\dots,x_{n}, 1\\right)$ be the _augmented input vector_, where the last component is a constant `1` that allows us to include the bias term in the weight vector. \n",
    "\n",
    "For each layer $i=1,2,\\dots,L$ of the network:\n",
    "1. For each node $j=1,2,\\dots,m_{i}$ in layer $i$:\n",
    "      1. Compute the input to the activation function: $a_{j} = \\mathbf{z}_{i-1}^{\\top}\\cdot{\\mathbf{w}^{(i)}_{j}}$, where $\\mathbf{w}^{(i)}_{j}$ is the parameter vector (weights and bias) for node $j$ in layer $i$, and $\\mathbf{z}^{\\top}_{i-1}$ is the transpose of the output vector from the previous layer, i.e., the inpt to layer $i$ is the output of layer $i-1$. \n",
    "      2. Compute the output of the activation function: $z_{j} = \\sigma_{i}(a_{j})$, where $\\sigma:\\mathbb{R}\\rightarrow\\mathbb{R}$ denotes the activation function for layer $i$, and $a_{j}$ denotes the input to the activation function. The activation function can be a $\\texttt{sigmoid}$, $\\texttt{tanh}$, $\\texttt{ReLU}$, or any other non-linear function that introduces non-linearity into the model. \n",
    "2. Store the output of each node in the vector $\\mathbf{z}_{i} = \\left\\{z_{1},z_{2},\\dots,z_{m_{i}}\\right\\}$, where $z_{j}\\in\\mathbb{R}$ is the $j$-th component of the output of layer $i$.\n",
    "4. The output of the last layer is the final model predicted output from the network: $\\hat{\\mathbf{y}} = \\mathbf{z}_{L}$.\n",
    "\n",
    "end\n",
    "\n",
    "### Backward Pass (Gradient Descent)\n",
    "The backward pass computes the gradient of a _loss function_ with respect to each weight and bias in the network by propagating the error backward through the network using the chain rule.\n",
    "* _What is a loss function_? Suppose we have a function $\\mathcal{L}(\\mathbf{y},f_{\\theta}(\\mathbf{x}))$ that measures the difference between the actual output $\\mathbf{y}$ and the model predicted output $\\hat{\\mathbf{y}} = f_{\\theta}(\\mathbf{x})$. The loss is _big_ when the predicted output is far from the actual output and _small_ when the predicted output is close to the actual output. The loss function can be considered a measure of how well the model performs on the training data. The goal of training is to minimize this loss function by adjusting the weights and biases of the network.\n",
    "* _What loss function do we use?_ The loss function depends upon the task we are trying to do. For example, the loss can be the mean squared error (MSE) for regression tasks, or [cross-entropy loss for classification tasks](https://en.wikipedia.org/wiki/Cross-entropy), negative log-likelihood for either classification and regression, etc.\n",
    "\n",
    "We assume $\\mathcal{L}(\\mathbf{y},f_{\\theta}(\\mathbf{x}))$ is _at least once differentiable_ with respect to the parameters, i.e., we can compute the gradient $\\nabla_{\\theta}{\\mathcal{L}}(\\cdot)$. The gradient points in the direction of the steepest increase of the function. Thus, we can iteratively update the parameters to minimize the objective function using the update rule:\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\theta_{k+1} = \\theta_{k} - \\alpha(k)\\cdot\\nabla_{\\theta}\\mathcal{L}(\\theta_{k})\\quad\\text{where}{~k = 0,1,2,\\dots}\n",
    "\\end{equation*}\n",
    "$$\n",
    "where $k$ denotes the iteration index, and $\\nabla_{\\theta}\\mathcal{L}(\\cdot)$ is the gradient of the loss function with respect to the parameters $\\theta$. \n",
    "* _What is $\\alpha(k)$?_ The (hyper) parameter $\\alpha(k)>0$ is the _learning rate_ which can be a function of the iteration count $k$. This is a user-adjustable parameter, and we'll assume it's constant for today.\n",
    "* _Stopping?_ Gradient descent will continue to iterate until a stopping criterion is met, i.e., $\\lVert\\theta_{k+1} - \\theta_{k}\\rVert\\leq\\epsilon$ or the maximum number of iterations is reached, or some other stopping criterion is met, i.e., the gradient is small at the current iteration $\\lVert\\nabla_{\\theta}\\mathcal{L}(\\theta_{k})\\rVert\\leq\\epsilon$.\n",
    "\n",
    "Pusedocode for a naive gradient descent algorithm (for a fixed learning rate) is shown in [the week-3 lecture notes](https://github.com/varnerlab/CHEME-5820-Lectures-Spring-2025/blob/main/lectures/week-3/L3c/docs/Notes.pdf). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd856363",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent\n",
    "Computing the full gradient can be expensive. Stochastic Gradient Descent (SGD) is a less expensive approximation to full gradient descent. Suppose we let $\\mathcal{L}(\\theta)$ denote the overall objective function computed over all $n$ of the training examples. Then, the loss function could be written as:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{L}(\\theta) &= \\frac{1}{n}\\sum_{i=1}^{n}\\mathcal{L}_{i}(\\theta)\n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\mathcal{L}_{i}(\\theta)$ denote the loss on the $i$-th training example. The parameter update rule for training dataset $\\mathcal{D}$ is then given by:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\theta_{k+1} = \\theta_{k} - \\frac{\\alpha(k)}{n}\\cdot\\sum_{i\\in\\mathcal{D}}\\nabla_{\\theta}\\mathcal{L}_{i}(\\theta_{k})\\quad\\text{where}{~k = 0,1,2,\\dots}\n",
    "\\end{align*}\n",
    "$$\n",
    "In _stochastic gradient descent_ (SGD), we _approximate_ the full gradient using only a _single_ training example $\\mathcal{L}_{i}(\\theta)$, i.e., we randomly sample a single training example from the dataset $\\mathcal{D}$ at each iteration. The parameter update rule for SGD is:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\theta \\gets \\theta - \\frac{\\alpha}{n}\\cdot\\nabla_{\\theta}\\mathcal{L}_{i}(\\theta)\n",
    "\\end{align*}\n",
    "$$\n",
    "where $i$ is a randomly selected training example from the dataset $\\mathcal{D}$. The basic SGD algorithm is something like:\n",
    "\n",
    "__Initialize__: Choose an initial value for parameters $\\theta$ and a learning rate (function) $\\alpha$.\n",
    "\n",
    "While _not_ converged:\n",
    "1. Randomly shuffle the order of the training data\n",
    "1. For $i = 1,2,\\dots,n$ do:\n",
    "    1. Compute the update: $\\theta \\gets \\theta - \\alpha\\cdot\\nabla_{\\theta}\\mathcal{L}_{i}(\\theta)$\n",
    "1. Check for convergence: $\\lVert \\theta^{\\prime} - \\theta \\rVert_{2}^{2}\\leq\\epsilon$ then converged.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e01c0c",
   "metadata": {},
   "source": [
    "## Strengths and Weaknesses of FNNs\n",
    "Using feedforward neural networks (FNNs) for machine learning tasks has several advantages and disadvantages. Here are some of the key points to consider:\n",
    "\n",
    "### Strengths\n",
    "* _Universal approximation theorem_: FNNs are universal approximators, meaning they can approximate any continuous function to arbitrary precision, given enough hidden units and training data. This makes them very powerful for a wide range of tasks.\n",
    "* _Flexibility_: FNNs can be used for various tasks, including classification, regression, and generative modeling. They can also be adapted to work with different data types, such as images, text, and time series. \n",
    "* _Non-linearity_: Using non-linear activation functions, FNNs can learn complex non-linear relationships between inputs and outputs. This allows them to model complex patterns in the data that linear models cannot capture. They can also be used with various data sources, including images, text, and time series data. This allows them to explore non-linearly separable datasets. \n",
    "\n",
    "### Weaknesses\n",
    "* _Overfitting_: FNNs can easily overfit the training data, especially when the model is too complex or the training data is limited. This can lead to poor generalization to new data. Regularization techniques such as dropout, L1/L2 regularization, and early stopping can help mitigate this issue.\n",
    "* _Computationally expensive_: Training FNNs can be computationally expensive, especially for large datasets or deep networks. This can require significant computational resources and time. However, this is less of a concern with modern hardware and software frameworks, but these techniques require (for the most part) expert-level knowledge to implement efficiently.\n",
    "* _Interpretability_: FNNs are often considered \"black box\" models, meaning it can be difficult to interpret how they make predictions. This can be a disadvantage in applications where interpretability is essential, such as healthcare or finance. However, there are techniques [such as LIME](https://arxiv.org/abs/1602.04938) and [SHAP](https://arxiv.org/abs/1705.07874) that can help improve the interpretability of FNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900ace57",
   "metadata": {},
   "source": [
    "## Lab\n",
    "In Lab `L12b`, we will implement (and train) a feed-forward model for a simple computer vision task. \n",
    "* _Cool, what is this task?_ We'll give handwritten digits to the model and ask it to classify them. The model will be a simple feedforward neural network with one hidden layer. We'll use [the MNIST handwritten image dataset](https://en.wikipedia.org/wiki/MNIST_database), which contains 60,000 images of handwritten digits (0-9). The goal is to train the model to recognize these digits based on the pixel values of the images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fdc635",
   "metadata": {},
   "source": [
    "# Today?\n",
    "That's a wrap! What are some of the interesting things we discussed today?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.4",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
