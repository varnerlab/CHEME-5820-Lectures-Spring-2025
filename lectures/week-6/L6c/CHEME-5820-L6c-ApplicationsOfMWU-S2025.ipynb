{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc4459d3",
   "metadata": {},
   "source": [
    "# L6c: Applications of the Multiplicative Weights Update Algorithm\n",
    "In this lecture, we'll continue our discussion of online learning and the multiplicative weights update algorithm. Today, we'll explore a basic implementation of the algorithm and some of its applications. The key ideas of this lecture are:\n",
    "* [The Multiplicative Weights Algorithm (MWA)](https://en.wikipedia.org/wiki/Multiplicative_weight_update_method)is a powerful online learning algorithm. The MWA updates expert weights based on past performance, assigning higher weights to better-performing experts and lower weights to others. This enables adaptation to changing data distributions and learning from mistakes.\n",
    "* A [zero-sum game](https://en.wikipedia.org/wiki/Zero-sum_game) is a competitive scenario where one participant's gain is exactly balanced by another participant's loss, resulting in a net change of zero in total wealth or benefit. This concept is commonly applied in economics and game theory, with examples including poker, chess, and financial transactions like futures and options contracts.\n",
    "\n",
    "The lecture notes today are taken from [the CMS 139 Course at Caltech prepared by Prof. Thomas Vidick](https://github.com/varnerlab/CHEME-5820-Lectures-Spring-2025/blob/main/lectures/week-6/L6c/docs/CMS139-Vidick-Caltech-multiplicative_weights-2018.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6be8805",
   "metadata": {},
   "source": [
    "## The Multiplicative Weights Update Algorithm (MWA)\n",
    "The Multiplicative Weights Update Algorithm (MWA) is a generalization of the Weighted Majority Algorithm and the Hedge strategy. The MWA is a simple and robust online learning algorithm that can solve many optimization problems. \n",
    "\n",
    "* __Game__: Let $t = 1, 2, \\ldots, T$ denote the current round of the game, and $i$ denote an expert advising us. In each round, we compute a _belief distribution_ $\\mathbf{p}^{(t)} = \\left\\{p_{1}^{(t)}, p_{2}^{(t)}, \\ldots, p_{N}^{(t)}\\right\\}$ over the experts, select a _random_ expert by _sampling_ this distribution and use the selected expert to make a decision. At this point, the _adversary_ (nature) reveals the outcome, and we compute the cost of the decision we've made, where $\\mathbf{m}^{(t)} = \\left\\{m_{1}^{(t)}, m_{2}^{(t)}, \\ldots, m_{N}^{(t)}\\right\\}$ is the overall cost vector and $m_{i}^{(t)}$ is the cost of expert decision $i$ at time $t$. Here, we assume that the costs are in the range $m_{i}^{(t)}\\in[-1, 1]$. Then, the total expected loss at time $t$ is: $L^{(t)} = \\sum_{i=1}^{N}p_{i}^{(t)}m_{i}^{(t)}$, while the overall loss experienced by the _aggregator_ (at the end of the game) is: $L_{A} = \\sum_{t=1}^{T}L^{(t)}$.\n",
    "* __Goal__: The goal of the aggregator (us) is to minimize the total expected loss $L_{A}$ throughout the game, such that we do not experience a loss that is significantly worse than the best decision in hindsight, i.e., $\\min_{i}\\left(\\sum_{t=1}^{T}m_{i}^{(t)}\\right)$.\n",
    "\n",
    "#### Algorithm\n",
    "Fix a learning rate $\\eta\\in\\left(0,{1}/{2}\\right]$, for each expert initialize the weight $w_{i}^{(1)} = 1$. The the costs for a correct/incorrect prediction are in the range $m_{i}^{(t)}\\in[-1, 1]$.\n",
    "\n",
    "For round $t=1,2,\\dots,T$:\n",
    "1. Chose expert $i$ with probability $p_{i}^{(t)} = w_{i}^{(t)}/\\sum_{j=1}^{N}w_{j}^{(t)}$. Ask expert $i$ what the outcome of the experiment should be, denote this outcome as: $y_{i}^{(t)}$.\n",
    "2. The adversary (nature) reveals the true outcome $y_{t}$. Compute the cost of the following expert $i$. If the expert predicted the outcome of the experiment _correctly_ the cost is $m_{i}^{(t)}$ = `-1`, otherwise the cost for an _incorrect prediction_ is $m_{i}^{(t)}$ = `1`. \n",
    "\n",
    "3. Update the weights of expert $i$ as:\n",
    "$$\n",
    "\\begin{align*}\n",
    "w_{i}^{(t+1)} = w_{i}^{(t)}\\cdot\\left(1-\\eta\\cdot{m_{i}^{(t)}}\\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "4. __Note__: The Caltech notes give the update rule as: $w_{i}^{(t+1)} = w_{i}^{(t)}\\cdot\\exp\\left(-\\eta\\cdot{m}_{i}^{(t)}\\right)$ and $\\eta\\in\\left(0,1\\right)$.\n",
    "\n",
    "__Theorem__: The MWA has the following theoretical guarantee. Assume all costs are in the range $m_{i}^{(t)}\\in[-1, 1]$ and $\\eta\\leq{1}/{2}$. Then the Multiplicative Weights Algorithm (MWA) guarantees that after $T$ rounds, for any expert $i$, we have:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\sum_{t=1}^{T}\\mathbf{m}^{(t)}\\cdot\\mathbf{p}^{(t)} & \\leq \\sum_{t = 1}^{T}m_{i}^{(t)}+\\eta\\sum_{t=1}^{T}|m_{i}^{(t)}|+\\frac{\\ln{n}}{\\eta}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314cc194",
   "metadata": {},
   "source": [
    "## Applications of the Multiplicative Weights Update Algorithm\n",
    "The Multiplicative Weights Update (MWA) algorithm has a wide range of applications across various fields, including machine learning, optimization, and game theory. Here are some of its key applications:\n",
    "\n",
    "* __Machine Learning and Prediction__: The MWA method is used in machine learning for online prediction problems, such as learning from expert advice. It helps in combining predictions from multiple experts by iteratively updating weights based on their performance, ensuring that the overall prediction is close to the best expert's performance.\n",
    "* __Game Theory and Portfolio Management__: In game theory, MWA is used to [solve zero-sum games](https://en.wikipedia.org/wiki/Zero-sum_game) by iteratively adjusting strategies based on outcomes. It is also applied in [portfolio management problems](https://www.cis.upenn.edu/~mkearns/finread/helmbold98line.pdf) to optimize investment strategies by dynamically updating the weights of different assets based on their performance. \n",
    "* __Optimization and Linear Programming__: The MWA can be applied to solve linear programs and other optimization problems by iteratively adjusting weights to satisfy constraints. It can efficiently handle systems of linear inequalities and is used in algorithms like Clarkson's for linear programming.\n",
    "* __Complexity Theory and Other Applications__: Additionally, the MWA is used in complexity theory for hardness amplification and in computational geometry for solving specific geometric problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c332e78",
   "metadata": {},
   "source": [
    "## Zero sum games\n",
    "Let's consider the application the mutiplicative weights update algorithm to zero sum games. In [a zero-sum game](https://en.wikipedia.org/wiki/Zero-sum_game), two players have _opposing interests_, and the sum of their payoffs is always zero. The goal of each player is to maximize their own payoff while minimizing the opponent's payoff. The MWA can be used to solve zero-sum games by iteratively adjusting strategies based on outcomes.\n",
    "* __Game__: A set of $k$ players play a zero-sum game. During each turn of the game, each player can choose an action $a\\in\\mathcal{A}$ from the set of actions $\\mathcal{A}$, where the number of possible actions is $\\dim\\mathcal{A} = N$. If we consider $k = 2$, the payoff for the players is represented in a payoff matrix $\\mathbf{M}\\in\\mathbb{R}^{N\\times{N}}$. Let player `1` be the row player, and player `2` be the column player; then $m_{ij}\\in\\mathbf{M}$ is the payoff for player `1` choosing action $i$ and player `2` choosing action $j$. \n",
    "* __Goal__: The goal of each player is to maximize their own payoff while minimizing the opponent's payoff. If the row player chooses action $i$ and the column player chooses action $j$, the payoff for the row player is $-m_{ij}$, and the payoff for the column player is $m_{ij}$. Suppose the row player chooses actions according to a distribution $p$, and the column player chooses actions based on a distribution $q$. The expected payoff for the row player is: $-p^{T}\\mathbf{M}q$ while the expected payoff for the column player is: $p^{T}\\mathbf{M}q$. Thus, the row player wants to mininize $p^{T}\\mathbf{M}q$, while the column player wants to maximize $p^{T}\\mathbf{M}q$.\n",
    "\n",
    "### Von Neumann's Minimax Theorem\n",
    "[Von Neumann's Minimax Theorem](https://en.wikipedia.org/wiki/Minimax_theorem) states that for any two-player zero-sum game, there exists an optimal mixed strategy for each player that minimizes the maximum expected payoff. The optimal mixed strategy for the row player is $p^{*}$ and for the column player is $q^{*}$, such that:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\max_{q}\\min_{p}p^{\\top}\\mathbf{M}q & = \\min_{p}\\max_{q}p^{\\top}\\mathbf{M}q = \\lambda^{\\star}\n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\lambda^{\\star}$ is the optimal utility (also called the value of the game). The _near optimal_ mixed strategies $p^{*}$ and $q^{*}$ can be computed using the MWA algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0154e01b",
   "metadata": {},
   "source": [
    "### Algorithm\n",
    "We have a [two player zero sum game](https://en.wikipedia.org/wiki/Zero-sum_game) with a payoff matrix $\\mathbf{M}\\in\\mathbb{R}^{N\\times{N}}$. The row player chooses actions according to a distribution $p$, and the column player chooses actions based on a distribution $q$. The MWA algorithm can compute the near-optimal mixed strategies $p^{*}$ and $q^{*}$ for the row and column players, respectively.\n",
    "\n",
    "__Initialize__ the weights $w_{i} = \\texttt{rand}$ for all actions $i\\in\\mathcal{A}$, and set the learning rate $\\eta\\in\\left(0,1\\right)$.\n",
    "\n",
    "For round $t=1,2,\\dots,T$:\n",
    "1. The row player chooses an action $i$ with probability $p^{(t)} = \\left\\{w_{i}^{(t)}/\\Phi^{(t)} \\mid i = 1,2,\\dots,N\\right\\}$ where $\\Phi^{(t)} = \\sum_{j=1}^{N}w_{j}^{(t)}$. \n",
    "2. Define $q^{(t)} = \\text{arg}\\max_{q}\\left\\{(p^{(t)})^{\\top}\\mathbf{M}q\\right\\}$ and $m^{(t)} = \\mathbf{M}q^{(t)}$ for the column player.\n",
    "3. Penalize costly decisions by updating the weights as: $w_{i}^{(t+1)} = w_{i}^{(t)}\\cdot\\exp\\left(-\\eta\\cdot{m}_{i}^{(t)}\\right)$ for all actions $i\\in\\mathcal{A}$\n",
    "\n",
    "where we assume that the payoffs (elements of $\\mathbf{M}$) are in the range $m_{ij}\\in[-1, 1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1d050a",
   "metadata": {},
   "source": [
    "## Example: Rock-Paper-Scissors\n",
    "Let's consider an example of a two-player zero-sum game: [Rock-Paper-Scissors](https://en.wikipedia.org/wiki/Rock_paper_scissors). In this game, each player _simultaneously_ chooses one of three possible actions: Rock, Paper, or Scissors. This game has three possible outcomes: win, loose or draw.\n",
    "* __Rules__:A player who decides to play rock will beat another player who chooses scissors (`rock crushes scissors`), but will lose to one who has played paper (`paper covers rock`); a play of paper will lose to a play of scissors (`scissors cuts paper`). If both players choose the same shape, the game is a draw.\n",
    "\n",
    "The payoff matrix for this game is the `3` $\\times$ `3` matrix:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{M} = \\begin{pmatrix}\n",
    "0 & -1 & 1\\\\\n",
    "1 & 0 & -1\\\\\n",
    "-1 & 1 & 0\n",
    "\\end{pmatrix}\n",
    "\\end{align*}\n",
    "$$\n",
    "where the rows correspond to the actions of the _row player_ and the columns, correspond to the actions of the _column player_. The payoff for the row player is $-m_{ij}$, and the payoff for the column player is $m_{ij}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1d3757a-b968-443d-a1a9-15520532d8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"Include.jl\"); # load my codes, packages, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67424f3-a604-4cee-a8b4-f6f8e7d95d38",
   "metadata": {},
   "source": [
    "__Build a model__. Let's construct an instance of [the `MyTwoPersonZeroSumGameModel` type](src/Types.jl) using [a custom `build(...)` method](src/Factory.jl). The model holds information associated with the game. \n",
    "\n",
    "We the game model in the `model::MyTwoPersonZeroSumGameModel` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8148173-7992-47d3-97d8-996698b79266",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = let\n",
    "\n",
    "    # setup \n",
    "    M = [0 -1 1; 1 0 -1 ; -1 1 0]; # rock paper scissors payoff matrix\n",
    "\n",
    "    # build a model -\n",
    "    model = build(MyTwoPersonZeroSumGameModel, (\n",
    "        ϵ = 0.8, # learning rate\n",
    "        n = 3, # number of actions\n",
    "        T = 20, # number of rounds we play the game\n",
    "        payoffmatrix = M, # payoff matrix\n",
    "    ));\n",
    "\n",
    "    model; # return the \n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff21629e-0abe-4ec2-98d5-1605be30f8ae",
   "metadata": {},
   "source": [
    "__Play the game__. Next, we play the game. We pass the `model::MyTwoPersonZeroSumGameModel` instance into [the `play(...)` method](src/Online.jl) as the only argument. This method returns the raw game output, where each row is a game instance (round), and each column is a player action, and the weights matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3020475-e444-4ba1-aa4f-1eed55fa77f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "(rps_sim, weights) = play(model);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7fc50bb1-ddcb-4851-8621-c8538c96a75d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21×3 Matrix{Float64}:\n",
       " 0.564277  0.16212   0.340581\n",
       " 0.253546  0.16212   0.757978\n",
       " 0.253546  0.360805  0.340581\n",
       " 0.564277  0.16212   0.340581\n",
       " 0.253546  0.16212   0.757978\n",
       " 0.253546  0.360805  0.340581\n",
       " 0.564277  0.16212   0.340581\n",
       " 0.253546  0.16212   0.757978\n",
       " 0.253546  0.360805  0.340581\n",
       " 0.564277  0.16212   0.340581\n",
       " 0.253546  0.16212   0.757978\n",
       " 0.253546  0.360805  0.340581\n",
       " 0.564277  0.16212   0.340581\n",
       " 0.253546  0.16212   0.757978\n",
       " 0.253546  0.360805  0.340581\n",
       " 0.564277  0.16212   0.340581\n",
       " 0.253546  0.16212   0.757978\n",
       " 0.253546  0.360805  0.340581\n",
       " 0.564277  0.16212   0.340581\n",
       " 0.253546  0.16212   0.757978\n",
       " 0.253546  0.360805  0.340581"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedb0260-4742-4790-ae5b-10684e837cf0",
   "metadata": {},
   "source": [
    "__Games outcome table__. `Unhide` the code block below to see how we constructed the game table [using the `pretty_tables(...)` method exported by the `PrettyTables.jl` package](https://github.com/ronisbr/PrettyTables.jl).\n",
    "* _Summary_: Each row of the table displays the game's outcome. The first column shows the action of the _row player_, while the second column shows the (near) optimal action of the _column_ player, given the action of the _row player_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "58895868-95c5-41df-923e-4a0d79fce793",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== ========== ===========\n",
      " \u001b[1m  game \u001b[0m \u001b[1m player_1 \u001b[0m \u001b[1m player_2 \u001b[0m\n",
      " \u001b[90m Int64 \u001b[0m \u001b[90m   String \u001b[0m \u001b[90m   String \u001b[0m\n",
      "======== ========== ===========\n",
      "      1       rock      paper\n",
      "      2   scissors       rock\n",
      "      3      paper   scissors\n",
      "      4       rock      paper\n",
      "      5   scissors       rock\n",
      "      6      paper   scissors\n",
      "      7       rock      paper\n",
      "      8   scissors       rock\n",
      "      9      paper   scissors\n",
      "     10       rock      paper\n",
      "     11   scissors       rock\n",
      "     12      paper   scissors\n",
      "     13       rock      paper\n",
      "     14   scissors       rock\n",
      "     15      paper   scissors\n",
      "     16       rock      paper\n",
      "     17   scissors       rock\n",
      "     18      paper   scissors\n",
      "     19       rock      paper\n",
      "     20   scissors       rock\n",
      "======== ========== ===========\n"
     ]
    }
   ],
   "source": [
    "let\n",
    "\n",
    "    # initialize -\n",
    "    T = model.T;\n",
    "    moves = Dict{Int, String}(1 => \"rock\", 2=> \"paper\", 3=>\"scissors\"); # setup moves map\n",
    "    df = DataFrame();\n",
    "\n",
    "    # build rounds table -\n",
    "    for t ∈ 1:T\n",
    "        row_df = (\n",
    "            game = t,\n",
    "            player_1 = rps_sim[t,1] |> i-> moves[i],\n",
    "            player_2 = rps_sim[t,2] |> i-> moves[i],\n",
    "        )\n",
    "        push!(df, row_df);\n",
    "    end\n",
    "    \n",
    "    # build a table -\n",
    "    pretty_table(df, tf = tf_simple)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c1f45c-7ff5-420a-bf7a-29e60a415363",
   "metadata": {},
   "source": [
    "### What about the min-max theorem?\n",
    "From the game output table, we see that the actions of the column player (in this case) are near optimal. Let's dig a little deeper into this, and look at the optimal payoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "df444707-227a-4ed7-aaf9-5c9f79a178f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Payoff for player 1: -1.0 and player 2: 1.0\n"
     ]
    }
   ],
   "source": [
    "let \n",
    "\n",
    "    # setup -\n",
    "    M = model. payoffmatrix;\n",
    "\n",
    "    # actions - (r,p,s)\n",
    "    p = [1,0,0]; # select a move for player 1\n",
    "    q = [0,0,1]; # select a move for player 2\n",
    "\n",
    "    # compute the payoff\n",
    "    payoff = transpose(p)*M*q\n",
    "    \n",
    "    # rerurn\n",
    "    println(\"Payoff for player 1: $(-payoff) and player 2: $(payoff)\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08d0aeb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Today?\n",
    "That's a wrap! What are some of the interesting things we discussed today?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.3",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
